{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02800477",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /orpo/\n",
    "categories:\n",
    "- Post-Training LLM\n",
    "date: '2024-07-30'\n",
    "colab: <a href=\"https://colab.research.google.com/drive/115cSiQsE55QReY7kHWs2KqJu1xFgpXkG#scrollTo=042131d5\"><img src=\"images/colab.png\" alt=\"Open In Colab\"></a>\n",
    "image: /images/orpo/thumbnail.jpg\n",
    "title: \"Fine-Tune Mistral v0.3 with ORPO and Unsloth \"\n",
    "subtitle: \"Low-Rank Adapter Model Fine-tuning\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c1d15",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/orpo/thumbnail.jpg\" alt=\"Image\">\n",
    "</center>\n",
    "\n",
    "The field of artificial intelligence and machine learning is marked by constant innovation, with new tools and methodologies emerging to expand the horizons of what these technologies can achieve. Recently, significant upgrades were introduced in popular AI model series, enhancing their capabilities and setting new benchmarks in AI development.\n",
    "\n",
    "However, to fully leverage the potential of these advanced models, it’s essential to employ sophisticated fine-tuning techniques like ORPO (Odds Ratio Preference Optimization) and Unsloth. ORPO simplifies the alignment process by integrating preference optimization directly into the training phase, eliminating the need for a separate alignment step. Unsloth, on the other hand, offers groundbreaking advancements in training efficiency, significantly speeding up the process while reducing memory consumption without compromising accuracy.\n",
    "\n",
    "In this article, we will explore how to fine-tune Mistral v0.3 using ORPO and Unsloth, demonstrating how these techniques can enhance model performance and efficiency. By understanding and applying these methods, you can unlock new levels of capability and efficiency in your AI projects. The code for this process can be found on [Google Colab](https://colab.research.google.com/drive/115cSiQsE55QReY7kHWs2KqJu1xFgpXkG#scrollTo=042131d5) and in the [LLM Tutorial](https://github.com/mattdepaolis/llm-tutorials) on GitHub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6498990",
   "metadata": {},
   "source": [
    "## ORPO\n",
    "\n",
    "Instruction tuning and preference alignment are crucial for customizing Large Language Models (LLMs) for specific tasks. This typically involves a multi-step process: first, Supervised Fine-Tuning (SFT) on instructions to tailor the model to the desired domain, and second, applying preference alignment techniques such as Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to enhance the probability of producing preferred responses over less desirable ones. Researchers have found that although SFT adjusts the model to the target domain, it also raises the chances of producing both unwanted and desired answers. Therefore, the preference alignment stage is essential to enlarge the disparity between the probabilities of accepted and rejected outputs.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/orpo/sft_drawback.jpg\" alt=\"Image\">\n",
    "</center>\n",
    "\n",
    "\n",
    "[Hong and Lee (2024)](https://arxiv.org/abs/2403.07691) introduced ORPO (Odds Ratio Preference Optimization), a groundbreaking method that aligns the language model without a reference model in a single-step manner by assigning a weak penalty to the rejected responses and a strong adaptation signal to the chosen responses with a simple log odds ratio term appended to the negative log-likelihood loss.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/orpo/orpo_comparison.jpg\" alt=\"Image\">\n",
    "</center>\n",
    "\n",
    "\n",
    " This approach enhances the traditional language modeling objective by integrating the negative log-likelihood (NLL) loss with an odds ratio (OR) component. The OR loss imposes a slight penalty on disfavored responses while significantly rewarding favored ones, enabling the model to concurrently master the target task and align with human preferences. The objective function for ORPO is defined as follows:\n",
    "\n",
    "$$\\mathscr{L}{ORPO} = \\mathbb{E}{(x, y_{w}, y_l)}[\\mathscr{L}{SFT} + \\lambda \\cdot \\mathscr{L}{OR}]$$\n",
    "\n",
    "In this formula, *SFT* represents the conventional supervised fine-tuning loss, *OR* denotes the odds ratio loss, and *Lambda* is a weighting factor that balances these two components. This integration ensures that the model adapts effectively to the desired domain while minimizing the generation of undesired outputs.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c65f69",
   "metadata": {},
   "source": [
    "## Unsloth\n",
    "Unsloth is a fine-tuning framework designed to accelerate the training of large language models (LLMs) like Llama and Mistral, while drastically reducing memory usage. It achieves this through several optimizations:\n",
    "\n",
    "1. Manual Derivation and Handwritten GPU Kernels: Unsloth optimizes computational steps by manually deriving and handwriting GPU kernels, bypassing inefficiencies in general-purpose libraries.\n",
    "\n",
    "2. Quantization Techniques: Utilizing 4-bit and 16-bit quantization (QLoRA) reduces memory requirements without compromising model accuracy.\n",
    "\n",
    "3. Optimized Attention Mechanisms: Integrating Flash Attention v2 for faster attention calculations and reduced memory usage.\n",
    "\n",
    "4. Enhanced Memory Management: Efficient memory allocation and data transfer processes optimize VRAM usage.\n",
    "\n",
    "Unsloth can make training up to 2 times faster on single GPUs and reduces memory usage by up to 60% without degrading accuracy, supporting diverse fine-tuning use cases, including instructional fine-tuning and direct preference optimization (DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdece630",
   "metadata": {},
   "source": [
    "## Fine-Tuning Mistral v0.3 with ORPO and Unsloth\n",
    "In this example we will QLoRA fine-tune the Mistral v0.3 7B model using ORPO and the Unsloth framework. ORPO necessitates a preference dataset that includes a prompt, a selected answer, and a discarded answer. To achieve this, we will utilize [llmat/dpo-orpo-mix-38k-balanced](https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced), a dataset that merges high-quality DPO datasets and has been further balanced using a clustering-based approach.\n",
    "\n",
    "Let's start by installing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042131d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce262fd-ffdb-486e-b1b1-eff94a688efe",
   "metadata": {},
   "source": [
    "Now let's login to our W&B workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "%env WANDB_NOTEBOOK_NAME = $Fine_tune_Mistral_with_ORPO\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c19c5",
   "metadata": {},
   "source": [
    "### Load the Model and Tokenizer for LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138ca7f",
   "metadata": {},
   "source": [
    "In the following, we will load the Mistral 7B v0.3 model in 4-bit precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb208d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = './model'\n",
    "model_id = 'mistralai/Mistral-7B-v0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba784f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e62f94",
   "metadata": {},
   "source": [
    "### Loading Checks\n",
    "After loading the model, it's crucial to ensure that all parameters are correctly placed on the GPU and that none are overflowing onto the CPU. This can be particularly important for large models where memory management is critical.\n",
    "\n",
    "To verify the placement of the model's parameters, you can iterate through the model's named parameters and check their device type. If any parameter is on the CPU (indicated by the device type 'meta'), it will be printed out. This ensures that your model is fully utilizing the GPU resources and avoiding any potential performance bottlenecks.\n",
    "\n",
    "Here is the code to perform this check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91076218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are no parameters overflowing onto cpu (meta).\n",
    "for n, p in model.named_parameters():\n",
    "    if p.device.type=='meta':\n",
    "        print(f\"{n} is on meta!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403062c0",
   "metadata": {},
   "source": [
    "## Setting Up LoRA Fine-Tuning\n",
    "To prepare your model for LoRA (Low-Rank Adaptation) fine-tuning, you need to configure it properly. This involves setting up the LoRA configuration.\n",
    "Here's a brief overview of the parameter settings:\n",
    "\n",
    "1. **`r`**: This parameter controls the rank of the low-rank adaptation matrices. It's suggested to choose a value greater than 0, with common choices being 8, 16, 32, 64, or 128. The best setting depends on the specific use case and computational resources, but a good starting point is 8 or 16.\n",
    "\n",
    "2. **`lora_alpha`**: This parameter scales the magnitude of the LoRA update. A higher value can lead to more significant changes in the model's behavior. In our example we are setting lora_alpha to 32.\n",
    "\n",
    "3. **`target_modules`**: This list specifies which modules in the model should be fine-tuned. The settings include key modules like `\"q_proj\"`, `\"k_proj\"`, `\"v_proj\"`, `\"o_proj\"`, `\"gate_proj\"`, `\"up_proj\"`, and `\"down_proj\"`. If the task involves chat fine-tuning, it's also beneficial to set `\"lm_head\"` (language model head) as trainable.\n",
    "\n",
    "6. **`use_gradient_checkpointing`**: This parameter activates gradient checkpointing to conserve memory. It is managed by Unsloth, which offloads input and output embeddings to disk, thereby saving VRAM.\n",
    "\n",
    "7. **`random_state`**: This parameter sets the seed for random number generation, ensuring reproducibility. The best setting is any integer value; in the code, it's set to 3407.\n",
    "\n",
    "8. **`use_rslora`**: This parameter activates RSLoRA, which adjusts the scaling factor of LoRA adapters to be proportional to 1/√r instead of 1/r. This adjustment enhances the stability of learning, particularly for higher adapter ranks, and improves fine-tuning performance as the rank increases.\n",
    "\n",
    "These settings provide a good starting point for fine-tuning a language model using PEFT. However, the optimal settings may vary depending on the specific task and dataset, so some experimentation may be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    lora_alpha = 32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n",
    "        \n",
    "    ],\n",
    "    \n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b4873f",
   "metadata": {},
   "source": [
    "## Set up Tokenizer and Padding\n",
    "\n",
    "Before starting the fine-tuning process, it's essential to configure the tokenizer and set up padding correctly. This ensures that the model can handle input sequences efficiently and that special tokens are properly managed.\n",
    "\n",
    "### Inspect the Tokenizer\n",
    "Print out the tokenizer details, including the vocabulary size, beginning-of-sequence (BOS) token, end-of-sequence (EOS) token, and chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd8dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56490c77-df63-4110-a4f4-75ae3f8b03e3",
   "metadata": {},
   "source": [
    "### Customize Chat Template\n",
    "\n",
    "When working with Llama/Mistral models, it's sometimes necessary to customize the chat template to ensure the conversation is formatted correctly. This customization is particularly useful when handling cases where the initial message in the conversation might not be from the assistant. By ensuring the beginning-of-sequence token (bos_token) is correctly placed, we can maintain the proper structure and flow of the conversation.\n",
    "\n",
    "The following code snippet demonstrates how to set the chat template manually for such scenarios. This template checks if the first message is from the assistant. If not, it adds the bos_token at the beginning. This step is crucial because we format the chosen and rejected responses separately, and we want to avoid adding an extra bos_token before the response when there's no initial user message.\n",
    "\n",
    "The template is defined using a Jinja-like syntax, which iterates through the messages and formats them based on their roles (`user` or `assistant`). For user messages, it wraps the content with `[INST]` and `[/INST]` tags, while for assistant messages, it appends an end-of-sequence token (`eos_token`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf5a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "# Test chat template\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'write a quick sorf algorithm in python.'},\n",
    "    {'role': 'assistant', 'content': 'here you are.'},\n",
    "    {'role': 'user', 'content': 'great.'},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06501bcc",
   "metadata": {},
   "source": [
    "### Set the Pad Token\n",
    "When working with tokenizers, it's essential to designate a token for padding sequences to ensure they all have the same length. This padding token helps maintain the consistency of input shapes when batching data for training models. The following code snippet demonstrates how to set the padding token (pad_token) in your tokenizer by checking for the presence of specific tokens in its vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5346909",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the pad token to <pad>, if not <|pad|>, if not <unk> if <unk>\n",
    "if '<pad>' in tokenizer.get_vocab():\n",
    "    print('<pad> token is is in the tokenizer. Usinh <pad> for pad')\n",
    "    #Set the pad token\n",
    "    tokenizer.pad_token = '<pad>'\n",
    "elif '<|pad|>' in tokenizer.get_vocab():\n",
    "    print('<|pad|> token is in the tokenizer. Using for <|pad|> for pad')\n",
    "    # Set the pad token\n",
    "    tokenizer.pad_token = '<|pad|>'\n",
    "elif '<unk>' in tokenizer.get_vocab():\n",
    "    print('<unk> token is in the tokenizer. Using for <unk> for pad')\n",
    "    # Set the pad token\n",
    "    tokenizer.pad_token = '<unk>'\n",
    "else:\n",
    "    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e270543",
   "metadata": {},
   "source": [
    "### Update the Model Configuration\n",
    "The following code snippet demonstrates how to update the pad token ID in both the model and its configuration to match the tokenizer's pad token ID. Additionally, it includes checks and print statements to verify the consistency of these IDs and provides information about the tokenizer's special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dcd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update pad token id in model and its config\n",
    "model.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check if they are equal\n",
    "assert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Special tokens map:', tokenizer.special_tokens_map)\n",
    "print('All special tokens:', tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310513b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccbaf5",
   "metadata": {},
   "source": [
    "## Set embed and norm layers to trainable \n",
    "#### (recommended for chat fine-tuning if the chat template has been changed)\n",
    "\n",
    "When fine-tuning a model for chat applications, it's often beneficial to set specific layers to be trainable, especially if you are changing the chat template. This ensures that the model can adapt to the new input format more effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57716653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold the names of the trainable parameters\n",
    "trainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n",
    "\n",
    "# Set modules to be trainable\n",
    "for n, p in model.named_parameters():\n",
    "    if any(k in n for k in trainable_params_names):\n",
    "        p.requires_grad_(True)\n",
    "    else:\n",
    "        p.requires_grad_(False) # Optional: Set the rest to be trainable\n",
    "\n",
    "# Make a dictionary of trainable parameters\n",
    "trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "# Convert trainable_params to state_dict format\n",
    "trainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15b651",
   "metadata": {},
   "source": [
    "## Prepare for LoRA fine-tuning\n",
    "Before starting the LoRA (Low-Rank Adaptation) fine-tuning process, it’s essential to understand which parameters in your model are trainable and which are not. This helps in ensuring that only the desired parameters are updated during training, which is crucial for efficient and effective fine-tuning.\n",
    "\n",
    "To achieve this, you can use the following function to print the number of trainable parameters in the model and list which parameters are trainable and which are not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model and lists which parameters\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    non_trainable_params = 0\n",
    "    all_params = 0\n",
    "\n",
    "    print(\"Trainable Parameters\")\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            print(f\" {name}\")\n",
    "        else:\n",
    "            non_trainable_params += param.numel()\n",
    "\n",
    "    print(\"\\nNon-Trainable Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\" {name}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49eefea",
   "metadata": {},
   "source": [
    "Print the trainable parameters to verify the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e683a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826704e",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Dataset for Fine-Tuning\n",
    "\n",
    "When working with large datasets, it's essential to streamline the process of loading, splitting, and formatting the data to ensure efficient model training and testing. The following Python code demonstrates how to achieve this using the Hugging Face datasets library, along with a tokenizer for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepared with the help of code from: https://github.com/xfactlab/orpo/.\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "dataset_name = 'llmat/dpo-orpo-mix-38k-balanced' # Ensure this is defined\n",
    "\n",
    "max_num_samples = None # Set to None to use the full dataset\n",
    "#max_num_samples = 10000 # set to None to use the full dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "def build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_size_ratio=0.1):\n",
    "    # Determin the split specification based on max_num samples\n",
    "    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n",
    "\n",
    "    # Load the dataset\n",
    "    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    if max_num_samples is not None:\n",
    "        full_data = full_data.shuffle(seed=42)\n",
    "    else:\n",
    "        full_data = full_data\n",
    "\n",
    "    # Determine the number of test samples\n",
    "    num_total_samples = len(full_data)\n",
    "    test_size = int(test_size_ratio * num_total_samples)\n",
    "\n",
    "    # Randomly split the data into training and test sets\n",
    "    dataset = full_data.train_test_split(test_size=test_size)\n",
    "\n",
    "    column_names = list(dataset['train'].features)\n",
    "\n",
    "    def apply_dpo_template(example):\n",
    "        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n",
    "        if all(k in example.keys() for k in ('chosen', 'rejected')):\n",
    "            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n",
    "            # We therefore need to extract the N-1 turns to form the prompt\n",
    "            prompt_messages = example['chosen'][:-1]\n",
    "\n",
    "            # Now we extract the final turn to define chosen/rejected responses\n",
    "            chosen_messages = example['chosen'][-1:]\n",
    "            rejected_messages = example['rejected'][-1:]\n",
    "            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n",
    "                desc='Formatting comparisons with prompt template',)\n",
    "\n",
    "    for split in ['train', 'test']:\n",
    "        dataset[split] = dataset[split].rename_columns(\n",
    "            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected'}\n",
    "        )\n",
    "\n",
    "    return dataset['train'], dataset['test']\n",
    "\n",
    "# Assuming 'tokenizer' and 'dataset_name' are already defined\n",
    "train, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fcf7c2",
   "metadata": {},
   "source": [
    "After preparing and formatting your dataset for fine-tuning, let's inspect the data to ensure that it has been correctly processed. This step helps you verify that the prompt, chosen, rejected, and messages fields are properly formatted and contain the expected information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prompt:', train['prompt'][0])\n",
    "print('\\n\\nChosen:', train['chosen'][0])\n",
    "print('\\n\\nRejected:', train['rejected'][0])\n",
    "print('\\n\\nMessages (incl. prompt):', train['messages'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4114fe98",
   "metadata": {},
   "source": [
    "## Setting Up and Running Training\n",
    "In this tutorial, we will go through the process of setting up and running the training for your model. This includes configuring training parameters, creating a custom logging callback, and initiating the training process.\n",
    "\n",
    "### Set Training Parameters\n",
    "Define the training parameters such as the model name, number of epochs, gradient accumulation steps, batch size, and the directory to save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "epochs=1\n",
    "grad_accum=4\n",
    "batch_size=8\n",
    "fine_tune_tag='ORPO'\n",
    "save_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ad96c",
   "metadata": {},
   "source": [
    "### Create a Custom Logging Callback\n",
    "Implement a custom callback to log training metrics to a file. This callback will write the training and evaluation loss to a log file and save the trainable parameters at checkpoint steps. Create an instance of the custom logging callback with the specified log file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Custom callback to log metrics\n",
    "class LoggingCallback(transformers.TrainerCallback):\n",
    "    def __init__(self, log_file_path):\n",
    "        self.log_file_path = log_file_path\n",
    "\n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        with open(self.log_file_path, 'a') as f:\n",
    "            if 'loss' in logs:\n",
    "                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n",
    "                if 'eval_loss' in logs:\n",
    "                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n",
    "                f.flush()  # Force flush the buffered data to file\n",
    "\n",
    "        # Check if the current step is a checkpoint step\n",
    "        if state.global_step % int(args.save_steps) == 0:\n",
    "            # Check if the last checkpoint path exists\n",
    "            if state.best_model_checkpoint:\n",
    "                checkpoint_dir = state.best_model_checkpoint\n",
    "            else:\n",
    "                # If not, construct the checkpoint directory path\n",
    "                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n",
    "\n",
    "            # Ensure the checkpoint directory exists\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save trainable params in the checkpoint directory\n",
    "            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n",
    "            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n",
    "            torch.save(current_trainable_params_state_dict, file_path)\n",
    "\n",
    "# Log file path\n",
    "cache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\n",
    "log_file_path = os.path.join(cache_dir, 'training_logs.txt')\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "logging_callback = LoggingCallback(log_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d7332",
   "metadata": {},
   "source": [
    "## Setting Up ORPO Training\n",
    "In this section, we'll walk through setting up and training a model using the ORPOTrainer from the trl library.\n",
    "\n",
    "I trained the model on the entire dataset (38k samples) using an RTX 4090 GPU (24 GB of VRAM). The training took 7 hours and 35 minutes. You can use smaller GPUs with less VRAM and a smaller batch size. In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like 'max_num_samples = 10000' to only load 10k samples.\n",
    "\n",
    "### Configure ORPO\n",
    "We define the configuration for the ORPO training. This configuration includes various hyperparameters and settings for training. An important parameter to set is beta. beta is the constant λ of the loss function in the paper. It controls how much weight we give to the preference part vs. the cross-entropy part. In our example, we set the value to 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8852ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import ORPOTrainer, ORPOConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "orpo_config = ORPOConfig(\n",
    "    beta=0.2,\n",
    "    save_steps=500, \n",
    "    logging_steps=1,\n",
    "    num_train_epochs=epochs,\n",
    "    output_dir=save_dir,\n",
    "    evaluation_strategy='steps', \n",
    "    do_eval=True,\n",
    "    eval_steps=0.2,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    log_level='debug',\n",
    "    optim='paged_adamw_8bit',\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=0.03,\n",
    "    learning_rate=1e-4, \n",
    "\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    "\n",
    "    max_completion_length=1024,\n",
    "    remove_unused_columns=True,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd425bd",
   "metadata": {},
   "source": [
    "### Initialize ORPOTrainer\n",
    "Create an instance of ORPOTrainer with the model, datasets, tokenizer, and the configuration defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "orpo_trainer = ORPOTrainer(\n",
    "    model,\n",
    "    args=orpo_config,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    callbacks=[logging_callback], # Add custom callback here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be72cd3",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Set the model configuration to avoid cache warnings and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b8170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False # silence the warnings\n",
    "orpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d93f7",
   "metadata": {},
   "source": [
    "## Plotting Training and Evaluation Losses\n",
    "\n",
    "After training your model, it's important to visualize the training and evaluation losses to understand how well your model is performing and to identify any potential issues. Visualizing the losses can help you diagnose problems such as overfitting or underfitting and make informed decisions about further training or model adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e300246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to hold training and evaluation losses and steps\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "train_steps = []\n",
    "eval_steps = []\n",
    "\n",
    "# Populate the lists from the log history\n",
    "for entry in orpo_trainer.state.log_history:\n",
    "    if 'loss' in entry:\n",
    "        train_losses.append(entry['loss'])\n",
    "        train_steps.append(entry['step'])\n",
    "    if 'eval_loss' in entry:\n",
    "        eval_losses.append(entry['eval_loss'])\n",
    "        eval_steps.append(entry['step'])\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(train_steps, train_losses, label='Train Loss')\n",
    "plt.plot(eval_steps, eval_losses, label='Eval Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32140da",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/orpo/orpo_loss.png\" alt=\"Image\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356451b5-fd3d-405f-ac80-1dacba228c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T11:33:25.353604Z",
     "iopub.status.busy": "2024-08-05T11:33:25.353223Z",
     "iopub.status.idle": "2024-08-05T11:33:25.357231Z",
     "shell.execute_reply": "2024-08-05T11:33:25.356533Z",
     "shell.execute_reply.started": "2024-08-05T11:33:25.353585Z"
    }
   },
   "source": [
    "Let’s now check the W&B plots. While the loss goes down, we also can see that the difference between the chosen and rejects answers becomes clearer.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/orpo/orpo_metrics.png\" alt=\"Image\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba08fb2",
   "metadata": {},
   "source": [
    "## Merging Adapters and Saving the Model to Hugging Face Hub\n",
    "As a last step, we merge the adapters with the original model using 16-bit precision to enhance quality. Initially, we save it locally in the \"model\" directory before uploading it to the Hugging Face Hub. The trained model is available at [llmat/Mistral-v0.3-7B-ORPO](https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ebd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n",
    "model.push_to_hub_merged(\"llmat/Mistral-v0.3-7B-ORPO\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d6f5a-3724-4f58-a6b4-698e4d35040c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This article presented a thorough overview of ORPO fine-tuning and its practical application to a Mistral v0.3 7B model. Utilizing QLoRA's efficient memory management, we successfully fine-tuned a 7B LLM on a high-quality dataset with minimal GPU resources.\n",
    "\n",
    "I hope you found this guide helpful. If you liked this article, follow me on Hugging Face [@llmat](https://huggingface.co/llmat). Best of luck with your model fine-tuning!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
