{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02800477",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /orpo/\n",
    "categories:\n",
    "- Large Language Models\n",
    "date: '2024-07-30'\n",
    "colab: <a href=\"https://colab.research.google.com/drive/115cSiQsE55QReY7kHWs2KqJu1xFgpXkG#scrollTo=042131d5\"><img src=\"images/colab.png\" alt=\"Open In Colab\"></a>\n",
    "image: /images/orpo/thumbnail.jpg\n",
    "title: \"Fine-Tune Mistral v0.3 with ORPO and Unsloth \"\n",
    "subtitle: \"Low-Rank Adapter Model Fine-tuning\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c1d15",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/orpo/thumbnail.jpg\" alt=\"Image\">\n",
    "</center>\n",
    "\n",
    "ORPO is an innovative fine-tuning method that merges traditional supervised fine-tuning with preference alignment into a unified process. This approach decreases the computational resources and time needed for training. Additionally, empirical evidence shows that ORPO surpasses other alignment techniques across different model sizes and benchmarks.\n",
    "\n",
    "In this article, we will fine-tune the newest Mistral 7B model using ORPO and the TRL library. The code is accessible on Google Colab and in the LLM Tutorial on GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6498990",
   "metadata": {},
   "source": [
    "## ORPO\n",
    "\n",
    "Instruction tuning and preference alignment are crucial methods for customizing Large Language Models (LLMs) for particular tasks. Typically, this entails a multi-step process: first, Supervised Fine-Tuning (SFT) on instructions to tailor the model to the desired domain, and second, applying preference alignment techniques such as Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to enhance the probability of producing preferred responses over less desirable ones.\n",
    "\n",
    "Researchers have discovered a drawback in this method. Although SFT successfully adjusts the model to the target domain, it also unintentionally raises the chances of producing both unwanted and desired answers. Therefore, the preference alignment stage is essential to enlarge the disparity between the probabilities of accepted and rejected outputs.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/orpo/sft_drawback.jpg\" alt=\"Image\">\n",
    "</center>\n",
    "\n",
    "[Hong and Lee (2024)](https://arxiv.org/abs/2403.07691) presented ORPO, an innovative approach that combines instruction tuning and preference alignment into a single training framework. ORPO modifies the conventional language modeling objective by incorporating the negative log-likelihood loss with an odds ratio (OR) component. This OR loss applies a mild penalty to rejected responses while greatly rewarding preferred ones, allowing the model to simultaneously learn the target task and align with human preferences.\n",
    "\n",
    "$$\\mathscr{L}{ORPO} = \\mathbb{E}{(x, y_{w}, y_l)}[\\mathscr{L}{SFT} + \\lambda \\cdot \\mathscr{L}{OR}]$$\n",
    "\n",
    "ORPO has been integrated into key fine-tuning libraries such as TRL, Axolotl, and LLaMA-Factory. The following section will demonstrate its usage with TRL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdece630",
   "metadata": {},
   "source": [
    "## Fine-Tuning Mistral v0.3 with ORPO and Unsoth\n",
    "Mistral AI's v0.3 is a significant update to their AI model, introducing improved performance and efficiency. This version includes enhanced instruction-following capabilities, making interactions more intuitive. Additionally, Mistral v0.3 incorporates advanced reasoning skills, enabling it to tackle complex tasks more effectively. The update also extends the context length to 32768 tokens, allowing for more detailed and coherent conversations. Technical details include an extended vocabulary (32000 to 32768), a new tokenizer, and support for function calling.\n",
    "\n",
    "ORPO necessitates a preference dataset that includes a prompt, a selected answer, and a discarded answer. To achieve this, we will utilize [llmat/dpo-orpo-mix-38k-balanced](https://huggingface.co/datasets/llmat/dpo-orpo-mix-38k-balanced), a dataset that merges high-quality DPO datasets and has been further balanced using a clustering-based approach.\n",
    "\n",
    "To efficiently fine-tune our model we will use the unlsoth library. Unsloth significantly improves speed and efficiency in the training of Large Language Models (LLMs). The speed and efficiency gains are achieved through several optimizations, including manual autograd and chained matrix multiplication. Furthermore, it utilizes Flash Attention via xformers and Tri Dao's implementation, which is a highly optimized approach to handling attention mechanisms in transformer models. Unsloth  makes fine-tuning 2 times faster with 50% less memory usage.\n",
    "\n",
    "Let's start by installing the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042131d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce262fd-ffdb-486e-b1b1-eff94a688efe",
   "metadata": {},
   "source": [
    "Now let's login to our W&B workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "%env WANDB_NOTEBOOK_NAME = $Fine_tune_Mistral_with_ORPO\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c19c5",
   "metadata": {},
   "source": [
    "### Load the Model and Tokenizer for LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138ca7f",
   "metadata": {},
   "source": [
    "In the following, we will load the Mistral 7B v0.3 model in 4-bit precision using bitsandbytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb208d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = './model'\n",
    "model_id = 'mistralai/Mistral-7B-v0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba784f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e62f94",
   "metadata": {},
   "source": [
    "### Loading Checks\n",
    "After loading the model, it's crucial to ensure that all parameters are correctly placed on the GPU and that none are overflowing onto the CPU. This can be particularly important for large models where memory management is critical.\n",
    "\n",
    "To verify this, you can iterate through the model's named parameters and check their device type. If any parameter is on the CPU (indicated by the device type 'meta'), it will be printed out.\n",
    "\n",
    "Here is the code to perform this check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91076218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are no parameters overflowing onto cpu (meta).\n",
    "for n, p in model.named_parameters():\n",
    "    if p.device.type=='meta':\n",
    "        print(f\"{n} is on meta!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86353cc",
   "metadata": {},
   "source": [
    "## Prepare for LoRA fine-tuning\n",
    "\n",
    "Before starting the LoRA (Low-Rank Adaptation) fine-tuning process, it's essential to understand which parameters in your model are trainable and which are not. This helps in ensuring that only the desired parameters are updated during training, which is crucial for efficient and effective fine-tuning.\n",
    "\n",
    "To achieve this, you can use the following function to print the number of trainable parameters in the model and list which parameters are trainable and which are not.\n",
    "\n",
    "Here is the code to perform this check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainablöe parameters in the model and lists which parameters\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    non_trainable_params = 0\n",
    "    all_params = 0\n",
    "\n",
    "    print(\"Trainable Parameters\")\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            print(f\" {name}\")\n",
    "        else:\n",
    "            non_trainable_params += param.numel()\n",
    "\n",
    "    print(\"\\nNon-Trainable Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\" {name}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2634ff",
   "metadata": {},
   "source": [
    "Let's take a look a the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403062c0",
   "metadata": {},
   "source": [
    "## Setting Up LoRA Fine-Tuning\n",
    "To prepare your model for LoRA (Low-Rank Adaptation) fine-tuning, you need to configure it properly. This involves setting up the LoRA configuration.\n",
    "Here's a brief overview of the parameters and their best settings:\n",
    "\n",
    "1. **`r`**: This parameter controls the rank of the low-rank adaptation matrices. It's suggested to choose a value greater than 0, with common choices being 8, 16, 32, 64, or 128. The best setting depends on the specific use case and computational resources, but a good starting point is 8 or 16.\n",
    "\n",
    "2. **`lora_alpha`**: This parameter scales the magnitude of the LoRA update. A higher value can lead to more significant changes in the model's behavior. The best setting is typically 32, as used in the code.\n",
    "\n",
    "3. **`target_modules`**: This list specifies which modules in the model should be fine-tuned. The best settings include key modules like `\"q_proj\"`, `\"k_proj\"`, `\"v_proj\"`, `\"o_proj\"`, `\"gate_proj\"`, `\"up_proj\"`, and `\"down_proj\"`. If the task involves chat fine-tuning, it's also beneficial to set `\"lm_head\"` (language model head) as trainable.\n",
    "\n",
    "6. **`use_gradient_checkpointing`**: This parameter activates gradient checkpointing to conserve memory. It is managed by Unsloth, which offloads input and output embeddings to disk, thereby saving VRAM.\n",
    "\n",
    "7. **`random_state`**: This parameter sets the seed for random number generation, ensuring reproducibility. The best setting is any integer value; in the code, it's set to 3407.\n",
    "\n",
    "8. **`use_rslora`**: This parameter activates RSLoRA, which adjusts the scaling factor of LoRA adapters to be proportional to 1/√r instead of 1/r. This adjustment enhances the stability of learning, particularly for higher adapter ranks, and improves fine-tuning performance as the rank increases.\n",
    "\n",
    "These settings provide a good starting point for fine-tuning a language model using PEFT. However, the optimal settings may vary depending on the specific task and dataset, so some experimentation may be necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f9b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    lora_alpha = 32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n",
    "        \n",
    "    ],\n",
    "    \n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b4873f",
   "metadata": {},
   "source": [
    "##### Set up Tokenizer and Padding\n",
    "\n",
    "Before starting the fine-tuning process, it's essential to configure the tokenizer and set up padding correctly. This ensures that the model can handle input sequences efficiently and that special tokens are properly managed.\n",
    "\n",
    "Here is a step-by-step guide to setting up the tokenizer and padding:\n",
    "\n",
    "1. Inspect the Tokenizer: Print out the tokenizer details, including the vocabulary size, beginning-of-sequence (BOS) token, end-of-sequence (EOS) token, and chat template.\n",
    "\n",
    "2. Optionally Set the Chat Template Manually: If needed, you can manually set the chat template. This is useful for ensuring that the conversation starts correctly depending on the initial message role.\n",
    "\n",
    "3. Apply the Chat Template: Use the chat template to format a list of messages.\n",
    "\n",
    "4. Set the Pad Token: Determine the appropriate pad token based on the tokenizer's vocabulary and set it accordingly.\n",
    "\n",
    "5. Update the Model Configuration: Ensure that the model and its configuration are updated with the correct pad token ID.\n",
    "\n",
    "Here is the code to perform these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd8dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56490c77-df63-4110-a4f4-75ae3f8b03e3",
   "metadata": {},
   "source": [
    "A custom chat template for a tokenizer, specifically designed for Llama/Mistral models is created. This template ensures that conversations start correctly by conditionally adding a beginning-of-sequence token (`bos_token`) if the first message is not from the assistant. This is particularly useful when formatting chosen and rejected responses separately, as it avoids adding an extra `bos_token` before the response.\n",
    "\n",
    "The template is defined using a Jinja-like syntax, which iterates through the messages and formats them based on their roles (`user` or `assistant`). For user messages, it wraps the content with `[INST]` and `[/INST]` tags, while for assistant messages, it appends an end-of-sequence token (`eos_token`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf5a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "# Test chat template\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'write a quick sorf algorithm in python.'},\n",
    "    {'role': 'assistant', 'content': 'here you are.'},\n",
    "    {'role': 'user', 'content': 'great.'},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5346909",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the pad token to <pad>, if not <|pad|>, if not <unk> if <unk>\n",
    "if '<pad>' in tokenizer.get_vocab():\n",
    "    print('<pad> token is is in the tokenizer. Usinh <pad> for pad')\n",
    "    #Set the pad token\n",
    "    tokenizer.pad_token = '<pad>'\n",
    "elif '<|pad|>' in tokenizer.get_vocab():\n",
    "    print('<|pad|> token is in the tokenizer. Using for <|pad|> for pad')\n",
    "    # Set the pad token\n",
    "    tokenizer.pad_token = '<|pad|>'\n",
    "elif '<unk>' in tokenizer.get_vocab():\n",
    "    print('<unk> token is in the tokenizer. Using for <unk> for pad')\n",
    "    # Set the pad token\n",
    "    tokenizer.pad_token = '<unk>'\n",
    "else:\n",
    "    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dcd82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update pad token id in model and its config\n",
    "model.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Check if they are equal\n",
    "assert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Special tokens map:', tokenizer.special_tokens_map)\n",
    "print('All special tokens:', tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310513b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccbaf5",
   "metadata": {},
   "source": [
    "## Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)\n",
    "\n",
    "When fine-tuning a model for chat applications, it's often beneficial to set specific layers to be trainable, especially if you are changing the chat template. This ensures that the model can adapt to the new input format more effectively.\n",
    "\n",
    "Here is a step-by-step guide to setting specific layers to trainable:\n",
    "\n",
    "1. Identify Trainable Parameters: Create a list of the names of the layers you want to set as trainable.\n",
    "\n",
    "2. Set Modules to Trainable: Iterate through the model's parameters and set the requires_grad attribute to True for the specified layers. Optionally, set the rest to False.\n",
    "\n",
    "3. Create a Dictionary of Trainable Parameters: Collect the trainable parameters into a dictionary for easy access.\n",
    "\n",
    "4. Convert to State Dict Format: Convert the trainable parameters to a state dictionary format, which can be useful for saving and loading the model's state.\n",
    "\n",
    "5. Print Trainable Parameters: Use a function to print the trainable parameters to verify the setup.\n",
    "\n",
    "Here is the code to perform these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57716653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold the names of the trainable parameters\n",
    "trainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n",
    "\n",
    "# Set modules to be trainable\n",
    "for n, p in model.named_parameters():\n",
    "    if any(k in n for k in trainable_params_names):\n",
    "        p.requires_grad_(True)\n",
    "    else:\n",
    "        p.requires_grad_(False) # Optional: Set the rest to be trainable\n",
    "\n",
    "# Make a dictionary of trainable parameters\n",
    "trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "# Convert trainable_params to state_dict format\n",
    "trainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e683a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826704e",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Dataset for Fine-Tuning\n",
    "In this code, we will guide you through the process of loading and preparing a dataset for fine-tuning a model. This involves loading the dataset, shuffling it, splitting it into training and test sets, and applying a specific template to format the data correctly.\n",
    "\n",
    "Here is a step-by-step guide to loading and preparing the dataset:\n",
    "\n",
    "1. Import Necessary Libraries: Import the required libraries, including json for handling JSON data and datasets for loading and manipulating the dataset.\n",
    "\n",
    "2. Define Dataset Parameters: Set the dataset name and the maximum number of samples to use. If you want to use the full dataset, set max_num_samples to None.\n",
    "\n",
    "3. Define the build_dataset Function: Create a function called build_dataset that takes a tokenizer, dataset name, cache directory, maximum number of samples, and other parameters as inputs. This function will load the dataset, shuffle it, split it into training and test sets, and apply a specific template to format the data.\n",
    "\n",
    "4. Load the Dataset: Use the load_dataset function from the datasets library to load the dataset. The dataset is split based on the max_num_samples parameter.\n",
    "\n",
    "5. Shuffle the Dataset: If max_num_samples is not None, shuffle the dataset to ensure randomness.\n",
    "\n",
    "6. Split the Dataset: Determine the number of test samples and split the dataset into training and test sets using the train_test_split method.\n",
    "\n",
    "7. Apply the DPO Template: Define a function called apply_dpo_template that formats the data according to the DPO (Direct Preference Optimization) template. This function extracts the necessary information from the dataset and applies the chat template using the tokenizer.\n",
    "\n",
    "8. Map the Dataset: Use the map method to apply the apply_dpo_template function to the dataset. Remove the original columns and rename the new columns accordingly.\n",
    "\n",
    "9. Return the Dataset: Return the training and test datasets.\n",
    "\n",
    "10. Check the Chat Template: Ensure that the chat template is correctly applied and that special tokens are not included when tokenizing the responses.\n",
    "\n",
    "Here is the code to perform these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e1646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepared with the help of code from: https://github.com/xfactlab/orpo/blob/main...\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "dataset_name = 'llmat/dpo-orpo-mix-38k-balanced' # Ensure this is defined\n",
    "\n",
    "max_num_samples = None # Set to None to use the full dataset\n",
    "#max_num_samples = 10000 # set to None to use the full dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "def build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_size_ratio=0.1):\n",
    "    # Determin the split specification based on max_num samples\n",
    "    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n",
    "\n",
    "    # Load the dataset\n",
    "    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    if max_num_samples is not None:\n",
    "        full_data = full_data.shuffle(seed=42)\n",
    "    else:\n",
    "        full_data = full_data\n",
    "\n",
    "    # Determine the number of test samples\n",
    "    num_total_samples = len(full_data)\n",
    "    test_size = int(test_size_ratio * num_total_samples)\n",
    "\n",
    "    # Randomly split the data into training and test sets\n",
    "    dataset = full_data.train_test_split(test_size=test_size)\n",
    "\n",
    "    column_names = list(dataset['train'].features)\n",
    "\n",
    "    def apply_dpo_template(example):\n",
    "        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n",
    "        if all(k in example.keys() for k in ('chosen', 'rejected')):\n",
    "            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n",
    "            # We therefore need to extract the N-1 turns to form the prompt\n",
    "            prompt_messages = example['chosen'][:-1]\n",
    "            example['messages'] = example['chosen']\n",
    "\n",
    "            # Now we extract the final turn to define chosen/rejected responses\n",
    "            chosen_messages = example['chosen'][-1:]\n",
    "            rejected_messages = example['rejected'][-1:]\n",
    "            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n",
    "                desc='Formatting comparisons with prompt template',)\n",
    "\n",
    "    for split in ['train', 'test']:\n",
    "        dataset[split] = dataset[split].rename_columns(\n",
    "            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected', 'messages': 'messages'}\n",
    "        )\n",
    "\n",
    "    return dataset['train'], dataset['test']\n",
    "\n",
    "# Assuming 'tokenizer' and 'dataset_name' are already defined\n",
    "train, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n",
    "\n",
    "# Check the chat template!!! <s> should not be included when tokenizing the respones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fcf7c2",
   "metadata": {},
   "source": [
    "After preparing and formatting your dataset for fine-tuning, it's crucial to inspect the data to ensure that it has been correctly processed. This step helps you verify that the prompt, chosen, rejected, and messages fields are properly formatted and contain the expected information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prompt:', train['prompt'][0])\n",
    "print('\\n\\nChosen:', train['chosen'][0])\n",
    "print('\\n\\nRejected:', train['rejected'][0])\n",
    "print('\\n\\nMessages (incl. prompt):', train['messages'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4114fe98",
   "metadata": {},
   "source": [
    "## Setting Up and Running Training\n",
    "In this tutorial, we will go through the process of setting up and running the training for your model. This includes configuring training parameters, creating a custom logging callback, and initiating the training process.\n",
    "\n",
    "Here is a step-by-step guide to setting up and running the training:\n",
    "\n",
    "1. Set Training Parameters: Define the training parameters such as the model name, number of epochs, gradient accumulation steps, batch size, and the directory to save the results.\n",
    "\n",
    "2. Create a Custom Logging Callback: Implement a custom callback to log training metrics to a file. This callback will write the training and evaluation loss to a log file and save the trainable parameters at checkpoint steps.\n",
    "\n",
    "3. Initialize the Logging Callback: Create an instance of the custom logging callback with the specified log file path.\n",
    "\n",
    "Here is the code to perform these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "epochs=1\n",
    "grad_accum=4\n",
    "batch_size=8\n",
    "fine_tune_tag='ORPO'\n",
    "save_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Custom callback to log metrics\n",
    "class LoggingCallback(transformers.TrainerCallback):\n",
    "    def __init__(self, log_file_path):\n",
    "        self.log_file_path = log_file_path\n",
    "\n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        with open(self.log_file_path, 'a') as f:\n",
    "            if 'loss' in logs:\n",
    "                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n",
    "                if 'eval_loss' in logs:\n",
    "                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n",
    "                f.flush()  # Force flush the buffered data to file\n",
    "\n",
    "        # Check if the current step is a checkpoint step\n",
    "        if state.global_step % int(args.save_steps) == 0:\n",
    "            # Check if the last checkpoint path exists\n",
    "            if state.best_model_checkpoint:\n",
    "                checkpoint_dir = state.best_model_checkpoint\n",
    "            else:\n",
    "                # If not, construct the checkpoint directory path\n",
    "                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n",
    "\n",
    "            # Ensure the checkpoint directory exists\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save trainable params in the checkpoint directory\n",
    "            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n",
    "            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n",
    "            torch.save(current_trainable_params_state_dict, file_path)\n",
    "\n",
    "# Log file path\n",
    "cache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\n",
    "log_file_path = os.path.join(cache_dir, 'training_logs.txt')\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "logging_callback = LoggingCallback(log_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d7332",
   "metadata": {},
   "source": [
    "## Setting Up ORPO Training\n",
    "In this section, we'll walk through setting up and training a model using the ORPOTrainer from the trl library.\n",
    "\n",
    "I trained the model on the entire dataset (38k samples) using an RTX 4090 GPU (24 GB of VRAM). The training took 7 hours and 35 minutes. You can use smaller GPUs with less VRAM and a smaller batch size. In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like 'max_num_samples = 10000' to only load 10k samples.\n",
    "\n",
    "### Configure ORPO\n",
    "Define the configuration for the ORPO training. This configuration includes various hyperparameters and settings for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8852ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import ORPOTrainer, ORPOConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "orpo_config = ORPOConfig(\n",
    "    beta=0.2,\n",
    "    save_steps=500, \n",
    "    logging_steps=1,\n",
    "    num_train_epochs=epochs,\n",
    "    output_dir=save_dir,\n",
    "    evaluation_strategy='steps', \n",
    "    do_eval=True,\n",
    "    eval_steps=0.2,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    log_level='debug',\n",
    "    optim='paged_adamw_8bit',\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=0.03,\n",
    "    learning_rate=1e-4, \n",
    "\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    "\n",
    "    max_completion_length=1024,\n",
    "    remove_unused_columns=True,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd425bd",
   "metadata": {},
   "source": [
    "### Initialize ORPOTrainer\n",
    "Create an instance of ORPOTrainer with the model, datasets, tokenizer, and the configuration defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "orpo_trainer = ORPOTrainer(\n",
    "    model,\n",
    "    args=orpo_config,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    callbacks=[logging_callback], # Add custom callback here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be72cd3",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Set the model configuration to avoid cache warnings and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b8170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False # silence the warnings\n",
    "orpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d93f7",
   "metadata": {},
   "source": [
    "## Plotting Training and Evaluation Losses with Matplotlib\n",
    "\n",
    "After training your model, it's important to visualize the training and evaluation losses to understand how well your model is performing and to identify any potential issues. Visualizing the losses can help you diagnose problems such as overfitting or underfitting and make informed decisions about further training or model adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e300246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to hold training and evaluation losses and steps\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "train_steps = []\n",
    "eval_steps = []\n",
    "\n",
    "# Populate the lists from the log history\n",
    "for entry in orpo_trainer.state.log_history:\n",
    "    if 'loss' in entry:\n",
    "        train_losses.append(entry['loss'])\n",
    "        train_steps.append(entry['step'])\n",
    "    if 'eval_loss' in entry:\n",
    "        eval_losses.append(entry['eval_loss'])\n",
    "        eval_steps.append(entry['step'])\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(train_steps, train_losses, label='Train Loss')\n",
    "plt.plot(eval_steps, eval_losses, label='Eval Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32140da",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/orpo/orpo_loss.png\" alt=\"Image\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356451b5-fd3d-405f-ac80-1dacba228c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-05T11:33:25.353604Z",
     "iopub.status.busy": "2024-08-05T11:33:25.353223Z",
     "iopub.status.idle": "2024-08-05T11:33:25.357231Z",
     "shell.execute_reply": "2024-08-05T11:33:25.356533Z",
     "shell.execute_reply.started": "2024-08-05T11:33:25.353585Z"
    }
   },
   "source": [
    "Let’s now check the W&B plots. While the loss goes down, we also can see that the difference between the chosen and rejects answers becomes clearer.\n",
    "\n",
    "<center>\n",
    "    <img src=\"images/orpo/orpo_metrics.png\" alt=\"Image\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba08fb2",
   "metadata": {},
   "source": [
    "## Merging Adapters and Saving the Model to Hugging Face Hub\n",
    "As a last step, we merge the adapters with the original model using 16-bit precision to enhance quality. Initially, we save it locally in the \"model\" directory before uploading it to the Hugging Face Hub. The trained model is available at llmat/Mistral-v0.3-7B-ORPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ebd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n",
    "model.push_to_hub_merged(\"llmat/Mistral-v0.3-7B-ORPO\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d6f5a-3724-4f58-a6b4-698e4d35040c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This article presented a thorough overview of ORPO fine-tuning and its practical application to a Mistral v0.3 7B model. Utilizing QLoRA's efficient memory management, we successfully fine-tuned a 7B LLM on a high-quality dataset with minimal GPU resources.\n",
    "\n",
    "I hope you found this guide helpful. If you liked this article, follow me on Hugging Face [@llmat](https://huggingface.co/llmat). Best of luck with your model fine-tuning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
