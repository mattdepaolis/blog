{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "aliases:\n",
        "- /decoding/\n",
        "- 2022-06-07-Decoding_strategies.ipynb\n",
        "categories:\n",
        "- Large Language Models\n",
        "colab: <a href=\"https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing\"><img src=\"images/colab.png\" alt=\"Open In Colab\"></a>\n",
        "date: '2023-06-07'\n",
        "image: /images/decoding/thumbnail.jpg\n",
        "title: \"Decoding Strategies in Large Language Models\"\n",
        "subtitle: A Guide to Text Generation From Beam Search to Nucleus Sampling\n",
        "twitter-card:\n",
        "    - title: \"Decoding Strategies in Large Language Models\"\n",
        "    - description: A Guide to Text Generation From Beam Search to Nucleus Sampling\n",
        "    - image: /images/decoding/thumbnail.jpg\n",
        "    - card-style: summary\n",
        "    - creator: \"@maximelabonne\"\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4-QXN_fZ81rH"
      },
      "source": [
        "<center><img src=\"/images/decoding/thumbnail.jpg\"></center>\n",
        "\n",
        "::: {.column-margin}\n",
        "Find many more architectures and applications using graph neural networks in my book, [**Hands-On Graph Neural Networks**](https://mlabonne.github.io/blog/book.html) üëá\n",
        "<a href=\"https://packt.link/a/9781804617526\"><img src=\"/images/gnnbook/cover.png\" alt=\"Hands-On Graph Neural Networks Using Python\" id=\"gnn-book\"></a>\n",
        ":::\n",
        "\n",
        "In the fascinating world of large language models (LLMs), much attention is given to model architectures, data processing, and optimization. However, decoding strategies like beam search, which play a crucial role in text generation, are often overlooked. In this article, we will explore how LLMs generate text by delving into the mechanics of greedy search and beam search, as well as sampling techniques with top-k and nucleus sampling.\n",
        "\n",
        "By the conclusion of this article, you'll not only understand these decoding strategies thoroughly but also be familiar with how to handle important hyperparameters like temperature, `num_beams`, `top_k`, and `top_p`.\n",
        "\n",
        "The code for this article can be found on [GitHub](https://github.com/mlabonne/how-to-data-science/blob/main/Decoding_Strategies_in_Large_Language%C2%A0Models.ipynb) and [Google Colab](https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing) for reference and further exploration.\n",
        "\n",
        "## üìö Background\n",
        "\n",
        "To kick things off, let's start with an example. We'll feed the text \"I have a dream\" to a GPT-2 model and ask it to generate the next five tokens (words or subwords)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LS5sCUPwzaD",
        "outputId": "00f83f4a-ff44-4eff-9e2b-6fcf2b0b32eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: I have a dream of being a doctor.\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model.eval()\n",
        "\n",
        "text = \"I have a dream\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
        "\n",
        "outputs = model.generate(input_ids, max_length=len(input_ids.squeeze())+5)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Generated text: {generated_text}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J9K6MFoJwyuA"
      },
      "source": [
        "The sentence \"I have a dream of being a doctor\" appears to have been generated by GPT-2. However, GPT-2 didn't *exactly* produce this sentence.\n",
        "\n",
        "There's a common misconception that LLMs like GPT-2 directly produce text. This isn't the case. Instead, LLMs calculate logits, which are scores assigned to every possible token in their vocabulary. To simplify, here's an illustrative breakdown of the process:\n",
        "\n",
        ":::{.column-page}\n",
        "<center><img src=\"/images/decoding/llm_flowchart.png\"></center>\n",
        ":::\n",
        "\n",
        "The tokenizer, [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) in this instance, translates each token in the input text into a corresponding token ID. Then, GPT-2 uses these token IDs as input and tries to predict the next most likely token. Finally, the model generates logits, which are converted into probabilities using a softmax function.\n",
        "\n",
        "For example, the model assigns a probability of 17% to the token for \"of\" being the next token after \"I have a dream\". This output essentially represents a ranked list of potential next tokens in the sequence. More formally, we denote this probability as $P(\\text{of } | \\text{ I have a dream}) = 17%$.\n",
        "\n",
        "Autoregressive models like GPT predict the next token in a sequence based on the preceding tokens. Consider a sequence of tokens $w = (w_1, w_2, \\ldots, w_t)$. The joint probability of this sequence $P(w)$ can be broken down as:\n",
        "\n",
        "\\begin{align}\n",
        "P(w) &= P(w_1, w_2, \\ldots, w_t) \\\\\n",
        "     &= P(w_1) P(w_2 | w_1) P(w_3 | w_2, w_1) \\ldots P(w_t | w_1, \\ldots, w_{t-1}) \\\\\n",
        "     &= \\prod_{i=1}^t P(w_i | w_1, \\dots, w_{i-1}).\n",
        "\\end{align}\n",
        "\n",
        "For each token $w_i$ in the sequence, $P(w_i | w_1, \\ldots, w_{i-1})$ represents the conditional probability of $w_i$ given all the preceding tokens $(w_1, \\ldots, w_{i-1})$. GPT-2 calculates this conditional probability for each of the 50,257 tokens in its vocabulary.\n",
        "\n",
        "This leads to the question: how do we use these probabilities to generate text? This is where decoding strategies, such as greedy search and beam search, come into play.\n",
        "\n",
        "## üèÉ‚Äç‚ôÇÔ∏è Greedy Search\n",
        "\n",
        "Greedy search is a decoding method that takes the most probable token at each step as the next token in the sequence. To put it simply, it only retains the most likely token at each stage, discarding all other potential options. Using our example:\n",
        "\n",
        "* **Step 1**: Input: \"I have a dream\" ‚Üí Most likely token: \" of\"\n",
        "* **Step 2**: Input: \"I have a dream of\" ‚Üí Most likely token: \" being\"\n",
        "* **Step 3**: Input: \"I have a dream of being\" ‚Üí Most likely token: \" a\"\n",
        "* **Step 4**: Input: \"I have a dream of being a\" ‚Üí Most likely token: \" doctor\"\n",
        "* **Step 5**: Input: \"I have a dream of being a doctor\" ‚Üí Most likely token: \".\"\n",
        "\n",
        "While this approach might sound intuitive, it's important to note that the greedy search is short-sighted:  it only considers the most probable token at each step without considering the overall effect on the sequence. This property makes it fast and efficient as it doesn't need to keep track of multiple sequences, but it also means that it can miss out on better sequences that might have appeared with slightly less probable next tokens.\n",
        "\n",
        "Next, let's illustrate the greedy search implementation using `graphviz` and `networkx`. We select the ID with the highest score, compute its log probability (we take the log to simplify calculations), and add it to the tree. We'll repeat this process for five tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm_AiUhK8yrP",
        "outputId": "6bcfee39-c8ec-4ca7-e81c-37a0af6bf5b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: I have a dream of being a doctor.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def get_log_prob(logits, token_id):\n",
        "    # Compute the softmax of the logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    log_probabilities = torch.log(probabilities)\n",
        "    \n",
        "    # Get the log probability of the token\n",
        "    token_log_probability = log_probabilities[token_id].item()\n",
        "    return token_log_probability\n",
        "\n",
        "def greedy_search(input_ids, node, length=5):\n",
        "    if length == 0:\n",
        "        return input_ids\n",
        "\n",
        "    outputs = model(input_ids)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "    # Get the predicted next sub-word (here we use top-k search)\n",
        "    logits = predictions[0, -1, :]\n",
        "    token_id = torch.argmax(logits).unsqueeze(0)\n",
        "\n",
        "    # Compute the score of the predicted token\n",
        "    token_score = get_log_prob(logits, token_id)\n",
        "\n",
        "    # Add the predicted token to the list of input ids\n",
        "    new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0)], dim=-1)\n",
        "\n",
        "    # Add node and edge to graph\n",
        "    next_token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "    current_node = list(graph.successors(node))[0]\n",
        "    graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n",
        "    graph.nodes[current_node]['token'] = next_token + f\"_{length}\"\n",
        "\n",
        "    # Recursive call\n",
        "    input_ids = greedy_search(new_input_ids, current_node, length-1)\n",
        "    \n",
        "    return input_ids\n",
        "\n",
        "# Parameters\n",
        "length = 5\n",
        "beams = 1\n",
        "\n",
        "# Create a balanced tree with height 'length'\n",
        "graph = nx.balanced_tree(1, length, create_using=nx.DiGraph())\n",
        "\n",
        "# Add 'tokenscore', 'cumscore', and 'token' attributes to each node\n",
        "for node in graph.nodes:\n",
        "    graph.nodes[node]['tokenscore'] = 100\n",
        "    graph.nodes[node]['token'] = text\n",
        "\n",
        "# Start generating text\n",
        "output_ids = greedy_search(input_ids, 0, length=length)\n",
        "output = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "print(f\"Generated text: {output}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lbke4ppTn6y3"
      },
      "source": [
        "Our greedy search generates the same text as the one from the transformers library: \"I have a dream of being a doctor.\" Let's visualize the tree we created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NeMNyl5maqTc",
        "outputId": "02fd01d3-1d78-4e14-ddf1-772a13a9a97f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "def plot_graph(graph, length, beams, score):\n",
        "    fig, ax = plt.subplots(figsize=(3+1.2*beams**length, max(5, 2+length)), dpi=300, facecolor='white')\n",
        "\n",
        "    # Create positions for each node\n",
        "    pos = nx.nx_agraph.graphviz_layout(graph, prog=\"dot\")\n",
        "\n",
        "    # Normalize the colors along the range of token scores\n",
        "    if score == 'token':\n",
        "        scores = [data['tokenscore'] for _, data in graph.nodes(data=True) if data['token'] is not None]\n",
        "    elif score == 'sequence':\n",
        "        scores = [data['sequencescore'] for _, data in graph.nodes(data=True) if data['token'] is not None]\n",
        "    vmin = min(scores)\n",
        "    vmax = max(scores)\n",
        "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
        "    cmap = LinearSegmentedColormap.from_list('rg', [\"r\", \"y\", \"g\"], N=256) \n",
        "\n",
        "    # Draw the nodes\n",
        "    nx.draw_networkx_nodes(graph, pos, node_size=2000, node_shape='o', alpha=1, linewidths=4, \n",
        "                          node_color=scores, cmap=cmap)\n",
        "\n",
        "    # Draw the edges\n",
        "    nx.draw_networkx_edges(graph, pos)\n",
        "\n",
        "    # Draw the labels\n",
        "    if score == 'token':\n",
        "        labels = {node: data['token'].split('_')[0] + f\"\\n{data['tokenscore']:.2f}%\" for node, data in graph.nodes(data=True) if data['token'] is not None}\n",
        "    elif score == 'sequence':\n",
        "        labels = {node: data['token'].split('_')[0] + f\"\\n{data['sequencescore']:.2f}\" for node, data in graph.nodes(data=True) if data['token'] is not None}\n",
        "    nx.draw_networkx_labels(graph, pos, labels=labels, font_size=10)\n",
        "    plt.box(False)\n",
        "\n",
        "    # Add a colorbar\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    if score == 'token':\n",
        "        fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label='Token probability (%)')\n",
        "    elif score == 'sequence':\n",
        "        fig.colorbar(sm, ax=ax, orientation='vertical', pad=0, label='Sequence score')\n",
        "    plt.show()\n",
        "\n",
        "# Plot graph\n",
        "plot_graph(graph, length, 1.5, 'token')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J16z-Mw295Jx"
      },
      "source": [
        "<center><img src=\"/images/decoding/greedy_tree.png\"></center>\n",
        "\n",
        "In this graph, the top node stores the input token (thus with a 100% probability), while all other nodes represent generated tokens. Although each token in this sequence was the most likely at the time of prediction, \"being\" and \"doctor\" were assigned relatively low probabilities of 9.68% and 2.86%, respectively. This suggests that \"of\", our first predicted token, may not have been the most suitable choice as it led to \"being\", which is quite unlikely.\n",
        "\n",
        "In the following section, we'll explore how beam search can address this problem.\n",
        "\n",
        "## ‚öñÔ∏è Beam Search\n",
        "\n",
        "Unlike greedy search, which only considers the next most probable token, beam search takes into account the $n$ most likely tokens, where $n$ represents the number of beams. This procedure is repeated until a predefined maximum length is reached or an end-of-sequence token appears. At this point, the sequence (or \"beam\") with the highest overall score is chosen as the output.\n",
        "\n",
        "We can adapt the previous function to consider the $n$ most probable tokens instead of just one. Here, we'll maintain the sequence score $\\log P(w)$, which is the cumulative sum of the log probability of every token in the beam. We normalize this score by the sequence length to prevent bias towards longer sequences (this factor can be adjusted). Once again, we'll generate five additional tokens to complete the sentence \"I have a dream.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0db36f1e0c1c4801a07809908ed3a6e6",
            "b5287e0c25b54e4f97f90cc13c02419b",
            "0086b6ae311c46339958771a825ba594",
            "222c73051b33432abaab69d6925ebb61",
            "facf596299c5422999763eeec6b4ebb2",
            "109a034f44b94b0c8a2fe764aaab0213",
            "8b2eeb05b70946999046d36d42caf435",
            "b7e2b8a6e90c456ab98978e93952649a",
            "daf267adb2604c65b5c8b2b93eae5e8d",
            "8b43bebd8b934dc5b26e1ea20f38ede3",
            "aae4c3664807436595b1a730d5b6f3fb"
          ]
        },
        "id": "DTDkrVdutCk1",
        "outputId": "546a2da0-95b6-40aa-e975-1dba2b8a08a7"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def greedy_sampling(logits, beams):\n",
        "    return torch.topk(logits, beams).indices\n",
        "    \n",
        "def beam_search(input_ids, node, bar, length, beams, sampling, temperature=0.1):\n",
        "    if length == 0:\n",
        "        return None\n",
        "\n",
        "    outputs = model(input_ids)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "    # Get the predicted next sub-word (here we use top-k search)\n",
        "    logits = predictions[0, -1, :]\n",
        "\n",
        "    if sampling == 'greedy':\n",
        "        top_token_ids = greedy_sampling(logits, beams)\n",
        "    elif sampling == 'top_k':\n",
        "        top_token_ids = top_k_sampling(logits, temperature, 20, beams)\n",
        "    elif sampling == 'nucleus':\n",
        "        top_token_ids = nucleus_sampling(logits, temperature, 0.5, beams)\n",
        "\n",
        "    for j, token_id in enumerate(top_token_ids):\n",
        "        bar.update(1)\n",
        "\n",
        "        # Compute the score of the predicted token\n",
        "        token_score = get_log_prob(logits, token_id)\n",
        "        cumulative_score = graph.nodes[node]['cumscore'] + token_score\n",
        "\n",
        "        # Add the predicted token to the list of input ids\n",
        "        new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
        "\n",
        "        # Add node and edge to graph\n",
        "        token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "        current_node = list(graph.successors(node))[j]\n",
        "        graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n",
        "        graph.nodes[current_node]['cumscore'] = cumulative_score\n",
        "        graph.nodes[current_node]['sequencescore'] = 1/(len(new_input_ids.squeeze())) * cumulative_score\n",
        "        graph.nodes[current_node]['token'] = token + f\"_{length}_{j}\"\n",
        "\n",
        "        # Recursive call\n",
        "        beam_search(new_input_ids, current_node, bar, length-1, beams, sampling, 1)\n",
        "\n",
        "# Parameters\n",
        "length = 5\n",
        "beams = 2\n",
        "\n",
        "# Create a balanced tree with height 'length' and branching factor 'k'\n",
        "graph = nx.balanced_tree(beams, length, create_using=nx.DiGraph())\n",
        "bar = tqdm(total=len(graph.nodes))\n",
        "\n",
        "# Add 'tokenscore', 'cumscore', and 'token' attributes to each node\n",
        "for node in graph.nodes:\n",
        "    graph.nodes[node]['tokenscore'] = 100\n",
        "    graph.nodes[node]['cumscore'] = 0\n",
        "    graph.nodes[node]['sequencescore'] = 0\n",
        "    graph.nodes[node]['token'] = text\n",
        "\n",
        "# Start generating text\n",
        "beam_search(input_ids, 0, bar, length, beams, 'greedy', 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3d-MCIqOEuf9"
      },
      "source": [
        "The function computes the scores for 63 tokens and beams^length = 5¬≤ = 25 possible sequences. In our implementation, all the information is stored in the graph. Our next step is to extract the best sequence.\n",
        "\n",
        "First, we identify the leaf node with the highest sequence score. Next, we find the shortest path from the root to this leaf. Every node along this path contains a token from the optimal sequence. Here's how we can implement it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xusUHHBa4lt6",
        "outputId": "0961d170-57a4-4246-e4a9-4ea672add768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: I have a dream. I have a dream\n"
          ]
        }
      ],
      "source": [
        "def get_best_sequence(G):\n",
        "    # Create a list of leaf nodes\n",
        "    leaf_nodes = [node for node in G.nodes() if G.out_degree(node)==0]\n",
        "\n",
        "    # Get the leaf node with the highest cumscore\n",
        "    max_score_node = None\n",
        "    max_score = float('-inf')\n",
        "    for node in leaf_nodes:\n",
        "        if G.nodes[node]['sequencescore'] > max_score:\n",
        "            max_score = G.nodes[node]['sequencescore']\n",
        "            max_score_node = node\n",
        "\n",
        "    # Retrieve the sequence of nodes from this leaf node to the root node in a list\n",
        "    path = nx.shortest_path(G, source=0, target=max_score_node)\n",
        "\n",
        "    # Return the string of token attributes of this sequence\n",
        "    sequence = \"\".join([G.nodes[node]['token'].split('_')[0] for node in path])\n",
        "    \n",
        "    return sequence, max_score\n",
        "\n",
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qMmV5Ou35iSF"
      },
      "source": [
        "The best sequence seems to be \"I have a dream. I have a dream,\" which is a common response from GPT-2, even though it may be surprising. To verify this, let's plot the graph.\n",
        "\n",
        "In this visualization, we'll display the sequence score for each node, which represents the score of the sequence up to that point. If the function `get_best_sequence()` is correct, the \"dream\" node in the sequence \"I have a dream. I have a dream\" should have the highest score among all the leaf nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "3gPbd4dw0J9k",
        "outputId": "c56f67a2-dd4c-4f25-ce68-61259bf0590a"
      },
      "outputs": [],
      "source": [
        "# Plot graph\n",
        "plot_graph(graph, length, beams, 'sequence')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmObi_P6bR0"
      },
      "source": [
        "::: {.column-screen}\n",
        "<center><img src=\"/images/decoding/beam_tree.png\"></center>\n",
        ":::\n",
        "\n",
        "Indeed, the \"dream\" token has the highest sequence score with a value of -0.69. Interestingly, we can see the score of the greedy sequence \"I have a dream of being a doctor.\" on the left with a value of -1.16.\n",
        "\n",
        "As expected, the greedy search leads to suboptimal results. But, to be honest, our new outcome is not particularly compelling either. To generate more varied sequences, we'll implement two sampling algorithms: top-k and nucleus.\n",
        "\n",
        "## üé≤ Top-k sampling\n",
        "\n",
        "Top-k sampling is a technique that leverages the probability distribution generated by the language model to **select a token randomly from the k most likely options**.\n",
        "\n",
        "To illustrate, suppose we have $k = 3$ and four tokens: A, B, C, and D, with respective probabilities: $P(A) = 30%$, $P(B) = 15%$, $P(C) = 5%$, and $P(D) = 1%$. In top-k sampling, token D is disregarded, and the algorithm will output A 60% of the time, B 30% of the time, and C 10% of the time. This approach ensures that we prioritize the most probable tokens while introducing an element of randomness in the selection process.\n",
        "\n",
        "Another way of introducing randomness is the concept of temperature. The temperature $T$ is a parameter that ranges from 0 to 1, which affects the probabilities generated by the softmax function, making the most likely tokens more influential. In practice, it simply consists of dividing the input logits by a value we call temperature:\n",
        "\n",
        "$$\\text{softmax}(x_i) = \\frac{e^{x_i / T}}{\\sum_{j} e^{x_j / T}}$$\n",
        "\n",
        "Here is a chart that demonstrates the impact of temperature on the probabilities generated for a given set of input logits [1.5, -1.8, 0.9, -3.2]. We've plotted three different temperature values to observe the differences.\n",
        "\n",
        "::: {.column-page}\n",
        "<center><img src=\"/images/decoding/temperature.png\"></center>\n",
        ":::\n",
        "\n",
        "A temperature of 1.0 is equivalent to a default softmax with no temperature at all. On the other hand, a low temperature setting (0.1) significantly alters the probability distribution. This is commonly used in text generation to control the level of \"creativity\" in the generated output. By adjusting the temperature, we can influence the extent to which the model produces more diverse or predictable responses.\n",
        "\n",
        "Let's now implement the top k sampling algorithm. We'll use it in the `beam_search()` function by providing the \"top_k\" argument. To illustrate how the algorithm works, we will also plot the probability distributions for `top_k=20`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsDJWQs-tav"
      },
      "outputs": [],
      "source": [
        "def plot_prob_distribution(probabilities, next_tokens, sampling, potential_nb, total_nb=50):\n",
        "    # Get top k tokens\n",
        "    top_k_prob, top_k_indices = torch.topk(probabilities, total_nb)\n",
        "    top_k_tokens = [tokenizer.decode([idx]) for idx in top_k_indices.tolist()]\n",
        "\n",
        "    # Get next tokens and their probabilities\n",
        "    next_tokens_list = [tokenizer.decode([idx]) for idx in next_tokens.tolist()]\n",
        "    next_token_prob = probabilities[next_tokens].tolist()\n",
        "\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(0.4*total_nb, 5), dpi=300, facecolor='white')\n",
        "    plt.rc('axes', axisbelow=True)\n",
        "    plt.grid(axis='y', linestyle='-', alpha=0.5)\n",
        "    if potential_nb < total_nb:\n",
        "        plt.axvline(x=potential_nb-0.5, ls=':', color='grey', label='Sampled tokens')\n",
        "    plt.bar(top_k_tokens, top_k_prob.tolist(), color='blue')\n",
        "    plt.bar(next_tokens_list, next_token_prob, color='red', label='Selected tokens')\n",
        "    plt.xticks(rotation=45, ha='right', va='top')\n",
        "    plt.gca().spines['top'].set_visible(False)\n",
        "    plt.gca().spines['right'].set_visible(False)\n",
        "    if sampling == 'top_k':\n",
        "        plt.title('Probability distribution of predicted tokens with top-k sampling')\n",
        "    elif sampling == 'nucleus':\n",
        "        plt.title('Probability distribution of predicted tokens with nucleus sampling')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'{sampling}_{time.time()}.png', dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "def top_k_sampling(logits, temperature, top_k, beams, plot=True):\n",
        "    assert top_k >= 1\n",
        "    assert beams <= top_k\n",
        "\n",
        "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "    new_logits = torch.clone(logits)\n",
        "    new_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probabilities = torch.nn.functional.softmax(new_logits / temperature, dim=-1)\n",
        "\n",
        "    # Sample n tokens from the resulting distribution\n",
        "    next_tokens = torch.multinomial(probabilities, beams)\n",
        "\n",
        "    # Plot distribution\n",
        "    if plot:\n",
        "        total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "        plot_prob_distribution(total_prob, next_tokens, 'top_k', top_k)\n",
        "\n",
        "    return next_tokens\n",
        "\n",
        "# Start generating text\n",
        "beam_search(input_ids, 0, bar, length, beams, 'top_k', 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I9qEaYVg83Uz"
      },
      "source": [
        ":::{.column-page}\n",
        "<center><img src=\"/images/decoding/top_k.gif\"></center>\n",
        ":::\n",
        "\n",
        "These plots give a good intuition of how top-k sampling works, with all the potentially selected tokens on the left of the horizontal bar. While the most probable tokens are selected (in red) most of the time, it also allows less likely tokens to be chosen. This offers an interesting tradeoff that can steer a sequence towards a less predictable but more natural-sounding sentence. Now let's print the text it generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUrEPE899TsK",
        "outputId": "6f22009e-1c8d-4716-c7be-0979d2fad33b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: I have a dream job and I want to\n"
          ]
        }
      ],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8aTXx1Jn9TDi"
      },
      "source": [
        "The top-k sampling found a new sequence: \"I have a dream job and I want to\", which feels significantly more natural than \"I have a dream. I have a dream\". We're making progress!\n",
        "\n",
        "Let's see how this decision tree differs from the previous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "qyop3NTp--4G",
        "outputId": "26d46da9-03e0-4b30-f19a-eb5b63ac2d73"
      },
      "outputs": [],
      "source": [
        "# Plot graph\n",
        "plot_graph(graph, length, beams, 'sequence')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qDd_PePe_Itd"
      },
      "source": [
        "::: {.column-screen}\n",
        "<center><img src=\"/images/decoding/top_k_tree.png\"></center>\n",
        ":::\n",
        "\n",
        "You can see how the nodes differ significantly from the previous iteration, making more diverse choices. Although the sequence score of this new outcome might not be the highest (-1.01 instead of -0.69 previously), it's important to remember that higher scores do not always lead to more realistic or meaningful sequences.\n",
        "\n",
        "Now that we've introduced top-k sampling, we have to present the other most popular sampling technique: nucleus sampling.\n",
        "\n",
        "## üî¨ Nucleus sampling\n",
        "\n",
        "Nucleus sampling, also known as top-p sampling, takes a different approach from top-k sampling. Rather than selecting the top $k$ most probable tokens, nucleus sampling chooses a cutoff value $p$ such that the **sum of the probabilities of the selected tokens exceeds $p$**. This forms a \"nucleus\" of tokens from which to randomly choose the next token.\n",
        "\n",
        "In other words, the model examines its top probable tokens in descending order and keeps adding them to the list until the total probability surpasses the threshold $p$. Unlike top-k sampling, the number of tokens included in the nucleus can vary from step to step. This variability often results in a more diverse and creative output, making nucleus sampling popular for tasks such as text generation.\n",
        "\n",
        "To implement the nucleus sampling method, we can use the \"nucleus\" parameter in the `beam_search()` function. In this example, we'll set the value of $p$ to 0.5. To make it easier, we'll include a minimum number of tokens equal to the number of beams. We'll also consider tokens with cumulative probabilities lower than $p$, rather than higher. It's worth noting that while the details may differ, the core idea of nucleus sampling remains the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exsgQ3NQ8oTS"
      },
      "outputs": [],
      "source": [
        "def nucleus_sampling(logits, temperature, p, beams, plot=True):\n",
        "    assert p > 0\n",
        "    assert p <= 1\n",
        "\n",
        "    # Sort the probabilities in descending order and compute cumulative probabilities\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n",
        "    cumulative_probabilities = torch.cumsum(probabilities, dim=-1)\n",
        "\n",
        "    # Create a mask for probabilities that are in the top-p\n",
        "    mask = cumulative_probabilities < p\n",
        "\n",
        "    # If there's not n index where cumulative_probabilities < p, we use the top n tokens instead\n",
        "    if mask.sum() > beams:\n",
        "        top_p_index_to_keep = torch.where(mask)[0][-1].detach().cpu().tolist()\n",
        "    else:\n",
        "        top_p_index_to_keep = beams\n",
        "\n",
        "    # Only keep top-p indices\n",
        "    indices_to_remove = sorted_indices[top_p_index_to_keep:]\n",
        "    sorted_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "    # Sample n tokens from the resulting distribution\n",
        "    probabilities = torch.nn.functional.softmax(sorted_logits / temperature, dim=-1)\n",
        "    next_tokens = torch.multinomial(probabilities, beams)\n",
        "\n",
        "    # Plot distribution\n",
        "    if plot:\n",
        "        total_prob = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
        "        plot_prob_distribution(total_prob, next_tokens, 'nucleus', top_p_index_to_keep)\n",
        "\n",
        "    return next_tokens\n",
        "\n",
        "# Start generating text\n",
        "beam_search(input_ids, 0, bar, length, beams, 'nucleus', 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DGl5sEiR7iJz"
      },
      "source": [
        ":::{.column-page}\n",
        "<center><img src=\"/images/decoding/nucleus.gif\"></center>\n",
        ":::\n",
        "\n",
        "In this plot, you can see that the number of tokens included in the nucleus fluctuates a lot. The generated probability distributions vary considerably, leading to the selection of tokens that are not always among the most probable ones. This opens the door to the generation of unique and varied sequences. Now, let's observe the text it generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6uTqcMnk0qb",
        "outputId": "6bdba28a-3b0c-486e-b9eb-f30f2cb261e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: I have a dream. I'm going to\n"
          ]
        }
      ],
      "source": [
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SUeE7J7d7wo3"
      },
      "source": [
        "The nucleus sampling algorithm produces the sequence: \"I have a dream. I'm going to\", which shows a notable enhancement in semantic coherence compared to greedy sampling.\n",
        "\n",
        "To compare the decision paths, let's visualize the new tree nucleus sampling generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "NPHx7ZRL2641",
        "outputId": "9bbd324e-b140-4c23-dcd2-45b61f46f8f7"
      },
      "outputs": [],
      "source": [
        "# Plot graph\n",
        "plot_graph(graph, length, beams, 'sequence')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tPlRqt0H-vCM"
      },
      "source": [
        "::: {.column-screen}\n",
        "<center><img src=\"/images/decoding/nucleus_tree.png\"></center>\n",
        ":::\n",
        "\n",
        "As with top-k sampling, this tree is very different from the one generated with greedy sampling, displaying more variety. Both top-k and nucleus sampling offer unique advantages when generating text, enhancing diversity, and introducing creativity into the output. Your choice between the two methods (or even greedy search) will depend on the specific requirements and constraints of your project.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "In this article, we have delved deep into various decoding methods used by LLMs, specifically GPT-2. We started with a simply **greedy search** and its immediate (yet often suboptimal) selection of the most probable next token. Next, we introduced the **beam search** technique, which considers several of the most likely tokens at each step. Although it offers more nuanced results, beam search can sometimes fall short in generating diverse and creative sequences.\n",
        "\n",
        "To bring more variability into the process, we then moved on to **top-k sampling** and **nucleus sampling**. Top-k sampling diversifies the text generation by randomly selecting among the k most probable tokens, while nucleus sampling takes a different path by dynamically forming a nucleus of tokens based on cumulative probability. Each of these methods brings unique strengths and potential drawbacks to the table, and the specific requirements of your project will largely dictate the choice among them.\n",
        "\n",
        "Ultimately, understanding these techniques and their trade-offs will equip you to better guide the LLMs towards producing increasingly realistic, nuanced, and compelling textual output.\n",
        "\n",
        "If you're interested in more technical content around LLMs, you can follow me on Twitter [@maximelabonne](https://twitter.com/maximelabonne)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0086b6ae311c46339958771a825ba594": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7e2b8a6e90c456ab98978e93952649a",
            "max": 63,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_daf267adb2604c65b5c8b2b93eae5e8d",
            "value": 63
          }
        },
        "0db36f1e0c1c4801a07809908ed3a6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5287e0c25b54e4f97f90cc13c02419b",
              "IPY_MODEL_0086b6ae311c46339958771a825ba594",
              "IPY_MODEL_222c73051b33432abaab69d6925ebb61"
            ],
            "layout": "IPY_MODEL_facf596299c5422999763eeec6b4ebb2"
          }
        },
        "109a034f44b94b0c8a2fe764aaab0213": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "222c73051b33432abaab69d6925ebb61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b43bebd8b934dc5b26e1ea20f38ede3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aae4c3664807436595b1a730d5b6f3fb",
            "value": " 186/? [35:34&lt;00:00,  2.26it/s]"
          }
        },
        "8b2eeb05b70946999046d36d42caf435": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b43bebd8b934dc5b26e1ea20f38ede3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aae4c3664807436595b1a730d5b6f3fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5287e0c25b54e4f97f90cc13c02419b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_109a034f44b94b0c8a2fe764aaab0213",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8b2eeb05b70946999046d36d42caf435",
            "value": ""
          }
        },
        "b7e2b8a6e90c456ab98978e93952649a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daf267adb2604c65b5c8b2b93eae5e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "facf596299c5422999763eeec6b4ebb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
