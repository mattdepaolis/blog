{
 "cells": [
  {
   "cell_type": "raw",
   "id": "29766e5c",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /axolotl/\n",
    "categories:\n",
    "- Large Language Models\n",
    "date: '2023-08-27'\n",
    "colab: <a href=\"https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing\"><img src=\"images/colab.png\" alt=\"Open In Colab\"></a>\n",
    "image: /images/axolotl/thumbnail.jpg\n",
    "title: \"A Beginner's Guide to LLM Fine-Tuning\"\n",
    "subtitle: \"How to fine-tune Llama and other LLMs with one tool\"\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "kowvEHW1B9cc",
   "metadata": {
    "id": "kowvEHW1B9cc"
   },
   "source": [
    "<center><img src=\"/images/axolotl/thumbnail.jpg\"></center>\n",
    "\n",
    "::: {.column-margin}\n",
    "Find many more architectures and applications using graph neural networks in my book, [**Hands-On Graph Neural Networks**](https://mlabonne.github.io/blog/book.html) 👇\n",
    "<a href=\"https://packt.link/a/9781804617526\"><img src=\"/images/gnnbook/cover.png\" alt=\"Hands-On Graph Neural Networks Using Python\" id=\"gnn-book\"></a>\n",
    ":::\n",
    "\n",
    "The growing interest in Large Language Models (LLMs) has led to a surge in **tools and wrappers designed to streamline their training process**.\n",
    "\n",
    "Popular options include [FastChat](https://github.com/lm-sys/FastChat) from LMSYS (used to train [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5)) and Hugging Face's [transformers](https://github.com/huggingface/transformers)/[trl](https://github.com/huggingface/trl) libraries (used in [my previous article](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)). In addition, each big LLM project, like [WizardLM](https://github.com/nlpxucan/WizardLM/tree/main), tends to have its own training script, inspired by the original [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) implementation.\n",
    "\n",
    "In this article, we will use [**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl), a tool created by the OpenAccess AI Collective. We will use it to fine-tune a [**Code Llama 7b**](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml) model on an evol-instruct dataset comprised of 1,000 samples of Python code.\n",
    "\n",
    "## 🤔 Why Axolotl?\n",
    "\n",
    "The main appeal of Axolotl is that it provides a one-stop solution, which includes numerous features, model architectures, and an active community. Here's a quick list of my favorite things about it:\n",
    "\n",
    "* **Configuration**: All parameters used to train an LLM are neatly stored in a yaml config file. This makes it convenient for sharing and reproducing models. You can see an example for Llama 2 [here](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/llama-2).\n",
    "\n",
    "* **Dataset Flexibility**: Axolotl allows the specification of multiple datasets with varied prompt formats such as alpaca (`{\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}`), sharegpt:chat (`{\"conversations\": [{\"from\": \"...\", \"value\": \"...\"}]}`), and raw completion (`{\"text\": \"...\"}`). Combining datasets is seamless, and the hassle of unifying the prompt format is eliminated.\n",
    "\n",
    "* **Features**: Axolotl is packed with SOTA techniques such as FSDP, deepspeed, LoRA, QLoRA, ReLoRA, sample packing, GPTQ, FlashAttention, xformers, and rope scaling.\n",
    "\n",
    "* **Utilities**: There are numerous user-friendly utilities integrated, including the addition or alteration of special tokens, or a custom wandb configuration.\n",
    "\n",
    "Some well-known models trained using this tool are [Manticore-13b](https://huggingface.co/openaccess-ai-collective/manticore-13b) from the OpenAccess AI Collective and [Samantha-1.11-70b](https://huggingface.co/ehartford/Samantha-1.11-70b) from Eric Hartford. Like other wrappers, it is built on top of the transformers library and uses many of its features.\n",
    "\n",
    "## ⚙️ Create your own config file\n",
    "\n",
    "Before anything, we need a configuration file. You can reuse an existing configuration from the [examples](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples) folder. In our case, we will tweak the [QLoRA config](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml) for Llama 2 to create our own **Code Llama** model. The model will be trained on a subset of 1,000 Python samples from the [nickrosh/Evol-Instruct-Code-80k-v1](https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1) dataset.\n",
    "\n",
    "First, we must change the base_model and base_model_config fields to \"codellama/CodeLlama-7b-hf\". To push our trained adapter to the Hugging Face Hub, let's add a new field hub_model_id, which corresponds to the name of our model, \"EvolCodeLlama-7b\". Now, we have to update the dataset to [mlabonne/Evol-Instruct-Python-1k](https://huggingface.co/datasets/mlabonne/Evol-Instruct-Python-1k) and set type to \"alpaca\".\n",
    "\n",
    "There's no sample bigger than 2048 tokens in this dataset, so we can reduce the sequence_len to \"2048\" and save some VRAM. Talking about VRAM, we're going to use a micro_batch_size of 10 and a gradient_accumulation_steps of 1 to maximize its use. In practice, you try different values until you use >95% of the available VRAM.\n",
    "\n",
    "For convenience, I'm going to add the name \"axolotl\" to the wandb_project field so it's easier to track on my account. I'm also setting the warmup_steps to \"100\" (personal preference) and the eval_steps to 0.01 so we'll end up with 100 evaluations.\n",
    "\n",
    "Here's how the final config file should look:\n",
    "\n",
    "```yaml\n",
    "base_model: codellama/CodeLlama-7b-hf\n",
    "base_model_config: codellama/CodeLlama-7b-hf\n",
    "model_type: LlamaForCausalLM\n",
    "tokenizer_type: LlamaTokenizer\n",
    "is_llama_derived_model: true\n",
    "hub_model_id: EvolCodeLlama-7b\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: true\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "    - path: mlabonne/Evol-Instruct-Python-1k\n",
    "    type: alpaca\n",
    "dataset_prepared_path: last_run_prepared\n",
    "val_set_size: 0.02\n",
    "output_dir: ./qlora-out\n",
    "\n",
    "adapter: qlora\n",
    "lora_model_dir:\n",
    "\n",
    "sequence_len: 2048\n",
    "sample_packing: true\n",
    "\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_modules:\n",
    "lora_target_linear: true\n",
    "lora_fan_in_fan_out:\n",
    "\n",
    "wandb_project: axolotl\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_run_id:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 1\n",
    "micro_batch_size: 10\n",
    "num_epochs: 3\n",
    "optimizer: paged_adamw_32bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: true\n",
    "fp16: false\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "warmup_steps: 100\n",
    "eval_steps: 0.01\n",
    "save_strategy: epoch\n",
    "save_steps:\n",
    "debug:\n",
    "deepspeed:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "    bos_token: \"<s>\"\n",
    "    eos_token: \"</s>\"\n",
    "    unk_token: \"<unk>\"\n",
    "```\n",
    "\n",
    "You can also find this config file [here](https://gist.github.com/mlabonne/8055f6335e2b85f082c8c75561321a66) as a GitHub gist.\n",
    "\n",
    "Before we start training our model, I want to introduce a few parameters that are important to understand:\n",
    "\n",
    "* **QLoRA**: We're using QLoRA for fine-tuning, which is why we're loading the base model in 4-bit precision (NF4 format). You can check [this article](https://medium.com/towards-data-science/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b) from [Benjamin Marie](undefined) to know more about QLoRA.\n",
    "\n",
    "* **Gradient checkpointing**: It lowers the VRAM requirements by removing some activations that are re-computed on demand during the backward pass. It also slows down training by about 20%, according to Hugging Face's [documentation](https://huggingface.co/docs/transformers/v4.18.0/en/performance).\n",
    "\n",
    "* **FlashAttention**: This implements the [FlashAttention ](https://github.com/Dao-AILab/flash-attention)mechanism, which improves the speed and memory efficiency of our model thanks to a clever fusion of GPU operations (learn more about it in [this article](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad) from [Aleksa Gordiç](undefined)).\n",
    "\n",
    "* **Sample packing**: Smart way of creating batches with as little padding as possible, by reorganizing the order of the samples ([bin packing problem](https://en.wikipedia.org/wiki/Bin_packing_problem)). As a result, we need fewer batches to train the model on the same dataset. It was inspired by the [Multipack Sampler](https://github.com/imoneoi/multipack_sampler/tree/master) (see [my note](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/multipack_sampler.html)) and [Krell et al.](https://arxiv.org/pdf/2107.02027.pdf)\n",
    "\n",
    "You can find FlashAttention in some other tools, but sample packing is relatively new. As far as I know, [OpenChat ](https://github.com/imoneoi/openchat)was the first project to use sample packing during fine-tuning. Thanks to Axolotl, we'll use these techniques for free.\n",
    "\n",
    "## 🦙 Fine-tune Code Llama\n",
    "\n",
    "Having the config file ready, it's time to get our hands dirty with the actual fine-tuning. You might consider running the training on a Colab notebook. However, for those without access to a high-performance GPU, a more cost-effective solution consists of renting **cloud-based GPU services**, like AWS, [Lambda Labs](https://lambdalabs.com/), [Vast.ai](https://vast.ai/), [Banana](https://www.banana.dev/), or [RunPod](https://www.runpod.io/).\n",
    "\n",
    "Personally, I use RunPod, which is a popular option in the fine-tuning community. It's not the cheapest service but it hits a good tradeoff with a clean UI. You can easily replicate the following steps using your favorite service.\n",
    "\n",
    "When your RunPod account is set up, go to Manage > Templates and click on \"New Template\". Here is a simple template:\n",
    "\n",
    "<center><img src=\"/images/axolotl/config.png\"></center>\n",
    "\n",
    "Let's review the different fields and their corresponding values:\n",
    "\n",
    "* **Template Name**: Axolotl (you can choose whatever you want)\n",
    "\n",
    "* **Container Image**: winglian/axolotl-runpod:main-py3.10-cu118-2.0.1\n",
    "\n",
    "* **Container Disk**: 100 GB\n",
    "\n",
    "* **Volume Disk**: 0 GB\n",
    "\n",
    "* **Volume Mount Path**: /workspace\n",
    "\n",
    "In addition, there are two handy environment variables can include:\n",
    "\n",
    "* **HUGGING_FACE_HUB_TOKEN**: you can find your token on [this page](https://huggingface.co/settings/tokens) (requires an account)\n",
    "\n",
    "* **WANDB_API_KEY**: you can find your key on [this page](https://wandb.ai/authorize) (requires an account)\n",
    "\n",
    "Alternatively, you can simply log in the terminal later (using huggingface-cli login and wandb login). Once you're set-up, go to Community Cloud and deploy an RTX 3090. Here you can search for the name of your template and select it as follows:\n",
    "\n",
    "<center><img src=\"/images/axolotl/launch.png\"></center>\n",
    "\n",
    "You can click on \"Continue\" and RunPod will deploy your template. You can see the installation in your pod's logs (Manage > Pods). When the option becomes available, click on \"Connect\". Here, click on \"tart Web Terminal\" and then \"Connect to Web Terminal\". You are now connected to your pod!\n",
    "\n",
    "The following steps are **the same no matter what service you choose**:\n",
    "\n",
    "1. We install Axolotl and the PEFT library as follows:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/OpenAccess-AI-Collective/axolotl\n",
    "cd axolotl\n",
    "\n",
    "pip3 install -e .[flash-attn]\n",
    "pip3 install -U git+https://github.com/huggingface/peft.git\n",
    "```\n",
    "\n",
    "2. Download the config file we created:\n",
    "\n",
    "```bash\n",
    "wget https://gist.githubusercontent.com/mlabonne/8055f6335e2b85f082c8c75561321a66/raw/93915a9563fcfff8df9a81fc0cdbf63894465922/EvolCodeLlama-7b.yaml\n",
    "```\n",
    "\n",
    "3. You can now **start fine-tuning the model** with the following command:\n",
    "\n",
    "```bash\n",
    "accelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml\n",
    "```\n",
    "\n",
    "If everything is configured correctly, you should be able to train the model in a little more than **one hour** (it took me 1h 11m 44s). If you check the GPU memory used, you'll see almost 100% with this config, which means we're optimizing it pretty nicely. If you're using a GPU with more VRAM (like an A100), you can increase the micro-batch size to make sure you're fully using it.\n",
    "\n",
    "In the meantime, feel free to close the web terminal and check your loss on Weights & Biases. We're using tmux so the training won't stop if you close the terminal. Here are my loss curves:\n",
    "\n",
    "<center><img src=\"/images/axolotl/loss.png\"></center>\n",
    "\n",
    "We see a steady improvement in the eval loss, which is a good sign. However, you can also spot drops in the eval loss that are not correlated with a decrease in the quality of the outputs. The best way to evaluate your model is simply by using it: you can run it in the terminal with the command accelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml --inference --lora_model_dir=\"./qlora-out\".\n",
    "\n",
    "The QLoRA adapter should already be uploaded to the Hugging Face Hub. However, you can also **merge the base Code Llama model with this adapter and push the merged model** there by following these steps:\n",
    "\n",
    "1. Download [this script](https://gist.github.com/mlabonne/a3542b0519708b8871d0703c938bba9f):\n",
    "\n",
    "```bash\n",
    "wget https://gist.githubusercontent.com/mlabonne/a3542b0519708b8871d0703c938bba9f/raw/60abc5afc07f9d843bc23d56f4e0b7ab072c4a62/merge_peft.py\n",
    "```\n",
    "\n",
    "2. Execute it with this command:\n",
    "\n",
    "```bash\n",
    "python merge_peft.py --base_model=codellama/CodeLlama-7b-hf --peft_model=./qlora-out --hub_id=EvolCodeLlama-7b\n",
    "```\n",
    "\n",
    "Congratulations, you should have **your own EvolCodeLlama-7b** on the Hugging Face Hub at this point! For reference, you can access my own model trained with this process here: [mlabonne/EvolCodeLlama-7b](https://huggingface.co/mlabonne/EvolCodeLlama-7b)\n",
    "\n",
    "Considering that our EvolCodeLlama-7b is a code LLM, it would be interesting to compare its performance with other models on **standard benchmarks**, such as [HumanEval](https://github.com/openai/human-eval) and [MBPP](https://github.com/google-research/google-research/tree/master/mbpp). For reference, you can find a leaderboard at the following address: [Multilingual Code Evals](https://huggingface.co/spaces/bigcode/multilingual-code-evals).\n",
    "\n",
    "If you're happy with this model, you can **quantize** it with GGML for local inference with [this free Google Colab notebook](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing). You can also fine-tune **bigger models** (e.g., 70b parameters) thanks to [deepspeed](https://github.com/microsoft/DeepSpeed), which only requires an additional config file.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this article, we've covered the essentials of **how to efficiently fine-tune LLMs**. We customized parameters to train on our Code Llama model on a small Python dataset. Finally, we merged the weights and uploaded the result on Hugging Face.\n",
    "\n",
    "I hope you found this guide useful. I recommend using Axolotl with a cloud-based GPU service to get some experience and upload a few models on Hugging Face. Build your own datasets, play with the parameters, and break stuff along the way. Like with every wrapper, don't hesitate to check the source code to get a good intuition of what it's actually doing. It will massively help in the long run.\n",
    "\n",
    "Thanks to the OpenAccess AI Collective and all the contributors!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
