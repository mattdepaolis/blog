{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /what-is-a-tensor/\n",
    "categories:\n",
    "- data science\n",
    "colab: <a href=\"https://colab.research.google.com/drive/1azq12DApWgLgdWuYB3wh0TcwJVAfVxCL?usp=sharing\"><img src=\"images/colab.png\" alt=\"Open In Colab\"></a>\n",
    "date: '2022-03-28'\n",
    "image: /images/tensor/thumbnail.png\n",
    "title: What is a Tensor in Machine Learning?\n",
    "subtitle: \"The difference between tensors, arrays, and\\_matrices\"\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7Wjjq6SRRSyR"
   },
   "source": [
    "<center><img alt=\"Thumbnail\" src=\"/images/tensor/thumbnail.png\"></center>\n",
    "\n",
    "What is a tensor, exactly?\n",
    "\n",
    "Most deep learning practitioners know about them but can't pinpoint an **exact definition**.\n",
    "\n",
    "TensorFlow, PyTorch: every deep learning framework relies on the same basic object: **tensors**. They're used to store almost everything in deep learning: input data, weights, biases, predictions, etc.\n",
    "\n",
    "And yet, their definition is incredibly fuzzy: the <a href=\"https://en.wikipedia.org/wiki/Category:Tensors\">Wikipedia category</a> alone has **over 100 pages** related to tensors.\n",
    "\n",
    "In this article, we'll give a **definitive answer** to the following question: what is a tensor in neural networks?\n",
    "\n",
    "## üíª Tensors in computer science\n",
    "\n",
    "So why are there so many definitions?\n",
    "\n",
    "It's quite simple: different fields have different definitions. Tensors in **mathematics** are not quite the same as tensors in **physics**, which are different from tensors in **computer science**.\n",
    "\n",
    "<center><img alt=\"Data structure vs. Objects\" src=\"/images/tensor/datastructure.png\" width=\"800\"></center>\n",
    "\n",
    "These definitions can be divided into two categories: tensors as a data structure or as objects (in an [object-oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming) sense).\n",
    "\n",
    "* **Data structure**: this is the definition we use in computer science. Tensors are [multidimensional arrays](https://en.wikipedia.org/wiki/Array_data_type#Multi-dimensional_arrays) that store a specific type of value.\n",
    "* **Objects**: this is the definition used in other fields. In <a href=\"https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)\">mathematics</a> and <a href=\"https://en.wikipedia.org/wiki/Infinitesimal_strain_theory\">physics</a>, tensors are not just a data structure: they also have a list of properties, like a specific product.\n",
    "\n",
    "This is why you see a lot of people (sometimes quite pedantically) saying \"*tensors are **not** n-dimensional arrays/matrices*\": they don't talk about data structures, but about **objects with properties**.\n",
    "\n",
    "Even the same words have **different meanings**. For instance, in computer science, a 2D tensor is a matrix (it's a tensor of rank 2). In linear algebra, a tensor with 2 dimensions means it only stores two values. The rank also has a completely different definition: it is the maximum number of its linearly independent column (or row) vectors.\n",
    "\n",
    "In computer science, we're only interested in a definition focused on the **data structure**. From this point of view, tensors truly are a generalization in $n$ dimensions of matrices.\n",
    "\n",
    "But we're still missing an important nuance when talking about tensors specifically in the context of deep learning...\n",
    "\n",
    "## üß† Tensors in deep learning\n",
    "\n",
    "<center><img alt=\"Array vs. Tensor\" src=\"/images/tensor/arrayvstensor.png\" width=\"800\">\n",
    "<i>Icons created by Freepik and smashingstocks - <a href=\"https://www.flaticon.com/\">Flaticon</a></i></center>\n",
    "\n",
    "So why are they called \"tensors\" instead of \"multidimensional arrays\"? Ok, it is shorter, but is it all there is to it? Actually, people make an **implicit assumption** when they talk about tensors.\n",
    "\n",
    "PyTorch's <a href=\"https://pytorch.org/tutorials/beginner/examples_tensor/polynomial_tensor.html#:~:text=PyTorch%3A%20Tensors,-A%20third%20order&text=A%20PyTorch%20Tensor%20is%20basically,used%20for%20arbitrary%20numeric%20computation.\">official documentation</a> gives us a practical answer:\n",
    "\n",
    "> The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either **CPU or GPU**.\n",
    "\n",
    "In deep learning, we need performance to compute a lot of matrix multiplications in a highly parallel way. These matrices (and n-dimensional arrays in general) are generally stored and processed on GPUs to speed up training and inference times.\n",
    "\n",
    "This is what was missing in our previous definition: tensors in deep learning are not just n-dimensional arrays, there's also the implicit assumption they can be **run on a GPU**.\n",
    "\n",
    "## ‚öîÔ∏è NumPy vs PyTorch\n",
    "\n",
    "Let's see the difference between NumPy arrays and PyTorch tensors.\n",
    "\n",
    "<center><img alt=\"Sclar, vector, matrix\" src=\"/images/tensor/scalar.png\" width=\"800\"></center>\n",
    "\n",
    "These two objects are very similar: we can initialize a **1D array** and a **1D tensor** with nearly the same syntax. They also share a lot of methods and can be easily converted into one another.\n",
    "\n",
    "You can find the code used in this article [at this address](https://github.com/mlabonne/how-to-data-science/blob/main/What_is_a_Tensor_in_Deep_Learning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Af6ham967Aax",
    "outputId": "5fd43800-41c6-4d07-816d-f6db922fea67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Array: [1 2 3]\n",
      "PyTorch Tensor: tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "array = np.array([1, 2, 3])\n",
    "print(f'NumPy Array: {array}')\n",
    "\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "print(f'PyTorch Tensor: {tensor}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cXdjYo2FHc6S"
   },
   "source": [
    "Initializing 2D arrays and 2D tensors is not more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rc6Nos7g-Olq",
    "outputId": "2627ad8d-a75a-4f2e-cca8-783b526de425"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Array:\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "PyTorch Tensor:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "print(f'NumPy Array:\\n{x}')\n",
    "\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "print(f'\\nPyTorch Tensor:\\n{x}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QwXSpGhGHmGh"
   },
   "source": [
    "We said that the only difference between tensors and arrays was the fact that tensors can be **run on GPUs**. So in the end, this distinction is based on performance. But is this boost that important?\n",
    "\n",
    "Let's compare the performance between NumPy arrays and PyTorch tensors on matrix multiplication. In the following example, we randomly initialize **4D arrays/tensors and multiply them**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z448Qo93-45T"
   },
   "outputs": [],
   "source": [
    "# You need to run this code on a computer with a GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# 4D arrays\n",
    "array1 = np.random.rand(100, 100, 100, 100)\n",
    "array2 = np.random.rand(100, 100, 100, 100)\n",
    "\n",
    "# 4D tensors\n",
    "tensor1 = torch.rand(100, 100, 100, 100).to(device)\n",
    "tensor2 = torch.rand(100, 100, 100, 100).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMrv4eTuAdkW",
    "outputId": "17758f7d-19d2-43bb-c4aa-af3ba574e326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 1.32 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.matmul(array1, array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OibOHbnFCgsl",
    "outputId": "8b7ee3f5-e069-4784-9119-12de23e4609c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 5: 25.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.matmul(tensor1, tensor2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1_RhSPzN-PKO"
   },
   "source": [
    "As we can see, PyTorch tensors completed outperformed NumPy arrays: they completed the multiplication **52 times faster**!\n",
    "\n",
    "We could attribute this performance to different factors, such as:\n",
    "\n",
    "* NumPy arrays use a `float64` format, whereas PyTorch tensors leverage the more efficient `float32` format. However, even when NumPy arrays are converted to `float32`, PyTorch tensors are still 40 times faster.\n",
    "* PyTorch tensors are stored on a GPU, unlike NumPy arrays. But if we repeat the same experiment on a CPU, PyTorch tensors still manage to be 2.8 times faster on average.\n",
    "\n",
    "Even when combining both factors, PyTorch tensors prove to be 1.4 times faster, showing that NumPy arrays are truly less performant for matrix multiplication.\n",
    "\n",
    "This is the true power of tensors: they're **blazingly fast**! Performance might vary depending on the dimensions, the implementation, and the hardware, but this speed is the reason why tensors (and not arrays) are so common in deep learning.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this article, we wrote a definition of tensors based on:\n",
    "\n",
    "1. Their use in **computer science** (data structure);\n",
    "2. More specifically, in **deep learning** (they can run on GPUs).\n",
    "\n",
    "Here's how we can summarize it in one sentence:\n",
    "\n",
    "> Tensors are **n-dimensional arrays** with the implicit assumption that they can **run on a GPU**.\n",
    "\n",
    "Finally, we saw the difference in performance between tensors and arrays, which motivates the need for tensors in deep learning.\n",
    "\n",
    "So next time someone tries to explain to you that tensors are not exactly a generalization of matrices, you'll know that they're right in a particular definition of tensors, but not in the computer science/deep learning one.\n",
    "\n",
    "If you're looking for more data science and machine learning content in n-dimensions, please **follow me on twitter [@maximelabonne](https://twitter.com/maximelabonne)**. üì£"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "What is a Tensor in Deep Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7d5390928ceca9e16542a6b449c67162a3ecd0d2a0e4ea61f2fe48adbab648d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
