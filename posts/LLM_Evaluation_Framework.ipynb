{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca60bc6f-0f0b-41d6-9057-2afef63765f9",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /eval/\n",
    "categories:\n",
    "- LLM Evaluation\n",
    "date: '2025-05-01'\n",
    "colab: <a href=\"https://colab.research.google.com/drive/12t6aLEcGJrDWCHcBv-JBpV2Z6tZkHke5?usp=sharing\"><img src=\"images/colab.png\" alt=\"Open In Colab\"></a>\n",
    "image: /images/eval/thumbnail.jpg\n",
    "title: \"LLM Evaluation Framework\"\n",
    "subtitle: \"Replicate Huggingface Open LLM Leaderboard Locally\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a77a7-ee11-424d-a694-a519b1be4ed3",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/eval/thumbnail.jpg\" alt=\"Image\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c5c0d-03db-4fa9-98d7-520d1b79b3d1",
   "metadata": {},
   "source": [
    "The discontinuation of Hugging Face‚Äôs Open LLM Leaderboard has left a gap in the community for standardized evaluation of large language models (LLMs). To address this, I developed the LLM Evaluation Framework, a comprehensive and modular tool designed to facilitate reproducible and extensible benchmarking of LLMs across various tasks and benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0ed81-16ac-4fbe-bfc3-c51ec0170fd8",
   "metadata": {},
   "source": [
    "## üß© Why This Framework Matters\n",
    "\n",
    "The Open LLM Leaderboard was instrumental in providing a centralized platform for evaluating and comparing LLMs. Its retirement has underscored the need for tools that allow researchers and developers to conduct their own evaluations with transparency and consistency. The LLM Evaluation Framework aims to fill this void by offering:\n",
    "- Modular Design: Inspired by microservice architecture, enabling easy integration and customization.\n",
    "- Multiple Model Backends: Support for Hugging Face (hf) and vLLM backends, allowing flexibility in model loading and inference.\n",
    "- Quantization Support: Evaluate quantized models (e.g., 4-bit, 8-bit with hf, AWQ with vLLM) to assess performance under resource constraints.\n",
    "- Comprehensive Benchmarks: Includes support for standard benchmarks like MMLU, GSM8K, BBH, and more.\n",
    "- Leaderboard Replication: Easily run evaluations mimicking the Open LLM Leaderboard setup with standardized few-shot settings.\n",
    "- Flexible Configuration: Customize evaluations via CLI arguments or programmatic usage.\n",
    "- Detailed Reporting: Generates JSON results and Markdown reports for easy analysis.\n",
    "- Parallelism: Leverages vLLM for efficient inference, including tensor parallelism across multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f485acb-141f-4f46-ac8d-6766cde9060a",
   "metadata": {},
   "source": [
    "## üöÄ Getting Started\n",
    "\n",
    "Installation\n",
    "1.\tClone the Repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mattdepaolis/llm-evaluation.git\n",
    "!cd llm-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1767477",
   "metadata": {},
   "source": [
    "2.\tSet Up a Virtual Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv .venv\n",
    "!source .venv/bin/activate  # On Windows use `.venv\\Scripts\\activate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfcb3c-79e6-4527-bbec-ff3a65cea069",
   "metadata": {},
   "source": [
    "3.\tInstall Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e lm-evaluation-harness\n",
    "!pip install torch numpy tqdm transformers accelerate bitsandbytes sentencepiece\n",
    "!pip install vllm  # If you plan to use the vLLM backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f0bc4-d456-4cd5-a9c6-e80abeda5924",
   "metadata": {},
   "source": [
    "## üß™ Example Usage\n",
    "\n",
    "**Using the Command-Line Interface (CLI)**\n",
    "\n",
    "Evaluate a model on the HellaSwag benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ae4b97-b2df-4866-bd45-da46b2540d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python llm_eval_cli.py \\\n",
    "  --model hf \\\n",
    "  --model_name google/gemma-2b \\\n",
    "  --tasks hellaswag \\\n",
    "  --num_fewshot 0 \\\n",
    "  --device cuda  # Use 'cpu' if you don't have a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508664b-dda8-41c7-b7a0-449c2a9c6d0a",
   "metadata": {},
   "source": [
    "This command will download the gemma-2b model (if not cached), run it on the HellaSwag benchmark with 0 few-shot examples, and save the results in the results/ and reports/ directories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89315047-4f4f-4ab1-b256-a47cb6082338",
   "metadata": {},
   "source": [
    "**Using as a Python Library**\n",
    "\n",
    "Integrate the evaluation logic directly into your Python scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba1fdde-5507-4920-a9d6-6e70820b40d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_eval import evaluate_model\n",
    "import os\n",
    "\n",
    "# Define evaluation parameters\n",
    "eval_config = {\n",
    "    \"model_type\": \"hf\",\n",
    "    \"model_name\": \"google/gemma-2b-it\",\n",
    "    \"tasks\": [\"mmlu\", \"gsm8k\"],\n",
    "    \"num_fewshot\": 0,\n",
    "    \"device\": \"cuda\",\n",
    "    \"quantize\": True,\n",
    "    \"quantization_method\": \"4bit\",\n",
    "    \"batch_size\": \"auto\",\n",
    "    \"output_dir\": \"./custom_results\"  # Optional: Specify output location\n",
    "}\n",
    "\n",
    "# Run the evaluation\n",
    "try:\n",
    "    results_summary, results_file_path = evaluate_model(**eval_config)\n",
    "\n",
    "    print(\"Evaluation completed successfully!\")\n",
    "    print(f\"Results summary: {results_summary}\")\n",
    "    print(f\"Detailed JSON results saved to: {results_file_path}\")\n",
    "\n",
    "    # Construct the expected report path\n",
    "    base_name = os.path.splitext(os.path.basename(results_file_path))[0]\n",
    "    report_file_path = os.path.join(os.path.dirname(results_file_path).replace('results', 'reports'), f\"{base_name}_report.md\")\n",
    "\n",
    "    if os.path.exists(report_file_path):\n",
    "        print(f\"Markdown report saved to: {report_file_path}\")\n",
    "    else:\n",
    "        print(\"Markdown report not found at expected location.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb3de2-61e7-47c4-9971-ba1e70c132c2",
   "metadata": {},
   "source": [
    "## üìä Reporting and Results\n",
    "\n",
    "The framework generates:\n",
    "- JSON Results: Detailed results for each task, including individual sample predictions (if applicable), metrics, and configuration details, saved in the results/ directory.\n",
    "- **Markdown Reports**: A summary report aggregating scores across tasks, generated in the reports/ directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007b73e-bbb3-4578-9311-75639696a9eb",
   "metadata": {},
   "source": [
    "## üîß Extending the Framework\n",
    "\n",
    "The modular design makes it easier to add new functionalities:\n",
    "1.\tAdding New Tasks/Benchmarks:\n",
    "- Define the task configuration in llm_eval/tasks/task_registry.py or a similar configuration file.\n",
    "- Ensure the task is compatible with the lm-evaluation-harness structure or adapt it.\n",
    "2.\tSupporting New Model Backends:\n",
    "- Create a new model handler class in llm_eval/models/ inheriting from a base model class (if applicable).\n",
    "- Implement the required methods for loading, inference, etc.\n",
    "- Register the new backend type. Ôøº\n",
    "3.\tCustomizing Reporting:\n",
    "- Modify the report generation logic in llm_eval/reporting/ to change the format or content of the Markdown/JSON outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390bb9f",
   "metadata": {},
   "source": [
    "## ü§ù Contributing\n",
    "\n",
    "Contributions are welcome! Please follow standard practices:\n",
    "1.\tFork the repository.\n",
    "2.\tCreate a new branch for your feature or bug fix (git checkout -b feature/my-new-feature).\n",
    "3.\tMake your changes and commit them (git commit -am 'Add some feature').\n",
    "4.\tPush to the branch (git push origin feature/my-new-feature).\n",
    "5.\tCreate a new Pull Request."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
