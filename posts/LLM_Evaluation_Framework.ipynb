{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca60bc6f-0f0b-41d6-9057-2afef63765f9",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /eval/\n",
    "categories:\n",
    "- LLM Evaluation\n",
    "date: '2025-05-01'\n",
    "colab: <a href=\"https://colab.research.google.com/drive/12t6aLEcGJrDWCHcBv-JBpV2Z6tZkHke5?usp=sharing\"><img src=\"images/colab.png\" alt=\"Open In Colab\"></a>\n",
    "image: /images/eval/thumbnail.jpg\n",
    "title: \"LLM Evaluation Framework\"\n",
    "subtitle: \"Replicate Huggingface Open LLM Leaderboard Locally\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a77a7-ee11-424d-a694-a519b1be4ed3",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"images/eval/thumbnail.jpg\" alt=\"Image\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c5c0d-03db-4fa9-98d7-520d1b79b3d1",
   "metadata": {},
   "source": [
    "The discontinuation of Hugging Face‚Äôs Open LLM Leaderboard has left a gap in the community for standardized evaluation of large language models (LLMs). To address this, I developed the LLM Evaluation Framework, a comprehensive and modular tool designed to facilitate reproducible and extensible benchmarking of LLMs across various tasks and benchmarks.\n",
    "\n",
    "The LLM Evalaution Framework can be found on my Github account: [LLM Evaluation Framework](https://github.com/mattdepaolis/llm-evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0ed81-16ac-4fbe-bfc3-c51ec0170fd8",
   "metadata": {},
   "source": [
    "## üß© Why This Framework Matters\n",
    "\n",
    "The Open LLM Leaderboard was instrumental in providing a centralized platform for evaluating and comparing LLMs. Its retirement has underscored the need for tools that allow researchers and developers to conduct their own evaluations with transparency and consistency. The LLM Evaluation Framework aims to fill this void by offering:\n",
    "- Modular Design: Inspired by microservice architecture, enabling easy integration and customization.\n",
    "- Multiple Model Backends: Support for Hugging Face (hf) and vLLM backends, allowing flexibility in model loading and inference.\n",
    "- Quantization Support: Evaluate quantized models (e.g., 4-bit, 8-bit with hf, AWQ with vLLM) to assess performance under resource constraints.\n",
    "- Comprehensive Benchmarks: Includes support for standard benchmarks like MMLU, GSM8K, BBH, and more.\n",
    "- Leaderboard Replication: Easily run evaluations mimicking the Open LLM Leaderboard setup with standardized few-shot settings.\n",
    "- Flexible Configuration: Customize evaluations via CLI arguments or programmatic usage.\n",
    "- Detailed Reporting: Generates JSON results and Markdown reports for easy analysis.\n",
    "- Parallelism: Leverages vLLM for efficient inference, including tensor parallelism across multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f485acb-141f-4f46-ac8d-6766cde9060a",
   "metadata": {},
   "source": [
    "## üöÄ Getting Started\n",
    "\n",
    "Installation\n",
    "1.\tClone the Repository:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mattdepaolis/llm-evaluation.git\n",
    "!cd llm-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1767477",
   "metadata": {},
   "source": [
    "2.\tSet Up a Virtual Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv .venv\n",
    "!source .venv/bin/activate  # On Windows use `.venv\\Scripts\\activate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfcb3c-79e6-4527-bbec-ff3a65cea069",
   "metadata": {},
   "source": [
    "3.\tInstall Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e lm-evaluation-harness\n",
    "!pip install torch numpy tqdm transformers accelerate bitsandbytes sentencepiece\n",
    "!pip install vllm  # If you plan to use the vLLM backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f0bc4-d456-4cd5-a9c6-e80abeda5924",
   "metadata": {},
   "source": [
    "## üß™ Example: Evaluating Your Model on the LEADERBOARD Benchmark\n",
    "\n",
    "**Using the Command-Line Interface (CLI)**\n",
    "\n",
    "Let‚Äôs illustrate how the LLM Evaluation Framework simplifies benchmarking by replicating the popular Hugging Face Open LLM Leaderboard setup‚Äîparticularly useful given its recent discontinuation. Here‚Äôs a practical CLI example that runs the complete leaderboard evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ae4b97-b2df-4866-bd45-da46b2540d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python llm_eval_cli.py \\\n",
    "  --model hf \\\n",
    "  --model_name meta-llama/Llama-2-13b-chat-hf \\\n",
    "  --leaderboard \\\n",
    "  --device cuda \\\n",
    "  --gpu_memory_utilization 0.9  # Adjust based on your GPU availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508664b-dda8-41c7-b7a0-449c2a9c6d0a",
   "metadata": {},
   "source": [
    "With this simple command, the framework evaluates your model across several key benchmarks including BBH, GPQA, MMLU-Pro, MUSR, IFEval, and Math-lvl-5, automatically configuring the appropriate few-shot examples for each benchmark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89315047-4f4f-4ab1-b256-a47cb6082338",
   "metadata": {},
   "source": [
    "**Using as a Python Library**\n",
    "\n",
    "Integrate the evaluation logic directly into your Python scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba1fdde-5507-4920-a9d6-6e70820b40d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_eval import evaluate_model\n",
    "\n",
    "# Run the evaluation\n",
    "results, output_path = evaluate_model(\n",
    "    model_type=\"hf\",\n",
    "    model_name=\"mistralai/Ministral-8B-Instruct-2410\",\n",
    "    tasks=[\"leaderboard\"],\n",
    "    num_samples=1,\n",
    "    device=\"cuda\",\n",
    "    quantize=True,\n",
    "    quantization_method=\"4bit\",\n",
    "    preserve_default_fewshot=True  # This ensures the correct few-shot settings for each benchmark task\n",
    ")\n",
    "\n",
    "# Print the paths to the results and report\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# The report path is derived from the output path\n",
    "import os\n",
    "from llm_eval.reporting.report_generator import get_reports_dir\n",
    "\n",
    "# Get the base filename without extension\n",
    "basename = os.path.basename(output_path)\n",
    "basename = os.path.splitext(basename)[0]\n",
    "\n",
    "# Construct the report path\n",
    "reports_dir = get_reports_dir()\n",
    "report_path = os.path.join(reports_dir, f\"{basename}_report.md\")\n",
    "\n",
    "if os.path.exists(report_path):\n",
    "    print(f\"Report generated at: {report_path}\")\n",
    "else:\n",
    "    print(\"Report was not generated. Check if there were any errors during evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb3de2-61e7-47c4-9971-ba1e70c132c2",
   "metadata": {},
   "source": [
    "## üìä Reporting and Results\n",
    "\n",
    "The framework generates:\n",
    "- JSON Results: Detailed results for each task, including individual sample predictions (if applicable), metrics, and configuration details, saved in the results/ directory.\n",
    "  \n",
    "- **Markdown Reports**: A summary report aggregating scores across tasks, generated in the reports/ directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a86e5d",
   "metadata": {},
   "source": [
    "## üìÑ How the Evaluation Report Looks\n",
    "\n",
    "When you run an evaluation using the LLM Evaluation Framework, it generates comprehensive yet easy-to-understand reports in both Markdown and JSON formats. Here‚Äôs a broad overview of what you can expect from the Markdown report:\n",
    "\n",
    "### 1. üìä Summary of Metrics\n",
    "\n",
    "This section offers a concise table summarizing your model‚Äôs performance across each task evaluated. Each row clearly indicates:\n",
    "\n",
    "‚Ä¢ Task: The specific benchmark or task evaluated (e.g., leaderboard_bbh_boolean_expressions).\n",
    "\n",
    "‚Ä¢ Metric: The evaluation metric employed (e.g., accuracy, exact match).\n",
    "\n",
    "‚Ä¢ Value: Your model‚Äôs performance score on that task.\n",
    "\n",
    "This summary makes it easy to quickly gauge overall performance across multiple tasks at a glance.\n",
    "\n",
    "### 2. üìà Normalized Scores\n",
    "\n",
    "To provide clearer insights, the framework calculates normalized scores, presenting a straightforward percentage-based representation of your model‚Äôs performance relative to established benchmarks. Each benchmark will show:\n",
    "\n",
    "‚Ä¢ Benchmark: Name of the benchmark.\n",
    "\n",
    "‚Ä¢ Score: Normalized percentage score.\n",
    "\n",
    "This helps you quickly pinpoint your model‚Äôs relative strengths and identify areas needing improvement.\n",
    "\n",
    "### 3. üîç Task Samples (Detailed Examples)\n",
    "\n",
    "The detailed samples section gives you valuable qualitative insights into your model‚Äôs performance by presenting clear examples directly from evaluated tasks. Each example includes:\n",
    "\n",
    "‚Ä¢ Question: The evaluation sample question posed to your model.\n",
    "\n",
    "‚Ä¢ Ground Truth: The expected correct answer.\n",
    "\n",
    "‚Ä¢ Model Response: Your model‚Äôs exact response, explicitly marked as correct or incorrect.\n",
    "\n",
    "These detailed examples are especially useful for conducting error analysis, allowing you to dive deeper into how your model handles specific questions or scenarios.\n",
    "\n",
    "### ‚öôÔ∏è Customization\n",
    "\n",
    "Beyond these default outputs, the reporting mechanism in this framework is highly customizable. You can easily extend or modify report generation logic to meet specialized requirements or incorporate additional analysis, enabling deeper and more tailored insights into your model‚Äôs performance.\n",
    "\n",
    "By providing structured and comprehensive reports, this framework empowers you to effectively evaluate, understand, and communicate the strengths and limitations of your large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007b73e-bbb3-4578-9311-75639696a9eb",
   "metadata": {},
   "source": [
    "## üîß Extending the Framework\n",
    "\n",
    "The modular design makes it easier to add new functionalities:\n",
    "\n",
    "1. Adding New Tasks/Benchmarks:\n",
    "- Define the task configuration in llm_eval/tasks/task_registry.py or a similar configuration file.\n",
    "- Ensure the task is compatible with the lm-evaluation-harness structure or adapt it.\n",
    "2. Supporting New Model Backends:\n",
    "- Create a new model handler class in llm_eval/models/ inheriting from a base model class (if applicable).\n",
    "- Implement the required methods for loading, inference, etc.\n",
    "- Register the new backend type. Ôøº\n",
    "3. Customizing Reporting:\n",
    "- Modify the report generation logic in llm_eval/reporting/ to change the format or content of the Markdown/JSON outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390bb9f",
   "metadata": {},
   "source": [
    "## ü§ù Contributing\n",
    "\n",
    "Contributions are welcome! Please follow standard practices:\n",
    "\n",
    "1. Fork the repository.\n",
    "2. Create a new branch for your feature or bug fix (git checkout -b feature/my-new-feature).\n",
    "3. Make your changes and commit them (git commit -am 'Add some feature').\n",
    "4. Push to the branch (git push origin feature/my-new-feature).\n",
    "5. Create a new Pull Request."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
