{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /4bit_quantization/\n",
    "categories:\n",
    "- Large Language Models\n",
    "colab: <a href=\"https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing\"><img src=\"images/colab.png\" alt=\"Open In Colab\"></a>\n",
    "date: '2023-07-30'\n",
    "image: /images/4bitquant/thumbnail.jpg\n",
    "title: \"4-bit LLM Quantization with GPTQ\"\n",
    "subtitle: \"Quantize your own open-source LLMs to run them on consumer hardware\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yezrHxYvg_wR"
   },
   "source": [
    "<center><img src=\"/images/4bitquant/thumbnail.jpg\"></center>\n",
    "\n",
    "::: {.column-margin}\n",
    "Find many more architectures and applications using graph neural networks in my book, [**Hands-On Graph Neural Networks**](https://mlabonne.github.io/blog/book.html) ðŸ‘‡\n",
    "<a href=\"https://packt.link/a/9781804617526\"><img src=\"/images/gnnbook/cover.png\" alt=\"Hands-On Graph Neural Networks Using Python\" id=\"gnn-book\"></a>\n",
    ":::\n",
    "\n",
    "Recent advancements in weight quantization allow us to run massive large language models on consumer hardware, like a LLaMA-30B model on an RTX 3090 GPU. This is possible thanks to novel 4-bit quantization techniques with minimal performance degradation, like [GPTQ](https://arxiv.org/abs/2210.17323), [GGML](https://github.com/ggerganov/ggml), and [NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes).\n",
    "\n",
    "In the [previous article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html), we introduced naÃ¯ve 8-bit quantization techniques and the excellent LLM.int8(). In this article, we will explore the popular GPTQ algorithm to understand how it works and implement it using the [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) library.\n",
    "\n",
    "You can find the code on [Google Colab](https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing) and [GitHub](https://github.com/mlabonne/llm-course/tree/main).\n",
    "\n",
    "## ðŸ§  Optimal Brain Quantization\n",
    "\n",
    "Let's start by introducing the problem we're trying to solve. For every layer $\\ell$ in the network, we want to find a quantized version $\\widehat{\\mathbf{W}}_\\ell$ of the original weights $\\mathbf{W}_\\ell$. This is called the **layer-wise compression problem**. More specifically, to minimize performance degradation, we want the outputs ($\\mathbf{\\widehat{W}_\\ell X_\\ell}$) of these new weights to be as close as possible to the original ones ($\\mathbf{W_\\ell X_\\ell}$). In other words, we want to find:\n",
    "\n",
    "$$\\arg \\min_{\\mathbf{\\widehat{W}}_\\ell} \\parallel\\mathbf{W_\\ell X_\\ell} - \\mathbf{\\widehat{W}_\\ell X_\\ell}\\parallel_2^2.$$\n",
    "\n",
    "Different approaches have been proposed to solve this problem, but we're interested in the [**Optimal Brain Quantizer**](https://arxiv.org/abs/2208.11580) (OBQ) framework here.\n",
    "\n",
    "This method is inspired by a **pruning technique** to carefully remove weights from a fully trained dense neural network (Optimal Brain Surgeon). It uses an approximation technique and provides explicit formulas for the best single weight $w_q$ to remove and optimal update $\\delta_F$ to adjust the set of remaining non-quantized weights $F$ to make up for the removal:\n",
    "\n",
    "\\begin{align*}\n",
    "w_q &= \\arg\\min_{w_q} \\frac{(\\text{quant}(w_q) - w_q)^2}{[\\mathbf{H}_F^{-1}]_{qq}},\\\\ \\quad \\delta_F &= -\\frac{w_q - \\text{quant}(w_q)}{[\\mathbf{H}_F^{-1}]_{qq}} \\cdot (\\mathbf{H}_F^{-1})_{:,q}.\n",
    "\\end{align*}\n",
    "\n",
    "where $\\text{quant}(w)$ is the weight rounding given by the quantization and $\\mathbf{H}_F$ is the Hessian.\n",
    "\n",
    "Using OBQ, we can quantize the easiest weight first and then adjust all remaining non-quantized weights to **compensate for this precision loss**. Then we pick the next weight to quantize, and so on.\n",
    "\n",
    "A potential issue with this approach is when there are outlier weights, which can result in high **quantization error**. Usually, these outliers would be quantized last, when there are few non-quantized weights left that could be adjusted to compensate for the large error. This effect can worsen when some weights are pushed further outside the grid by intermediate updates. A simple heuristic is applied to prevent this: outliers are quantized as soon as they appear.\n",
    "\n",
    "This process could be computationally heavy, especially for LLMs. To deal with this, the OBQ method uses a trick that avoids redoing the entire computation each time a weight is simplified. After quantizing a weight, it adjusts the matrix used in calculations (the Hessian) by **removing the row and column** associated with that weight (using Gaussian elimination):\n",
    "\n",
    "$$\n",
    "\\mathbf{H}^{-1}_{-q} = \\left( \\mathbf{H}^{-1} - \\frac{1}{[\\mathbf{H}^{-1}]_{qq}} \\mathbf{H}^{-1}_{:,q} \\mathbf{H}^{-1}_{q,:} \\right)_{-p}.\n",
    "$$\n",
    "\n",
    "The method also employs vectorization to process multiple rows of the weight matrix at once. Despite its efficiency, the OBQ's computation time increases significantly as the size of the weight matrix increases. This cubic growth makes it difficult to use OBQ on very large models with billions of parameters.\n",
    "\n",
    "## ðŸ§® The GPTQ Algorithm\n",
    "\n",
    "Introduced by Frantar et al. (2023), the [GPTQ algorithm](https://arxiv.org/abs/2210.17323) takes inspiration from the OBQ method, but with significant improvements to scale it for (very) large language models.\n",
    "\n",
    "### Step 1: Arbitrary Order Insight\n",
    "\n",
    "The OBQ method selects weights (parameters in a model) for quantization in a certain order, determined by which will **add the least additional error**. However, GPTQ observes that for large models, quantizing weights in any fixed order can perform just as well. This is because even though some weights might introduce more error individually, they are quantized later in the process when there are few other weights left that could increase the error. So the order doesn't matter as much as we thought.\n",
    "\n",
    "Based on this insight, GPTQ aims to quantize all weights in the **same order for all rows** of a matrix. This makes the process faster because certain computations have to be done only once for each column, rather than once for each weight.\n",
    "\n",
    "<center><img src=\"/images/4bitquant/cholesky.png\"></center>\n",
    "\n",
    "### Step 2: Lazy Batch-Updates\n",
    "\n",
    "This scheme won't be fast because it requires updating a **huge matrix** with very few computations for each entry. This type of operation can't utilize the full compute capabilities of GPUs and will be slowed down by memory limitations (memory throughput bottleneck).\n",
    "\n",
    "To resolve this, GPTQ introduces \"lazy batch\" updates. It turns out that the final rounding decisions for a given column are only affected by updates performed on that column, not on later columns. Therefore, GPTQ can apply the algorithm to a **batch of columns at a time** (like 128 columns), updating only those columns and a corresponding block of the matrix. After a block is fully processed, the algorithm performs global updates on the entire matrix.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{\\delta}_F &= -(\\mathbf{w}_Q - \\text{quant}(\\mathbf{w}_Q))([\\mathbf{H}_F^{-1}]_{QQ})^{-1} (\\mathbf{H}_F^{-1})_{:,Q}, \\\\\n",
    "\\mathbf{H}^{-1}_{-Q} &= \\left(\\mathbf{H}^{-1} - \\mathbf{H}^{-1}_{:,Q}([\\mathbf{H}_F^{-1}]_{QQ})^{-1}\\mathbf{H}^{-1}_{Q,:}\\right)_{-Q}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### Step 3: Cholesky Reformulation\n",
    "\n",
    "However, there's one more issue to address. When the algorithm scales up to very large models, numerical inaccuracies can become a problem. Specifically, repeated applications of a certain operation can **accumulate numerical errors**.\n",
    "\n",
    "To tackle this, GPTQ uses a [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition), a numerically stable method for solving certain mathematical problems. It involves precomputing some required information from the matrix using the Cholesky method. This approach, combined with a slight \"dampening\" (adding a small constant to diagonal elements of the matrix), helps the algorithm to avoid numerical issues.\n",
    "\n",
    "The full algorithm can be summarized in a few steps:\n",
    "\n",
    "1. The GPTQ algorithm begins with a Cholesky decomposition of the Hessian inverse (a matrix that helps decide how to adjust the weights)\n",
    "2. It then runs in loops, handling batches of columns at a time.\n",
    "3. For each column in a batch, it quantizes the weights, calculates the error, and updates the weights in the block accordingly.\n",
    "4. After processing the batch, it updates all remaining weights based on the block's errors.\n",
    "\n",
    "The GPTQ algorithm was tested on various language generation tasks. It was compared with other quantization methods, like rounding all weights to the nearest quantized value (RTN). GPTQ was used with the BLOOM (176B parameters) and OPT (175B parameters) model families, and models were quantized using a **single NVIDIA A100 GPU**.\n",
    "\n",
    "## ðŸ’» Quantize an LLM with AutoGPTQ\n",
    "\n",
    "GPTQ has been very popular to create models in 4-bit precision that can efficiently run on GPUs. You can find many examples on the Hugging Face Hub, especially from [TheBloke](https://huggingface.co/TheBloke). If you're looking for an approach that is more CPU-friendly, [GGML](https://github.com/ggerganov/ggml) is currently your best option. Finally, the `transformers` library with `bitsandbytes` allows you to quantize a model when it's loaded using the `load_in_4bit=true` argument, which requires downloading full models and storing them in your RAM.\n",
    "\n",
    "Let's implement the GPTQ algorithm using the AutoGPTQ library and quantize a GPT-2 model. This requires a GPU, but a free T4 on Google Colab will do. We start by loading the libraries and defining the model we want to quantize (in this case, GPT-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BhufqqQAaz6e"
   },
   "outputs": [],
   "source": [
    "!BUILD_CUDA_EXT=0 pip install -q auto-gptq transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dg8NyBL0ZNyw"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Define base model and output directory\n",
    "model_id = \"gpt2\"\n",
    "out_dir = model_id + \"-GPTQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oj9HAsNaF_w"
   },
   "source": [
    "We now want to load the model and the tokenizer. The tokenizer is loaded using the classic `AutoTokenizer` class from the `transformers` library. On the other hand, we need to pass a specific configuration (`BaseQuantizeConfig`) to load the model.\n",
    "\n",
    "In this configuration, we can specify the number of bits to quantize (here, `bits=4`) and the group size (size of the lazy batch). Note that this group size is optional: we could also use **one set of parameters** for the entire weight matrix. In practice, these groups generally improve the quality of the quantization at a very low cost (especially with `group_size=1024`). The `damp_percent` value is here to help the Cholesky reformulation and should not be changed.\n",
    "\n",
    "Finally, the `desc_act` (also called act order) is a tricky parameter. It allows you to **process rows based on decreasing activation**, meaning the most important or impactful rows (determined by sampled inputs and outputs) are processed first. This method aims to place most of the quantization error (inevitably introduced during quantization) on less significant weights. This approach improves the overall accuracy of the quantization process by ensuring the most significant weights are processed with greater precision. However, when used alongside group size, `desc_act` can lead to performance slowdowns due to the need to frequently reload quantization parameters. For this reason, we won't use it here (it will probably be fixed in the future, however)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C9352jN0ZP6I"
   },
   "outputs": [],
   "source": [
    "# Load quantize config, model and tokenizer\n",
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=128,\n",
    "    damp_percent=0.01,\n",
    "    desc_act=False,\n",
    ")\n",
    "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia0C_ExVmTz7"
   },
   "source": [
    "The quantization process **relies heavily on samples** to evaluate and enhance the quality of the quantization. They provide a means of comparison between the outputs produced by the origina and the newly quantized model. The larger the number of samples provided, the greater the potential for more accurate and effective comparisons, leading to improved quantization quality.\n",
    "\n",
    "In the context of this article, we utilize the **[C4 (Colossal Clean Crawled Corpus) dataset](https://huggingface.co/datasets/c4)** to generate our samples. The C4 dataset is a large-scale, multilingual collection of web text gathered from the Common Crawl project. This expansive dataset has been cleaned and prepared specifically for training large-scale language models, making it a great resource for tasks such as this. The WikiText dataset is another popular option.\n",
    "\n",
    "In the following code block, we load 1024 samples from the C4 dataset, tokenize them, and format them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wuBLe6aZSe-",
    "outputId": "e4ebd71a-2854-4347-cebe-08cf040d1eb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/allenai___json/allenai--c4-6e494e9c0ee1404e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2441065 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Load data and tokenize examples\n",
    "n_samples = 1024\n",
    "data = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.00001-of-01024.json.gz\", split=f\"train[:{n_samples*5}]\")\n",
    "tokenized_data = tokenizer(\"\\n\\n\".join(data['text']), return_tensors='pt')\n",
    "\n",
    "# Format tokenized examples\n",
    "examples_ids = []\n",
    "for _ in range(n_samples):\n",
    "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
    "    j = i + tokenizer.model_max_length\n",
    "    input_ids = tokenized_data.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRm-2b8_2Q64"
   },
   "source": [
    "Now that dataset is ready, we can start the quantization process with a batch size of 1. Optionally, we also use [OpenAI Triton](https://github.com/openai/triton), a CUDA alternative, to communicate with the GPU. Once this is done, we save the tokenizer and the model in a safetensors format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETsG2iYrXaUg",
    "outputId": "e48b825e-0ebc-4a73-dbfd-b5571cafd24e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 35s, sys: 3.49 s, total: 4min 39s\n",
      "Wall time: 5min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gpt2-GPTQ/tokenizer_config.json',\n",
       " 'gpt2-GPTQ/special_tokens_map.json',\n",
       " 'gpt2-GPTQ/vocab.json',\n",
       " 'gpt2-GPTQ/merges.txt',\n",
       " 'gpt2-GPTQ/added_tokens.json',\n",
       " 'gpt2-GPTQ/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Quantize with GPTQ\n",
    "model.quantize(\n",
    "    examples_ids,\n",
    "    batch_size=1,\n",
    "    use_triton=True,\n",
    ")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_quantized(out_dir, use_safetensors=True)\n",
    "tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mERM7fp32vHm"
   },
   "source": [
    "As per usual, the model and tokenizer can then be loaded from the output directory using the `AutoGPTQForCausalLM` and `AutoTokenizer` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nktu1FsdZ9sd",
    "outputId": "9943c829-1b58-474a-f245-6aefa09d81dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.modeling:The safetensors archive passed at gpt2-GPTQ/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "WARNING:auto_gptq.modeling._base:GPT2GPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
      "WARNING:auto_gptq.modeling._base:GPT2GPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    out_dir,\n",
    "    device=device,\n",
    "    use_triton=True,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFTgcVO6240z"
   },
   "source": [
    "Let's check that the model is working correctly. The AutoGPTQ model (mostly) works as a normal `transformers` model, which makes it compatible with inference pipelines, as shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRhIGrXdiFdt",
    "outputId": "6dca2078-6f01-44da-9895-3a03bdfb4b5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPT2GPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream,\" she told CNN last week. \"I have this dream of helping my mother find her own. But, to tell that for the first time, now that I'm seeing my mother now, just knowing how wonderful it is that\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "result = generator(\"I have a dream\", do_sample=True, max_length=50)[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP0BzjBK3RcV"
   },
   "source": [
    "We managed to get a convincing completion from our quantized GPT-2 model. A more in-depth evaluation would require **measuring the perplexity** of the quantized model versus the original one. However, we will leave it out of the scope of this article.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this article, we introduced the GPTQ algorithm, a state-of-the-art quantization technique to run LLMs on consumer-grade hardware. We showed how it addresses the layer-wise compression problem, based on an improved OBS technique with arbitrary order insight, lazy batch updates, and Cholesky reformulation. This novel approach **significantly reduces memory and computation requirements**, making LLMs accessible to a broader audience.\n",
    "\n",
    "In addition, we **quantized our own LLM model** on a free T4 GPU and ran it to generate text. You can push your own version of a GPTQ 4-bit quantized model on the Hugging Face Hub. As mentioned in the introduction, GPTQ is not the only 4-bit quantization algorithm: [GGML](https://github.com/ggerganov/ggml) and [NF4](https://huggingface.co/blog/4bit-transformers-bitsandbytes) are excellent alternatives with slightly different scopes. I encourage you to learn more about them and give them a shot!\n",
    "\n",
    "If you're interested in more technical content around LLMs, follow me on Twitter [@maximelabonne](https://twitter.com/maximelabonne).\n",
    "\n",
    "## References\n",
    "\n",
    "* B. Hassibi, D. G. Stork and G. J. Wolff, [\"Optimal Brain Surgeon and general network pruning,\"](https://ieeexplore.ieee.org/document/298572) IEEE International Conference on Neural Networks, San Francisco, CA, USA, 1993, pp. 293-299 vol.1, doi: 10.1109/ICNN.1993.298572.\n",
    "* Elias Frantar, Sidak Pal Singh, & Dan Alistarh. (2023). [Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning](https://arxiv.org/abs/2208.11580).\n",
    "* Elias Frantar, Saleh Ashkboos, Torsten Hoefler, & Dan Alistarh. (2023). [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323).\n",
    "* Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, & Peter J. Liu. (2020). [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683v3).\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
