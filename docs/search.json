[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Articles",
    "section": "",
    "text": "Fine-Tune TinyLlama with SFT\n\n\nLow-Rank Adapter Model Fine-tuning\n\n\n\nLarge Language Models\n\n\n\n\n\n\nJun 12, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Matthias De Paolis",
    "section": "",
    "text": "Matthias De Paolis is a Machine Learning Engineer and Data Scientist at Yarowa AG, serving as the lead AI Engineer. He holds a Master in Applied Information and Data Science from the university of applied science in Lucerne.\nConnect with him on LinkedIn @matthiasdepaolis.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "",
    "text": "Supervised fine-tuning of large language models is a critical process in natural language processing that involves the adaptation of pre-trained models to specific tasks. This process begins with a large language model that has been pre-trained on a vast corpus of text data, allowing it to understand the nuances of language, including grammar, syntax, and semantics. However, these models, while adept at generating human-like text, often require further training to perform well on specific tasks, such as sentiment analysis, question answering, or named entity recognition. This is where supervised fine-tuning comes into play. In this process, the pre-trained model is further trained on a labeled dataset related to the specific task. The labeled data provides the model with the necessary supervision to learn how to perform the task effectively.\nTo make fine-tuning more efficient and to prevent overfitting, techniques like LoRA (Low-Rank Adapters) are used. LoRA is a method that freezes the pre-trained model‚Äôs weights and inserts trainable rank decomposition matrices into the model. These matrices, or adapters, are of lower rank than the original weight matrices, significantly reducing the number of trainable parameters. This not only makes the fine-tuning process more computationally efficient but also helps in preserving the broad language understanding the model gained during pre-training.\nThe fine-tuning process adjusts these adapters to optimize the model‚Äôs performance on the task. This combination of pre-training, supervised fine-tuning, and techniques like LoRA has proven to be a powerful approach in natural language processing. It allows for the creation of models that not only understand language but can also apply that understanding to solve specific problems accurately and efficiently.\n!ls\n\nFine_Tune_TinyLlama_with_SFT.ipynb thumbnail.jpg\n_metadata.yml",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-model",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-model",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Load Model",
    "text": "Load Model\n\n%env HF_HUB_ENABLEHF_TRANSFER = True\n\nenv: HF_HUB_ENABLEHF_TRANSFER=True\n\n\n\n#!pip install wandb -q -U\nimport wandb\nimport os\n\n#%env WANDB_NOTEBOOK_NAME = $Fine_tune_tinyllama_with_DPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\n\nenv: WANDB_API_KEY=738485c7fc8f7262c37f7daab2a829956acea924\n\n\nwandb: Currently logged in as: matthias-depaolis (demaz). Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\n\nTrue\n\n\n\n#!python -m pip install --upgrade pip\n#!pip install -U -q transformers\n#!pip install -U -q bitsandbytes\n#!pip install -U -q peft\n#!pip install -U -q accelerate\n#!pip install -U -q scipy\n#!pip install -U -q trl\n\n\ncache_dir = ''\n\nmodel_id = \"./TinyLlama/TinyLlama_v1.1\"\nnew_model = \"./llmat/TinyLlama-1.1B_SFT\"",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-model-and-tokenizer-for-lora",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-model-and-tokenizer-for-lora",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Load the Model and Tokenizer for LoRA",
    "text": "Load the Model and Tokenizer for LoRA\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    #config=config,\n    #quantization_config=bnb_config,\n    #rope_scaling={'type': 'Linear', 'factor': 2.0},\n    device_map='auto',\n    attn_implementation = \"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    cache_dir=cache_dir\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True, cache_dir=cache_dir)\n\nin oss file",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#loading-checks",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#loading-checks",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Loading Checks",
    "text": "Loading Checks\n\n# Check there are no parameters overflowing onto cpu (meta).\nfor n, p in model.named_parameters():\n    if p.device.type=='meta':\n        print(f\"{n} is on meta!\")\n\n\nprint(model.config.max_position_embeddings)\nprint(model.config.eos_token_id)\n\n2048\n2",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-and-run-training",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-and-run-training",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Set up and run Training",
    "text": "Set up and run Training\n\nmodel_name = model_id.split('/')[-1]\n\nepochs=1\ngrad_accum=4\nbatch_size=8\nfine_tune_tag='SFT'\nsave_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\nprint(save_dir)\n\n./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT\n\n\n\nimport transformers\nimport os\nimport torch\n\n# Custom callback to log metrics\nclass LoggingCallback(transformers.TrainerCallback):\n    def __init__(self, log_file_path):\n        self.log_file_path = log_file_path\n\n    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n        with open(self.log_file_path, 'a') as f:\n            if 'loss' in logs:\n                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n                if 'eval_loss' in logs:\n                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n                f.flush()  # Force flush the buffered data to file\n\n        # Check if the current step is a checkpoint step\n        if state.global_step % int(args.save_steps) == 0:\n            # Check if the last checkpoint path exists\n            if state.best_model_checkpoint:\n                checkpoint_dir = state.best_model_checkpoint\n            else:\n                # If not, construct the checkpoint directory path\n                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n\n            # Ensure the checkpoint directory exists\n            os.makedirs(checkpoint_dir, exist_ok=True)\n\n            # Save trainable params in the checkpoint directory\n            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n            torch.save(current_trainable_params_state_dict, file_path)\n\n# Log file path\ncache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\nlog_file_path = os.path.join(cache_dir, 'training_logs.txt')\n\n# Create an instance of the custom callback\nlogging_callback = LoggingCallback(log_file_path)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#sft",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#sft",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "SFT",
    "text": "SFT\n\n# Remove unnecessary columns\ntrain = train.remove_columns(['prompt', 'chosen', 'rejected'])\n\nprint(train)\n\nDataset({\n    features: ['messages'],\n    num_rows: 43245\n})\n\n\n\nfrom transformers import Trainer\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    # peft_config=peft_config, # commet out if passing a peft model\n    max_seq_length=1024,\n    tokenizer=tokenizer, # Trainer uses the chat template passsed by the tokenizer. If data consist only of the column messages, this is fine\n    model=model,\n    train_dataset=train,\n    eval_dataset=test,\n    args=transformers.TrainingArguments(\n        # max_step=1, # comment this out after the first time you...\n        save_steps=150, ### MAKE SURE TO CHECK THIS VALUE IS GOOD FOR\n        logging_steps=1,\n        num_train_epochs=epochs,\n        output_dir=save_dir,\n        eval_strategy='steps',\n        do_eval=True,\n        eval_steps=0.2,\n        per_device_eval_batch_size=batch_size,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=grad_accum,\n        log_level='debug',\n        #optim='paged_adamw_8bit',\n        #fp16=True, # For non-Ampere GPUs\n        bf16=True, # For Ampere GPUs or later\n        max_grad_norm=0.3,\n        lr_scheduler_type='linear',\n        #hub_private_repo=True,\n        warmup_ratio=0.03, # optional, may help stability at the (learning rate is ower for the first stepts)\n        optim='adamw_torch', # comment out for LoRA +\n        learning_rate=1e-4, # comment out for LoRA +\n    ),\n\n    callbacks=[logging_callback] # Add custom callback here\n    # neftune_noise_alpha=5 # Aff in noise to embeddings to improve...\n)\n        \n\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\nPyTorch: setting up devices\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\nUsing auto half precision backend\n\n\n\nmodel.config.use_cache=False # silence the warnings\ntrainer.train()\n\nCurrently training with a batch size of: 8\n***** Running training *****\n  Num examples = 43,245\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 4\n  Total optimization steps = 1,351\n  Number of trainable parameters = 65,628,160\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\nwandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\n\nTracking run with wandb version 0.17.1\n\n\nRun data is saved locally in /workspace/_LEARNING/me/FineTuning/wandb/run-20240612_091154-2f6so3xa\n\n\nSyncing run ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/demaz/huggingface\n\n\n View run at https://wandb.ai/demaz/huggingface/runs/2f6so3xa\n\n\n\n    \n      \n      \n      [1351/1351 1:11:40, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n271\n1.450700\n1.353275\n\n\n542\n1.271700\n1.335227\n\n\n813\n1.317600\n1.329108\n\n\n1084\n1.406600\n1.327019\n\n\n\n\n\n\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTrainOutput(global_step=1351, training_loss=1.3279773617214312, metrics={'train_runtime': 4304.6658, 'train_samples_per_second': 10.046, 'train_steps_per_second': 0.314, 'total_flos': 2.616444791832576e+17, 'train_loss': 1.3279773617214312, 'epoch': 0.9996300406955235})\n\n\n\ntrainer.save_model(new_model)\n\nSaving model checkpoint to ./llmat/TinyLlama-1.1B_SFT\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./llmat/TinyLlama-1.1B_SFT/tokenizer_config.json\nSpecial tokens file saved in ./llmat/TinyLlama-1.1B_SFT/special_tokens_map.json",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#plotting",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#plotting",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Plotting",
    "text": "Plotting\n\nimport matplotlib.pyplot as plt\n\n# Initialize lists to hold training and evaluation losses and steps\ntrain_losses = []\neval_losses = []\ntrain_steps = []\neval_steps = []\n\n# Populate the lists from the log history\nfor entry in trainer.state.log_history:\n    if 'loss' in entry:\n        train_losses.append(entry['loss'])\n        train_steps.append(entry['step'])\n    if 'eval_loss' in entry:\n        eval_losses.append(entry['eval_loss'])\n        eval_steps.append(entry['step'])\n\n# Plot the losses\nplt.plot(train_steps, train_losses, label='Train Loss')\nplt.plot(eval_steps, eval_losses, label='Eval Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#save-trainable-params-if-training-non-lora-modules",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#save-trainable-params-if-training-non-lora-modules",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Save trainable params if training non-LoRA modules",
    "text": "Save trainable params if training non-LoRA modules\n\n# Update the dictionary to reflect the final state of the model's\ntrainable_params_state_dict = {n: p.data for n, p in model.named_parameters() if p.requires_grad}\n\n# Save the final state of the trainable parameters (ONLY RELEVANT FOR NON-LORA ADAPTERS)\nfinal_save_path = os.path.join(save_dir, 'trainable_params_final.pt')\ntorch.save(trainable_params_state_dict, final_save_path)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  }
]