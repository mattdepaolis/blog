[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Articles",
    "section": "",
    "text": "Fine-Tune Mistral v0.3 with ORPO and Unsloth\n\n\nLow-Rank Adapter Model Fine-tuning\n\n\n\nLarge Language Models\n\n\n\n\n\n\nJul 30, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Matthias De Paolis",
    "section": "",
    "text": "Matthias De Paolis is a Machine Learning Engineer and Data Scientist at Yarowa AG, serving as the lead AI Engineer. He holds a Master in Applied Information and Data Science from the university of applied science in Lucerne.\nConnect with him on LinkedIn @matthiasdepaolis.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "",
    "text": "Supervised fine-tuning of large language models is a crucial step in natural language processing. It‚Äôs about taking a pre-trained model and making it really good at specific tasks. You start with a model that‚Äôs been trained on a massive amount of text. This gives it a solid understanding of language, including grammar, syntax, and semantics. However, while these models can generate text that sounds human, they often need more training to excel at particular tasks like sentiment analysis, question answering, or named entity recognition. This is where supervised fine-tuning comes in. You take the pre-trained model and train it further on a labeled dataset that‚Äôs specific to the task. The labeled data provides the guidance the model needs to learn how to perform the task well.\nAfter pretraining, auto-regressive models like TinyLlama can predict the next token in a sequence. However, this capability alone doesn‚Äôt make them particularly useful as assistants since they don‚Äôt inherently respond to instructions. That‚Äôs why we use instruction tuning to align their responses with human expectations. Instruction tuning involves two main techniques:\nThe data typically has two columns: User and Assistant. The rows are created by humans or other language models to help the model get used to the flow of conversation.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-model",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-model",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Load Model",
    "text": "Load Model\n\n%env HF_HUB_ENABLEHF_TRANSFER = True\n\nenv: HF_HUB_ENABLEHF_TRANSFER=True\n\n\n\n#!pip install wandb -q -U\nimport wandb\nimport os\n\n#%env WANDB_NOTEBOOK_NAME = $Fine_tune_tinyllama_with_DPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\n\n#!python -m pip install --upgrade pip\n#!pip install -U -q transformers\n#!pip install -U -q bitsandbytes\n#!pip install -U -q peft\n#!pip install -U -q accelerate\n#!pip install -U -q scipy\n#!pip install -U -q trl\n#!pip install -U -q torch\n\n\ncache_dir = ''\n\nmodel_id = \"./TinyLlama/TinyLlama_v1.1\"\nnew_model = \"./llmat/TinyLlama-1.1B_SFT\"",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-model-and-tokenizer-for-lora",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-model-and-tokenizer-for-lora",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Load the Model and Tokenizer for LoRA",
    "text": "Load the Model and Tokenizer for LoRA\nIn this example, we are using the TinyLlama base model (1.1B parameters). If you have a smaller GPU, such as those available in a Google Colab notebook, you can use quantization to make the model fit. Quantization reduces the size of the model by representing its weights with fewer bits.\nWhen specifying the data type:\n\nUse bfloat16 for newer GPUs like the A6000 or A100.\nUse float16 for other GPUs. Also, if you are using Colab, you might need to disable Flash Attention, which is an optimization technique for faster processing.\n\nCode Explanation: 1. Set the directories for the model: Define where your models are located.\n\nmodel_id is the path to the TinyLlama base model.\nnew_model is the path to the modified TinyLlama model.\n\n\nImport necessary libraries:\n\n\nAutoTokenizer and AutoModelForCausalLM from the transformers library are used to load the tokenizer and model.\ntorch is the PyTorch library, used for tensor computations and model handling.\nBitsAndBytesConfig is used for model quantization settings.\n\n\nConfigure quantization settings: Create a configuration object bnb_config to specify quantization parameters:\n\n\nload_in_4bit=True: Load the model weights in 4-bit precision.\nbnb_4bit_use_double_quant=True: Use double quantization for better precision.\nbnb_4bit_quant_type=‚Äònf4‚Äô: Specify the type of quantization.\nbnb_4bit_compute_dtype=torch.bfloat16: Set the data type for computation to bfloat16.\n\n\nLoad the model: Use the AutoModelForCausalLM.from_pretrained function to load the model with the specified configurations.\n\n\nmodel_id: Path to the TinyLlama base model.\ndevice_map=‚Äòauto‚Äô: Automatically map the model to available devices (GPUs/CPUs).\ntorch_dtype=torch.bfloat16: Set the data type for the model.\n\n\nLoad the tokenizer: Use the AutoTokenizer.from_pretrained function to load the tokenizer.\n\n\ncache_dir = ''\n\nmodel_id = \"./TinyLlama/TinyLlama_v1.1\"\nnew_model = \"./llmat/TinyLlama-1.1B_SFT\"\n\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    #config=config,\n    #quantization_config=bnb_config,\n    #rope_scaling={'type': 'Linear', 'factor': 2.0},\n    device_map='auto',\n    #attn_implementation = \"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    cache_dir=cache_dir\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True, cache_dir=cache_dir)\n\nin oss file",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#loading-checks",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#loading-checks",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Loading Checks",
    "text": "Loading Checks\nAfter loading the model, it‚Äôs crucial to ensure that all the model‚Äôs parameters are correctly loaded onto the GPU. If any parameters are still on the CPU (referred to as the ‚Äúmeta device‚Äù), it means they haven‚Äôt been properly moved to the GPU.\nCode Explanation: 1. Check parameter locations: Iterate through all the parameters of the model to verify their device location.\n\nmodel.named_parameters(): This function returns an iterator over model parameters, providing both the name (n) and the parameter itself (p).\np.device.type==‚Äòmeta‚Äô: This condition checks if a parameter is on the ‚Äúmeta‚Äù device, which indicates it is not on the GPU.\nIf any parameter is found on the ‚Äúmeta‚Äù device, a message is printed to identify which parameter is not correctly loaded.\n\n\nPrint model configurations: Display important configurations of the model.\n\n\nmodel.config.max_position_embeddings: This prints the maximum number of positions the model can handle, which relates to the input sequence length.\nmodel.config.eos_token_id: This prints the ID of the end-of-sequence token, which the model uses to identify the end of a text sequence.\n\n\n# Check there are no parameters overflowing onto cpu (meta).\nfor n, p in model.named_parameters():\n    if p.device.type=='meta':\n        print(f\"{n} is on meta!\")\n\n\nprint(model.config.max_position_embeddings)\nprint(model.config.eos_token_id)\n\n2048\n2",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-and-run-training",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-and-run-training",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Set up and run Training",
    "text": "Set up and run Training\n\nmodel_name = model_id.split('/')[-1]\n\nepochs=1\ngrad_accum=4\nbatch_size=8\nfine_tune_tag='SFT'\nsave_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\nprint(save_dir)\n\n./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT\n\n\n\nimport transformers\nimport os\nimport torch\n\n# Custom callback to log metrics\nclass LoggingCallback(transformers.TrainerCallback):\n    def __init__(self, log_file_path):\n        self.log_file_path = log_file_path\n\n    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n        with open(self.log_file_path, 'a') as f:\n            if 'loss' in logs:\n                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n                if 'eval_loss' in logs:\n                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n                f.flush()  # Force flush the buffered data to file\n\n        # Check if the current step is a checkpoint step\n        if state.global_step % int(args.save_steps) == 0:\n            # Check if the last checkpoint path exists\n            if state.best_model_checkpoint:\n                checkpoint_dir = state.best_model_checkpoint\n            else:\n                # If not, construct the checkpoint directory path\n                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n\n            # Ensure the checkpoint directory exists\n            os.makedirs(checkpoint_dir, exist_ok=True)\n\n            # Save trainable params in the checkpoint directory\n            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n            torch.save(current_trainable_params_state_dict, file_path)\n\n# Log file path\ncache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\nlog_file_path = os.path.join(cache_dir, 'training_logs.txt')\n\n# Create an instance of the custom callback\nlogging_callback = LoggingCallback(log_file_path)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#sft",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#sft",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "SFT",
    "text": "SFT\nSetting a fine-tuning tag (either SFT or ORPO), we then set up a logging callback, connect weights and biases, and clean the dataset for supervised fine-tuning. The SFT trainer automatically uses the chat template of the tokenizer, allowing for consistent formatting. We use a maximum sequence length of 1,000 tokens and a learning rate of 1e-4, with an optional warm-up period.\n\n# Remove unnecessary columns\ntrain = train.remove_columns(['prompt', 'chosen', 'rejected'])\n\nprint(train)\n\nDataset({\n    features: ['messages'],\n    num_rows: 43245\n})\n\n\n\nfrom transformers import Trainer\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    # peft_config=peft_config, # comment out if passing a peft model\n    max_seq_length=2048,\n    tokenizer=tokenizer, # Trainer uses the chat template passsed by the tokenizer. If data consist only of the column messages, this is fine\n    model=model,\n    train_dataset=train,\n    eval_dataset=test,\n    args=transformers.TrainingArguments(\n        save_steps=150,\n        logging_steps=1,\n        num_train_epochs=epochs,\n        output_dir=save_dir,\n        eval_strategy='steps',\n        do_eval=True,\n        eval_steps=0.2,\n        per_device_eval_batch_size=batch_size,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=grad_accum,\n        log_level='debug',\n        #optim='paged_adamw_8bit',\n        #fp16=True, # For non-Ampere GPUs\n        bf16=True, # For Ampere GPUs or later\n        max_grad_norm=0.3,\n        lr_scheduler_type='linear',\n        #hub_private_repo=True,\n        warmup_ratio=0.03, # optional, may help stability at the (learning rate is lower for the first steps)\n        optim='adamw_torch', \n        learning_rate=1e-4, \n    ),\n\n    callbacks=[logging_callback] # Add custom callback here\n    # neftune_noise_alpha=5 # Aff in noise to embeddings to improve...\n)\n        \n\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\nPyTorch: setting up devices\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\nUsing auto half precision backend\n\n\n\nmodel.config.use_cache=False # silence the warnings\ntrainer.train()\n\nCurrently training with a batch size of: 8\n***** Running training *****\n  Num examples = 43,245\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 4\n  Total optimization steps = 1,351\n  Number of trainable parameters = 65,628,160\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\nwandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\n\nTracking run with wandb version 0.17.1\n\n\nRun data is saved locally in /workspace/_LEARNING/me/FineTuning/wandb/run-20240612_091154-2f6so3xa\n\n\nSyncing run ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/demaz/huggingface\n\n\n View run at https://wandb.ai/demaz/huggingface/runs/2f6so3xa\n\n\n\n    \n      \n      \n      [1351/1351 1:11:40, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n271\n1.450700\n1.353275\n\n\n542\n1.271700\n1.335227\n\n\n813\n1.317600\n1.329108\n\n\n1084\n1.406600\n1.327019\n\n\n\n\n\n\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTrainOutput(global_step=1351, training_loss=1.3279773617214312, metrics={'train_runtime': 4304.6658, 'train_samples_per_second': 10.046, 'train_steps_per_second': 0.314, 'total_flos': 2.616444791832576e+17, 'train_loss': 1.3279773617214312, 'epoch': 0.9996300406955235})\n\n\n\ntrainer.save_model(new_model)\n\nSaving model checkpoint to ./llmat/TinyLlama-1.1B_SFT\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./llmat/TinyLlama-1.1B_SFT/tokenizer_config.json\nSpecial tokens file saved in ./llmat/TinyLlama-1.1B_SFT/special_tokens_map.json",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#plotting",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#plotting",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Plotting",
    "text": "Plotting\n\nimport matplotlib.pyplot as plt\n\n# Initialize lists to hold training and evaluation losses and steps\ntrain_losses = []\neval_losses = []\ntrain_steps = []\neval_steps = []\n\n# Populate the lists from the log history\nfor entry in trainer.state.log_history:\n    if 'loss' in entry:\n        train_losses.append(entry['loss'])\n        train_steps.append(entry['step'])\n    if 'eval_loss' in entry:\n        eval_losses.append(entry['eval_loss'])\n        eval_steps.append(entry['step'])\n\n# Plot the losses\nplt.plot(train_steps, train_losses, label='Train Loss')\nplt.plot(eval_steps, eval_losses, label='Eval Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nTraining results show a decrease in training loss and validation loss, indicating effective fine-tuning.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#save-trainable-params-if-training-non-lora-modules",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#save-trainable-params-if-training-non-lora-modules",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Save trainable params if training non-LoRA modules",
    "text": "Save trainable params if training non-LoRA modules\nAfter training, we save the trainable parameters and evaluate the model.\n\n# Update the dictionary to reflect the final state of the model's\ntrainable_params_state_dict = {n: p.data for n, p in model.named_parameters() if p.requires_grad}\n\n# Save the final state of the trainable parameters (ONLY RELEVANT FOR NON-LORA ADAPTERS)\nfinal_save_path = os.path.join(save_dir, 'trainable_params_final.pt')\ntorch.save(trainable_params_state_dict, final_save_path)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#install-libraries",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#install-libraries",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Install Libraries",
    "text": "Install Libraries\nOur first step involves installing the necessary libraries. Once the installation is complete, we can proceed to load the model.\n\n#!python -m pip install --upgrade pip\n#!pip install -U -q transformers\n#!pip install -U -q bitsandbytes\n#!pip install -U -q peft\n#!pip install -U -q accelerate\n#!pip install -U -q scipy\n#!pip install -U -q trl\n#!pip install -U -q torch\n#!pip install wandb -q -U\n\n\n%env HF_HUB_ENABLEHF_TRANSFER = True\n\nenv: HF_HUB_ENABLEHF_TRANSFER=True\n\n\n\nimport wandb\nimport os\n\n#%env WANDB_NOTEBOOK_NAME = $Fine_tune_tinyllama_with_DPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#prepare-for-lora-fine-tuning",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#prepare-for-lora-fine-tuning",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Prepare for LoRA fine-tuning",
    "text": "Prepare for LoRA fine-tuning\nWe will fine-tune the model using Low-Rank Adaptation (LoRA) to reduce the VRAM (Video RAM) usage. LoRA helps to make the training process more efficient by focusing on specific parts of the model. In this case, we‚Äôll train the attention layers and the language model head (LM head) to ensure that the model can handle chat templating effectively. The chosen configuration uses a rank of 8 and a LoRA alpha of 32, which are parameters that control the adaptation process.\nThe function below prints the number of trainable parameters:\nCode Explanation: 1. Print trainable parameters: This function helps to understand which parts of the model can be trained and how many parameters are involved. - model.named_parameters(): This function returns an iterator over model parameters, providing both the name (name) and the parameter itself (param). - param.requires_grad: This condition checks if a parameter is trainable. - The function prints the names of all trainable parameters and counts the total number of trainable and non-trainable parameters.\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainabl√∂e parameters in the model and lists which parameters\n    \"\"\"\n    trainable_params = 0\n    non_trainable_params = 0\n    all_params = 0\n\n    print(\"Trainable Parameters\")\n    for name, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n            print(f\" {name}\")\n        else:\n            non_trainable_params += param.numel()\n\n    print(\"\\nNon-Trainable Parameters:\")\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            print(f\" {name}\")\n\n    print(\n        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n        \n\nSpecifically, we‚Äôll train the attention layers, the up and down layers, and the language model head (LM head) to ensure proper chat templating. The chosen configuration uses a rank of 8 and a LoRA alpha of 32.\nCode Explanation: 1. Enable gradient checkpointing: This feature saves VRAM by storing the intermediate activations during training.\n\nmodel.gradient_checkpointing_enable(): Enable this if you want to save VRAM.\n\n\nPrepare model for LoRA fine-tuning: Convert the model to use LoRA.\n\n\nprepare_model_for_kbit_training(model): Uncomment this if you are using quantization.\n\n\nDefine LoRA configuration: Set up the parameters for LoRA fine-tuning.\n\n\nLoraConfig: Create a configuration object for LoRA.\nr=8: Set the rank to 8.\nlora_alpha=32: Set the LoRA alpha to 32.\ntarget_modules: Specify the layers of the model to be fine-tuned.\nlora_dropout=0.1: Set the dropout rate for LoRA.\nbias=‚Äúnone‚Äù: Do not include additional biases.\ntask_type=‚ÄúCAUSAL_LM‚Äù: Specify the task type as causal language modeling.\n\n\nApply LoRA to the model: Convert the model to use the defined LoRA configuration.\n\n\nget_peft_model(model, peft_config): Apply the LoRA configuration to the model.\n\n\nfrom peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable() # Comment this in to save VRAM\n# model = prepare_model_for_kbit_training(model) # only set this if using quantization\n\nfrom peft import LoraConfig, get_peft_model\n\npeft_config = LoraConfig( # matching the Llama recipe\n    r=8,\n    lora_alpha=32,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        # \"self_attn.rotary_emb.inv_freq\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n        #\"input_layernorm.weight\", #can't be lora fine-tuned as it's not a linear layer\n        #\"post_attention_layernorm.weights, #can't be lora fine-tuned as it's not a linear layer\n        #\"model.norm.weight\", #can't be lora fine-tuned as it's not a linear layer\n    ],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, peft_config) # move to a peft model",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-you-are-changing-the-template",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-you-are-changing-the-template",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Set embed and norm layers to trainable (recommended for chat fine-tuning if you are changing the template)",
    "text": "Set embed and norm layers to trainable (recommended for chat fine-tuning if you are changing the template)\nAfter setting the pad token to the UN token already present in the tokenizer, we enable the trainability of the embedding and normalization layers. These layers cannot be adapted with LoRA, but fine-tuning them is crucial for adjusting the chat templating. Since we‚Äôre fine-tuning a base model unfamiliar with our chat format, making these layers trainable increases the number of trainable parameters, typically by 1-5%.\n\n# List to hold the names of the trainable parameters\ntrainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n\n# Set modules to be trainable\nfor n, p in model.named_parameters():\n    if any(k in n for k in trainable_params_names):\n        p.requires_grad_(True)\n    else:\n        p.requires_grad_(False) # Optional: Set the rest to be trainable\n\n# Make a dictionary of trainable parameters\ntrainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n\n# Convert trainable_params to state_dict format\ntrainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}\n\n\nprint_trainable_parameters(model)\n\nTrainable Parameters\n base_model.model.model.embed_tokens.weight\n base_model.model.model.layers.0.input_layernorm.weight\n base_model.model.model.layers.0.post_attention_layernorm.weight\n base_model.model.model.layers.1.input_layernorm.weight\n base_model.model.model.layers.1.post_attention_layernorm.weight\n base_model.model.model.layers.2.input_layernorm.weight\n base_model.model.model.layers.2.post_attention_layernorm.weight\n base_model.model.model.layers.3.input_layernorm.weight\n base_model.model.model.layers.3.post_attention_layernorm.weight\n base_model.model.model.layers.4.input_layernorm.weight\n base_model.model.model.layers.4.post_attention_layernorm.weight\n base_model.model.model.layers.5.input_layernorm.weight\n base_model.model.model.layers.5.post_attention_layernorm.weight\n base_model.model.model.layers.6.input_layernorm.weight\n base_model.model.model.layers.6.post_attention_layernorm.weight\n base_model.model.model.layers.7.input_layernorm.weight\n base_model.model.model.layers.7.post_attention_layernorm.weight\n base_model.model.model.layers.8.input_layernorm.weight\n base_model.model.model.layers.8.post_attention_layernorm.weight\n base_model.model.model.layers.9.input_layernorm.weight\n base_model.model.model.layers.9.post_attention_layernorm.weight\n base_model.model.model.layers.10.input_layernorm.weight\n base_model.model.model.layers.10.post_attention_layernorm.weight\n base_model.model.model.layers.11.input_layernorm.weight\n base_model.model.model.layers.11.post_attention_layernorm.weight\n base_model.model.model.layers.12.input_layernorm.weight\n base_model.model.model.layers.12.post_attention_layernorm.weight\n base_model.model.model.layers.13.input_layernorm.weight\n base_model.model.model.layers.13.post_attention_layernorm.weight\n base_model.model.model.layers.14.input_layernorm.weight\n base_model.model.model.layers.14.post_attention_layernorm.weight\n base_model.model.model.layers.15.input_layernorm.weight\n base_model.model.model.layers.15.post_attention_layernorm.weight\n base_model.model.model.layers.16.input_layernorm.weight\n base_model.model.model.layers.16.post_attention_layernorm.weight\n base_model.model.model.layers.17.input_layernorm.weight\n base_model.model.model.layers.17.post_attention_layernorm.weight\n base_model.model.model.layers.18.input_layernorm.weight\n base_model.model.model.layers.18.post_attention_layernorm.weight\n base_model.model.model.layers.19.input_layernorm.weight\n base_model.model.model.layers.19.post_attention_layernorm.weight\n base_model.model.model.layers.20.input_layernorm.weight\n base_model.model.model.layers.20.post_attention_layernorm.weight\n base_model.model.model.layers.21.input_layernorm.weight\n base_model.model.model.layers.21.post_attention_layernorm.weight\n base_model.model.model.norm.weight\n\nNon-Trainable Parameters:\n base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.0.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.0.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.1.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.1.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.2.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.2.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.3.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.3.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.4.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.4.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.5.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.5.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.6.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.6.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.7.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.7.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.8.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.8.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.9.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.9.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.10.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.10.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.11.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.11.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.12.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.12.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.13.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.13.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.14.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.14.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.15.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.15.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.16.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.16.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.17.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.17.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.18.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.18.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.19.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.19.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.20.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.20.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.21.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.21.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n base_model.model.lm_head.base_layer.weight\n base_model.model.lm_head.lora_A.default.weight\n base_model.model.lm_head.lora_B.default.weight\n\nSummary:\n Trainable params: 65628160\n Non-Trainable params: 1041000448\n All Parameters: 1106628608",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-evaluation",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-evaluation",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Set up Evaluation",
    "text": "Set up Evaluation\nTo evaluate the model‚Äôs performance, we define simple evaluation questions, such as ‚ÄúWhat is 1+1?‚Äù and ‚ÄúGive me some Python code to add the first five Fibonacci numbers.‚Äù Running this evaluation on the base model shows that it isn‚Äôt familiar with the chat template, as it repeats the question and provides suboptimal code responses.\n\nfrom transformers import TextStreamer\nfrom peft import PeftModel\nimport torch\nimport gc # import Python's garbage collection module\n\n# Define a stream\ndef stream(user_prompt, model_type, tokenizer, checkpoint=''):\n    if model_type == 'base':\n        eval_model = model\n    elif model_type == 'fine-tuned':\n        eval_model = PeftModel.from_pretrained(model, checkpoint)\n        eval_model = eval_model.to('cuda')\n\n        for n, p in eval_model.named_parameters():\n            if p.device.type == 'cpu':\n                print(f'{n} is on cpu!')\n    \n    else:\n        print(\"You must set the model_type to base or fine-tuned\")\n        exit()\n    \n    print(f'Proceeding to inference with peft adapters from {checkpoint}')\n    \n    eval_model.config.use_cache = True\n\n    messages = [\n        { 'role': 'user', 'content': f\"{user_prompt.strip()}\"},\n    ]\n\n    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    inputs = tokenizer([inputs], return_tensors='pt', add_special_tokens=False).to('cuda')\n\n    if \"token_type_ids\" in inputs:\n        del inputs[\"token_type_ids\"]\n    \n    streamer = TextStreamer(tokenizer)\n    \n    print(f'eval_model is on: {next(eval_model.parameters()).device}') # Debug line\n    print(f'input_ids are on: {inputs[\"input_ids\"].device}') # Debug line\n\n    # Despite returning the usal output, the streamer will also print the generated \n    #_ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=250, do_s)\n    _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=250, do_sample=True)\n    \n    # Clear GPU cache and run garbage collection\n    torch.cuda.empty_cache()\n    gc.collect()\n    \ndef evaluation(model_type, checkpoint=''):\n    questions = [\n        \"What is one plus one?\",\n        \"Give me some python code to add the first five Fibonacci numbers.\",\n    ]\n    \n    answers = [\n        \"Two.\",\n        \"...\",\n    ]\n    \n    for question , answer in zip(questions, answers):\n        stream(question, model_type, tokenizer, checkpoint)\n        print('\\n\\n')\n        \n\n\nprint(model.config)\n\nLlamaConfig {\n  \"_name_or_path\": \"./TinyLlama/TinyLlama_v1.1\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": 0,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.41.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n\n\n\nprint(model.generation_config)\n\nGenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"max_length\": 2048,\n  \"pad_token_id\": 0\n}\n\n\n\n\nevaluation('base', tokenizer)\n\nProceeding to inference with peft adapters from LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;unk&gt;'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n    0: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    1: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\neval_model is on: cuda:0\ninput_ids are on: cuda:0\n&lt;s&gt; [INST] What is one plus one? [/INST] IN TH MEMEME ME MEME MEMEME MEME MEMEME ME MEME MEME ME ME MEMEME E ME MEME MEMEME ME ME ME MEMEME MEME MEME MEMEME MEAT ME MEMEME ME DMEME MEMEME MEVER MEME EMEME TEMEME MEMEME MAR MARMAMMAMA MARMAT MARMAMMA MA MR MARMA MR M MAR MR MAR MR M MAR MR M MAR MR MR MR MR MR MR MR MR MR M MR MR MR MR MR MR MR M MR MR MR MR MAR MR MRMR MAR M MARMR MAR MAR MARMA MAR H MAR MARMA MAR Mar MAR MAR MAR MAMA RAT MR M MR MR MR MR MR MAR MAR MAR MA MR MR MR MR MR MRMMMRMMMRMRMMMR MYMA MR MR MR MR MRMRMRMPMRTR MR MRMRMM MR MR MRMA MR MRMR MR MR MRMRMRMRMR MR TUR MR MRMT MRM MRMR M TR MR MR MR MR MR MR MRMRMR MR T MR MR MR MR MR R MR MR MR MR MRMO MR MR MRMR mour MR MR MRMMR MR\n\n\n\nProceeding to inference with peft adapters from LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;unk&gt;'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n    0: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    1: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\neval_model is on: cuda:0\ninput_ids are on: cuda:0\n&lt;s&gt; [INST] Give me some python code to add the first five Fibonacci numbers. [/INST]\"]], [\"INFORMATION\"]].\n[[INFORMATION]]\nNAME: \"Alexandre\"\nPIT: -1.0\nREMARKS: \"French\", \"Dutch\", [\"German\", \"Vietnamese\", \"Scissors, Cut\", \"Teddy bear\", \"French\"]\n\n\n{\n  \"NAME\": \"Maria\",\n  \"REMARKS\": \".Japanese\",\n  \"WITHDRAW\":\n     {\"MOTHER\": \"\",\n      \"POTATOES\": \"12\"},\n  \"CLOSESURE\": {\n     \"$TYPE\": {\n       \"MOTHER\": \"'French'\"\n     },\n     \"CHILD\": [\n       {\n         \"MOTHER\": \"0\",\n         \"POTATOES\": \"122\"\n       }]\n    }\n}\n\n\n\n\n\n\n\n&lt;/s&gt;",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-dataset",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-dataset",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Load the Dataset",
    "text": "Load the Dataset\nNext, we load the dataset. The dataset we are using is designed for DPO training. Therefore it includes chosen and rejected responses. The dataset, is a combination of different high-quality datasets, already contains formatted chosen and rejected answers. This dataset, derived from high-quality responses.\nThe dataset script applies the chat template to three elements: the chosen text, rejected text, and the prompt. For the prompt message, we take all but the last message, while for the chosen and rejected responses, we take only the last message. This setup creates a dataset with formatted prompts, chosen responses, and rejected responses. Additionally, an extra field includes the full list of messages for supervised fine-tuning.\n\nprint(tokenizer.chat_template)\n\n{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n\n\n\n\n#¬†Prepared with the help of code from: https://github.com/xfactlab/orpo/blob/main...\nimport json\n\n# Load the dataset\ndataset_name = 'llmat/dpo-orpo-mix-50k' #¬†Ensure this is defined\n\nmax_num_samples = None #¬†Set to None to use the full dataset\n# max_num_samples = 1000 #¬†set to None to use the full dataset\n\nfrom datasets import load_dataset\n\ndef build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_split_max=1000):\n    # Determin the split specification based on max_num samples\n    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n\n    # Load the dataset\n    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n\n    # Shuffle the dataset\n    if max_num_samples is not None:\n        full_data = full_data.shuffle(seed=42)\n    else:\n        full_data = full_data\n\n    # Determine the number of test samples\n    num_total_samples = len(full_data)\n    test_size = min(test_split_max, min(1000, int(0.1 * num_total_samples)))\n\n    # Randomly split the data into training and test sets\n    dataset = full_data.train_test_split(test_size=test_size)\n\n    # ds_train = dataset['train']\n    # ds_test = dataser['test']\n\n    column_names = list(dataset['train'].features)\n\n    def apply_dpo_template(example):\n        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n        if all(k in example.keys() for k in ('chosen', 'rejected')):\n            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n            # We therefore need to extract the N-1 turns to form the prompt\n            prompt_messages = example['chosen'][:-1]\n            example['messages'] = example['chosen']\n\n            # Now we extract the final turn to define chosen/rejected responses\n            chosen_messages = example['chosen'][-1:]\n            rejected_messages = example['rejected'][-1:]\n            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n        return example\n\n    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n                desc='Formatting comparisons with prompt template',)\n\n    for split in ['train', 'test']:\n        dataset[split] = dataset[split].rename_columns(\n            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected', 'messages': 'messages'}\n        )\n\n    return dataset['train'], dataset['test']\n\n# Assuming 'tokenizer' and 'dataset_name' are already defined\ntrain, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n\n# Check the chat template!!! &lt;s&gt; should not be included when tokenizing the respones\n\n\n\n\n\n\n\nPrinting the dataset reveals the formatted prompt, chosen answer, and rejected answer, all combined in a list of dictionaries for use in supervised fine-tuning. We set training parameters, keeping them consistent for both SFT and ORPO for a fair comparison. Training runs for one epoch with a batch size of 16 and gradient accumulation of two, achieving an effective batch size of 32.\n\nprint('Prompt:', train['prompt'][0])\nprint('\\n\\nChosen:', train['chosen'][0])\nprint('\\n\\nRejected:', train['rejected'][0])\nprint('\\n\\nMessages (incl. prompt):', train['messages'][0])\n\nPrompt: &lt;s&gt;[INST] Q: Did Al-Farabi ever meet Mohammed?\nA: no\nExplanation: Al-Farabi was born in 872 AD. Mohammed died in 832 AD.\n\nQ: Can music be used as a weapon?\nA: yes\nExplanation: Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion. The military calls the use of loud disorienting music part of psychological operations.\n\nQ: Can you write a whole Haiku in a single tweet?\nA: [/INST]\n\n\nChosen: A spring breeze whispers  \nThrough cherry blossoms  \nNature's symphony  \n(5 syllables in the first line, 7 syllables in the second line, 5 syllables in the third line)&lt;/s&gt;\n\n\nRejected: No, 140 characters \n\nUnfurl the petal's secret\nA Haiku, unfolding a rustling dance\nHere's a delicate answer.&lt;/s&gt;\n\n\nMessages (incl. prompt): [{'content': 'Q: Did Al-Farabi ever meet Mohammed?\\nA: no\\nExplanation: Al-Farabi was born in 872 AD. Mohammed died in 832 AD.\\n\\nQ: Can music be used as a weapon?\\nA: yes\\nExplanation: Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion. The military calls the use of loud disorienting music part of psychological operations.\\n\\nQ: Can you write a whole Haiku in a single tweet?\\nA:', 'role': 'user'}, {'content': \"A spring breeze whispers  \\nThrough cherry blossoms  \\nNature's symphony  \\n(5 syllables in the first line, 7 syllables in the second line, 5 syllables in the third line)\", 'role': 'assistant'}]",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#train",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#train",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Train!",
    "text": "Train!",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-tokenizer-and-padding",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-tokenizer-and-padding",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Set up Tokenizer and Padding",
    "text": "Set up Tokenizer and Padding\nNext, we load the tokenizer, checking the start and end tokens, and then set up a custom chat template.\n\nprint(tokenizer)\nprint(tokenizer.vocab_size)\n\nLlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n    0: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    1: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n32000\n\n\n\nprint(tokenizer.bos_token)\nprint(tokenizer.eos_token)\n\n&lt;s&gt;\n&lt;/s&gt;\n\n\n\nprint(tokenizer.chat_template)\n\nNone\n\n\nThis template includes the Beginning of Sequence (BOS) token only if there is a user message. If a message sequence starts with an assistant message, the BOS token is excluded. This is done to ensure that the conversation starts correctly depending on the initial message role.\n\ntokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\"\"\"\n\nmessages = [\n    {'role': 'user', 'content': 'write a quick sort algorithm in python.'},\n    {'role': 'assistant', 'content': 'here you are.'},\n    {'role': 'user', 'content': 'great.'},\n]\n\ninputs = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(inputs)\n\n&lt;s&gt;[INST] write a quick sorf algorithm in python. [/INST]here you are.&lt;/s&gt;[INST] great. [/INST]\n\n\nWe are setting the pad token to the ‚Äòunk‚Äô token.\n\n# set the pad token to &lt;pad&gt;, if not &lt;|pad|&gt;, if not &lt;unk&gt; if &lt;unk&gt;\nif '&lt;pad&gt;' in tokenizer.get_vocab():\n    print('&lt;pad&gt; token is is in the tokenizer. Usinh &lt;pad&gt; for pad')\n    #Set the pad token\n    tokenizer.pad_token = '&lt;pad&gt;'\nelif '&lt;|pad|&gt;' in tokenizer.get_vocab():\n    print('&lt;|pad|&gt; token is in the tokenizer. Using for &lt;|pad|&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;|pad|&gt;'\nelif '&lt;unk&gt;' in tokenizer.get_vocab():\n    print('&lt;unk&gt; token is in the tokenizer. Using for &lt;unk&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;unk&gt;'\nelse:\n    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n    tokenizer.pad_token = tokenizer.eos_token\n\n&lt;unk&gt; token is in the tokenizer. Using for &lt;unk&gt; for pad\n\n\n\n# Update pad token id in model and its config\nmodel.pad_token_id = tokenizer.pad_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n#¬†Check if they are equal\nassert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n\n# Print the pad token ids\nprint('Tokenizer pad token ID:', tokenizer.pad_token_id)\nprint('Model pad token ID:', model.pad_token_id)\nprint('Model config pad token ID:', model.config.pad_token_id)\nprint('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n\nTokenizer pad token ID: 0\nModel pad token ID: 0\nModel config pad token ID: 0\nNumber of tokens now in tokenizer: 32000\n\n\n\nprint('Special tokens map:', tokenizer.special_tokens_map)\nprint('All special tokens:', tokenizer.all_special_tokens)\n\nSpecial tokens map: {'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;unk&gt;'}\nAll special tokens: ['&lt;s&gt;', '&lt;/s&gt;', '&lt;unk&gt;']\n\n\n\nprint(tokenizer)\n\nLlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;unk&gt;'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n    0: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    1: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html",
    "href": "posts/ORPO_Mistral-unsloth-publish.html",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "",
    "text": "ORPO is an innovative fine-tuning method that merges traditional supervised fine-tuning with preference alignment into a unified process. This approach decreases the computational resources and time needed for training. Additionally, empirical evidence shows that ORPO surpasses other alignment techniques across different model sizes and benchmarks.\nIn this article, we will fine-tune the newest Mistral 7B model using ORPO and the TRL library. The code is accessible on Google Colab and in the LLM Tutorial on GitHub."
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#orpo",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#orpo",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "ORPO",
    "text": "ORPO\nInstruction tuning and preference alignment are crucial methods for customizing Large Language Models (LLMs) for particular tasks. Typically, this entails a multi-step process: first, Supervised Fine-Tuning (SFT) on instructions to tailor the model to the desired domain, and second, applying preference alignment techniques such as Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to enhance the probability of producing preferred responses over less desirable ones.\nResearchers have discovered a drawback in this method. Although SFT successfully adjusts the model to the target domain, it also unintentionally raises the chances of producing both unwanted and desired answers. Therefore, the preference alignment stage is essential to enlarge the disparity between the probabilities of accepted and rejected outputs.\n\n\n\nHong and Lee (2024) presented ORPO, an innovative approach that combines instruction tuning and preference alignment into a single training framework. ORPO modifies the conventional language modeling objective by incorporating the negative log-likelihood loss with an odds ratio (OR) component. This OR loss applies a mild penalty to rejected responses while greatly rewarding preferred ones, allowing the model to simultaneously learn the target task and align with human preferences.\n\\mathscr{L}{ORPO} = \\mathbb{E}{(x, y_{w}, y_l)}[\\mathscr{L}{SFT} + \\lambda \\cdot \\mathscr{L}{OR}]\nORPO has been integrated into key fine-tuning libraries such as TRL, Axolotl, and LLaMA-Factory. The following section will demonstrate its usage with TRL."
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#fine-tuning-mistral-v0.3-with-orpo-and-unsoth",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#fine-tuning-mistral-v0.3-with-orpo-and-unsoth",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Fine-Tuning Mistral v0.3 with ORPO and Unsoth",
    "text": "Fine-Tuning Mistral v0.3 with ORPO and Unsoth\nMistral AI‚Äôs v0.3 is a significant update to their AI model, introducing improved performance and efficiency. This version includes enhanced instruction-following capabilities, making interactions more intuitive. Additionally, Mistral v0.3 incorporates advanced reasoning skills, enabling it to tackle complex tasks more effectively. The update also extends the context length to 32768 tokens, allowing for more detailed and coherent conversations. Technical details include an extended vocabulary (32000 to 32768), a new tokenizer, and support for function calling.\nORPO necessitates a preference dataset that includes a prompt, a selected answer, and a discarded answer. To achieve this, we will utilize llmat/dpo-orpo-mix-38k-balanced, a dataset that merges high-quality DPO datasets and has been further balanced using a clustering-based approach.\nTo efficiently fine-tune our model we will use the unlsoth library. Unsloth significantly improves speed and efficiency in the training of Large Language Models (LLMs). The speed and efficiency gains are achieved through several optimizations, including manual autograd and chained matrix multiplication. Furthermore, it utilizes Flash Attention via xformers and Tri Dao‚Äôs implementation, which is a highly optimized approach to handling attention mechanisms in transformer models. Unsloth makes fine-tuning 2 times faster with 50% less memory usage.\nLet‚Äôs start by installing the required libraries:\n\n!pip install python-dotenv\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers&lt;0.0.27\" \"trl&lt;0.9.0\" peft accelerate bitsandbytes\n\nNow let‚Äôs login to our W&B workspace\n\nimport wandb\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\n%env WANDB_NOTEBOOK_NAME = $Fine_tune_Mistral_with_ORPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\n\nLoad the Model and Tokenizer for LoRA\nIn the following, we will load the Mistral 7B v0.3 model in 4-bit precision using bitsandbytes.\n\ncache_dir = './model'\nmodel_id = 'mistralai/Mistral-7B-v0.3'\n\n\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_id,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n\n\nLoading Checks\nAfter loading the model, it‚Äôs crucial to ensure that all parameters are correctly placed on the GPU and that none are overflowing onto the CPU. This can be particularly important for large models where memory management is critical.\nTo verify this, you can iterate through the model‚Äôs named parameters and check their device type. If any parameter is on the CPU (indicated by the device type ‚Äòmeta‚Äô), it will be printed out.\nHere is the code to perform this check:\n\n# Check there are no parameters overflowing onto cpu (meta).\nfor n, p in model.named_parameters():\n    if p.device.type=='meta':\n        print(f\"{n} is on meta!\")"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#prepare-for-lora-fine-tuning",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#prepare-for-lora-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Prepare for LoRA fine-tuning",
    "text": "Prepare for LoRA fine-tuning\nBefore starting the LoRA (Low-Rank Adaptation) fine-tuning process, it‚Äôs essential to understand which parameters in your model are trainable and which are not. This helps in ensuring that only the desired parameters are updated during training, which is crucial for efficient and effective fine-tuning.\nTo achieve this, you can use the following function to print the number of trainable parameters in the model and list which parameters are trainable and which are not.\nHere is the code to perform this check:\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainabl√∂e parameters in the model and lists which parameters\n    \"\"\"\n    trainable_params = 0\n    non_trainable_params = 0\n    all_params = 0\n\n    print(\"Trainable Parameters\")\n    for name, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n            print(f\" {name}\")\n        else:\n            non_trainable_params += param.numel()\n\n    print(\"\\nNon-Trainable Parameters:\")\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            print(f\" {name}\")\n\n    print(\n        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n        \n\nLet‚Äôs take a look a the model\n\nprint(model)"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-lora-fine-tuning",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-lora-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up LoRA Fine-Tuning",
    "text": "Setting Up LoRA Fine-Tuning\nTo prepare your model for LoRA (Low-Rank Adaptation) fine-tuning, you need to configure it properly. This involves setting up the LoRA configuration. Here‚Äôs a brief overview of the parameters and their best settings:\n\nr: This parameter controls the rank of the low-rank adaptation matrices. It‚Äôs suggested to choose a value greater than 0, with common choices being 8, 16, 32, 64, or 128. The best setting depends on the specific use case and computational resources, but a good starting point is 8 or 16.\nlora_alpha: This parameter scales the magnitude of the LoRA update. A higher value can lead to more significant changes in the model‚Äôs behavior. The best setting is typically 32, as used in the code.\ntarget_modules: This list specifies which modules in the model should be fine-tuned. The best settings include key modules like \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", and \"down_proj\". If the task involves chat fine-tuning, it‚Äôs also beneficial to set \"lm_head\" (language model head) as trainable.\nuse_gradient_checkpointing: This parameter activates gradient checkpointing to conserve memory. It is managed by Unsloth, which offloads input and output embeddings to disk, thereby saving VRAM.\nrandom_state: This parameter sets the seed for random number generation, ensuring reproducibility. The best setting is any integer value; in the code, it‚Äôs set to 3407.\nuse_rslora: This parameter activates RSLoRA, which adjusts the scaling factor of LoRA adapters to be proportional to 1/‚àör instead of 1/r. This adjustment enhances the stability of learning, particularly for higher adapter ranks, and improves fine-tuning performance as the rank increases.\n\nThese settings provide a good starting point for fine-tuning a language model using PEFT. However, the optimal settings may vary depending on the specific task and dataset, so some experimentation may be necessary.\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128\n    lora_alpha = 32,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n        \n    ],\n    \n    lora_dropout = 0, \n    bias = \"none\",    \n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = True,\n    \n)\n\n\nSet up Tokenizer and Padding\nBefore starting the fine-tuning process, it‚Äôs essential to configure the tokenizer and set up padding correctly. This ensures that the model can handle input sequences efficiently and that special tokens are properly managed.\nHere is a step-by-step guide to setting up the tokenizer and padding:\n\nInspect the Tokenizer: Print out the tokenizer details, including the vocabulary size, beginning-of-sequence (BOS) token, end-of-sequence (EOS) token, and chat template.\nOptionally Set the Chat Template Manually: If needed, you can manually set the chat template. This is useful for ensuring that the conversation starts correctly depending on the initial message role.\nApply the Chat Template: Use the chat template to format a list of messages.\nSet the Pad Token: Determine the appropriate pad token based on the tokenizer‚Äôs vocabulary and set it accordingly.\nUpdate the Model Configuration: Ensure that the model and its configuration are updated with the correct pad token ID.\n\nHere is the code to perform these steps:\n\nprint(tokenizer)\nprint(tokenizer.vocab_size)\n\n\nprint(tokenizer.bos_token)\nprint(tokenizer.eos_token)\n\n\nprint(tokenizer.chat_template)\n\nA custom chat template for a tokenizer, specifically designed for Llama/Mistral models is created. This template ensures that conversations start correctly by conditionally adding a beginning-of-sequence token (bos_token) if the first message is not from the assistant. This is particularly useful when formatting chosen and rejected responses separately, as it avoids adding an extra bos_token before the response.\nThe template is defined using a Jinja-like syntax, which iterates through the messages and formats them based on their roles (user or assistant). For user messages, it wraps the content with [INST] and [/INST] tags, while for assistant messages, it appends an end-of-sequence token (eos_token).\n\ntokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n\"\"\"\n\n# Test chat template\nmessages = [\n    {'role': 'user', 'content': 'write a quick sorf algorithm in python.'},\n    {'role': 'assistant', 'content': 'here you are.'},\n    {'role': 'user', 'content': 'great.'},\n]\n\ninputs = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(inputs)\n\n\n##¬†set the pad token to &lt;pad&gt;, if not &lt;|pad|&gt;, if not &lt;unk&gt; if &lt;unk&gt;\nif '&lt;pad&gt;' in tokenizer.get_vocab():\n    print('&lt;pad&gt; token is is in the tokenizer. Usinh &lt;pad&gt; for pad')\n    #Set the pad token\n    tokenizer.pad_token = '&lt;pad&gt;'\nelif '&lt;|pad|&gt;' in tokenizer.get_vocab():\n    print('&lt;|pad|&gt; token is in the tokenizer. Using for &lt;|pad|&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;|pad|&gt;'\nelif '&lt;unk&gt;' in tokenizer.get_vocab():\n    print('&lt;unk&gt; token is in the tokenizer. Using for &lt;unk&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;unk&gt;'\nelse:\n    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n    tokenizer.pad_token = tokenizer.eos_token\n\n\n# Update pad token id in model and its config\nmodel.pad_token_id = tokenizer.pad_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n#¬†Check if they are equal\nassert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n\n# Print the pad token ids\nprint('Tokenizer pad token ID:', tokenizer.pad_token_id)\nprint('Model pad token ID:', model.pad_token_id)\nprint('Model config pad token ID:', model.config.pad_token_id)\nprint('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n\n\nprint('Special tokens map:', tokenizer.special_tokens_map)\nprint('All special tokens:', tokenizer.all_special_tokens)\n\n\nprint(tokenizer)"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-chat-template-has-been-changed",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-chat-template-has-been-changed",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)",
    "text": "Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)\nWhen fine-tuning a model for chat applications, it‚Äôs often beneficial to set specific layers to be trainable, especially if you are changing the chat template. This ensures that the model can adapt to the new input format more effectively.\nHere is a step-by-step guide to setting specific layers to trainable:\n\nIdentify Trainable Parameters: Create a list of the names of the layers you want to set as trainable.\nSet Modules to Trainable: Iterate through the model‚Äôs parameters and set the requires_grad attribute to True for the specified layers. Optionally, set the rest to False.\nCreate a Dictionary of Trainable Parameters: Collect the trainable parameters into a dictionary for easy access.\nConvert to State Dict Format: Convert the trainable parameters to a state dictionary format, which can be useful for saving and loading the model‚Äôs state.\nPrint Trainable Parameters: Use a function to print the trainable parameters to verify the setup.\n\nHere is the code to perform these steps:\n\n# List to hold the names of the trainable parameters\ntrainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n\n# Set modules to be trainable\nfor n, p in model.named_parameters():\n    if any(k in n for k in trainable_params_names):\n        p.requires_grad_(True)\n    else:\n        p.requires_grad_(False) # Optional: Set the rest to be trainable\n\n# Make a dictionary of trainable parameters\ntrainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n\n# Convert trainable_params to state_dict format\ntrainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}\n\n\nprint_trainable_parameters(model)"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#loading-and-preparing-the-dataset-for-fine-tuning",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#loading-and-preparing-the-dataset-for-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Loading and Preparing the Dataset for Fine-Tuning",
    "text": "Loading and Preparing the Dataset for Fine-Tuning\nIn this code, we will guide you through the process of loading and preparing a dataset for fine-tuning a model. This involves loading the dataset, shuffling it, splitting it into training and test sets, and applying a specific template to format the data correctly.\nHere is a step-by-step guide to loading and preparing the dataset:\n\nImport Necessary Libraries: Import the required libraries, including json for handling JSON data and datasets for loading and manipulating the dataset.\nDefine Dataset Parameters: Set the dataset name and the maximum number of samples to use. If you want to use the full dataset, set max_num_samples to None.\nDefine the build_dataset Function: Create a function called build_dataset that takes a tokenizer, dataset name, cache directory, maximum number of samples, and other parameters as inputs. This function will load the dataset, shuffle it, split it into training and test sets, and apply a specific template to format the data.\nLoad the Dataset: Use the load_dataset function from the datasets library to load the dataset. The dataset is split based on the max_num_samples parameter.\nShuffle the Dataset: If max_num_samples is not None, shuffle the dataset to ensure randomness.\nSplit the Dataset: Determine the number of test samples and split the dataset into training and test sets using the train_test_split method.\nApply the DPO Template: Define a function called apply_dpo_template that formats the data according to the DPO (Direct Preference Optimization) template. This function extracts the necessary information from the dataset and applies the chat template using the tokenizer.\nMap the Dataset: Use the map method to apply the apply_dpo_template function to the dataset. Remove the original columns and rename the new columns accordingly.\nReturn the Dataset: Return the training and test datasets.\nCheck the Chat Template: Ensure that the chat template is correctly applied and that special tokens are not included when tokenizing the responses.\n\nHere is the code to perform these steps:\n\n#¬†Prepared with the help of code from: https://github.com/xfactlab/orpo/blob/main...\nimport json\n\n# Load the dataset\ndataset_name = 'llmat/dpo-orpo-mix-38k-balanced' #¬†Ensure this is defined\n\nmax_num_samples = None #¬†Set to None to use the full dataset\n#max_num_samples = 10000 #¬†set to None to use the full dataset\n\nfrom datasets import load_dataset\n\ndef build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_size_ratio=0.1):\n    # Determin the split specification based on max_num samples\n    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n\n    # Load the dataset\n    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n\n    # Shuffle the dataset\n    if max_num_samples is not None:\n        full_data = full_data.shuffle(seed=42)\n    else:\n        full_data = full_data\n\n    # Determine the number of test samples\n    num_total_samples = len(full_data)\n    test_size = int(test_size_ratio * num_total_samples)\n\n    # Randomly split the data into training and test sets\n    dataset = full_data.train_test_split(test_size=test_size)\n\n    column_names = list(dataset['train'].features)\n\n    def apply_dpo_template(example):\n        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n        if all(k in example.keys() for k in ('chosen', 'rejected')):\n            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n            # We therefore need to extract the N-1 turns to form the prompt\n            prompt_messages = example['chosen'][:-1]\n            example['messages'] = example['chosen']\n\n            # Now we extract the final turn to define chosen/rejected responses\n            chosen_messages = example['chosen'][-1:]\n            rejected_messages = example['rejected'][-1:]\n            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n        return example\n\n    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n                desc='Formatting comparisons with prompt template',)\n\n    for split in ['train', 'test']:\n        dataset[split] = dataset[split].rename_columns(\n            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected', 'messages': 'messages'}\n        )\n\n    return dataset['train'], dataset['test']\n\n# Assuming 'tokenizer' and 'dataset_name' are already defined\ntrain, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n\n# Check the chat template!!! &lt;s&gt; should not be included when tokenizing the respones\n\nAfter preparing and formatting your dataset for fine-tuning, it‚Äôs crucial to inspect the data to ensure that it has been correctly processed. This step helps you verify that the prompt, chosen, rejected, and messages fields are properly formatted and contain the expected information.\n\nprint('Prompt:', train['prompt'][0])\nprint('\\n\\nChosen:', train['chosen'][0])\nprint('\\n\\nRejected:', train['rejected'][0])\nprint('\\n\\nMessages (incl. prompt):', train['messages'][0])"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-and-running-training",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-and-running-training",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up and Running Training",
    "text": "Setting Up and Running Training\nIn this tutorial, we will go through the process of setting up and running the training for your model. This includes configuring training parameters, creating a custom logging callback, and initiating the training process.\nHere is a step-by-step guide to setting up and running the training:\n\nSet Training Parameters: Define the training parameters such as the model name, number of epochs, gradient accumulation steps, batch size, and the directory to save the results.\nCreate a Custom Logging Callback: Implement a custom callback to log training metrics to a file. This callback will write the training and evaluation loss to a log file and save the trainable parameters at checkpoint steps.\nInitialize the Logging Callback: Create an instance of the custom logging callback with the specified log file path.\n\nHere is the code to perform these steps:\n\nmodel_name = model_id.split('/')[-1]\n\nepochs=1\ngrad_accum=4\nbatch_size=8\nfine_tune_tag='ORPO'\nsave_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\nprint(save_dir)\n\n\nimport transformers\nimport os\nimport torch\n\n# Custom callback to log metrics\nclass LoggingCallback(transformers.TrainerCallback):\n    def __init__(self, log_file_path):\n        self.log_file_path = log_file_path\n\n    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n        with open(self.log_file_path, 'a') as f:\n            if 'loss' in logs:\n                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n                if 'eval_loss' in logs:\n                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n                f.flush()  # Force flush the buffered data to file\n\n        # Check if the current step is a checkpoint step\n        if state.global_step % int(args.save_steps) == 0:\n            # Check if the last checkpoint path exists\n            if state.best_model_checkpoint:\n                checkpoint_dir = state.best_model_checkpoint\n            else:\n                # If not, construct the checkpoint directory path\n                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n\n            # Ensure the checkpoint directory exists\n            os.makedirs(checkpoint_dir, exist_ok=True)\n\n            # Save trainable params in the checkpoint directory\n            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n            torch.save(current_trainable_params_state_dict, file_path)\n\n# Log file path\ncache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\nlog_file_path = os.path.join(cache_dir, 'training_logs.txt')\n\n# Create an instance of the custom callback\nlogging_callback = LoggingCallback(log_file_path)"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-orpo-training",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-orpo-training",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up ORPO Training",
    "text": "Setting Up ORPO Training\nIn this section, we‚Äôll walk through setting up and training a model using the ORPOTrainer from the trl library.\nI trained the model on the entire dataset (38k samples) using an RTX 4090 GPU (24 GB of VRAM). The training took 7 hours and 35 minutes. You can use smaller GPUs with less VRAM and a smaller batch size. In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like ‚Äòmax_num_samples = 10000‚Äô to only load 10k samples.\n\nConfigure ORPO\nDefine the configuration for the ORPO training. This configuration includes various hyperparameters and settings for training.\n\nfrom trl import ORPOTrainer, ORPOConfig\nfrom unsloth import is_bfloat16_supported\n\norpo_config = ORPOConfig(\n    beta=0.2,\n    save_steps=500, \n    logging_steps=1,\n    num_train_epochs=epochs,\n    output_dir=save_dir,\n    evaluation_strategy='steps', \n    do_eval=True,\n    eval_steps=0.2,\n    per_device_eval_batch_size=batch_size,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=grad_accum,\n    log_level='debug',\n    optim='paged_adamw_8bit',\n    fp16 = not is_bfloat16_supported(),\n    bf16 = is_bfloat16_supported(),\n    max_grad_norm=0.3,\n    lr_scheduler_type='linear',\n    warmup_ratio=0.03,\n    learning_rate=1e-4, \n\n    max_prompt_length=512,\n    max_length=1024,\n\n    max_completion_length=1024,\n    remove_unused_columns=True,\n    \n)\n\n\n\nInitialize ORPOTrainer\nCreate an instance of ORPOTrainer with the model, datasets, tokenizer, and the configuration defined earlier.\n\norpo_trainer = ORPOTrainer(\n    model,\n    args=orpo_config,\n    train_dataset=train,\n    eval_dataset=test,\n    tokenizer=tokenizer,\n\n    callbacks=[logging_callback], # Add custom callback here\n)\n\n\n\nTrain the Model\nSet the model configuration to avoid cache warnings and start the training process.\n\nmodel.config.use_cache = False # silence the warnings\norpo_trainer.train()"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#plotting-training-and-evaluation-losses-with-matplotlib",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#plotting-training-and-evaluation-losses-with-matplotlib",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Plotting Training and Evaluation Losses with Matplotlib",
    "text": "Plotting Training and Evaluation Losses with Matplotlib\nAfter training your model, it‚Äôs important to visualize the training and evaluation losses to understand how well your model is performing and to identify any potential issues. Visualizing the losses can help you diagnose problems such as overfitting or underfitting and make informed decisions about further training or model adjustments.\n\nimport matplotlib.pyplot as plt\n\n# Initialize lists to hold training and evaluation losses and steps\ntrain_losses = []\neval_losses = []\ntrain_steps = []\neval_steps = []\n\n# Populate the lists from the log history\nfor entry in orpo_trainer.state.log_history:\n    if 'loss' in entry:\n        train_losses.append(entry['loss'])\n        train_steps.append(entry['step'])\n    if 'eval_loss' in entry:\n        eval_losses.append(entry['eval_loss'])\n        eval_steps.append(entry['step'])\n\n# Plot the losses\nplt.plot(train_steps, train_losses, label='Train Loss')\nplt.plot(eval_steps, eval_losses, label='Eval Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nLet‚Äôs now check the W&B plots. While the loss goes down, we also can see that the difference between the chosen and rejects answers becomes clearer."
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#merging-adapters-and-saving-the-model-to-hugging-face-hub",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#merging-adapters-and-saving-the-model-to-hugging-face-hub",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Merging Adapters and Saving the Model to Hugging Face Hub",
    "text": "Merging Adapters and Saving the Model to Hugging Face Hub\nIn the subsequent steps, we merge the adapters with the original model using 16-bit precision to enhance quality. Initially, we save it locally in the ‚Äúmodel‚Äù directory before uploading it to the Hugging Face Hub. The trained model is available at llmat/Mistral-v0.3-7B-ORPO.\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"llmat/Mistral-v0.3-7B-ORPO\", tokenizer, save_method=\"merged_16bit\")"
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "",
    "text": "ORPO is an innovative fine-tuning method that merges traditional supervised fine-tuning with preference alignment into a unified process. This approach decreases the computational resources and time needed for training. Additionally, empirical evidence shows that ORPO surpasses other alignment techniques across different model sizes and benchmarks.\nIn this article, we will fine-tune the newest Mistral 7B model using ORPO and the TRL library. The code is accessible on Google Colab and in the LLM Tutorial on GitHub."
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#orpo",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#orpo",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "ORPO",
    "text": "ORPO\nInstruction tuning and preference alignment are crucial methods for customizing Large Language Models (LLMs) for particular tasks. Typically, this entails a multi-step process: first, Supervised Fine-Tuning (SFT) on instructions to tailor the model to the desired domain, and second, applying preference alignment techniques such as Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to enhance the probability of producing preferred responses over less desirable ones.\nResearchers have discovered a drawback in this method. Although SFT successfully adjusts the model to the target domain, it also unintentionally raises the chances of producing both unwanted and desired answers. Therefore, the preference alignment stage is essential to enlarge the disparity between the probabilities of accepted and rejected outputs.\n\n\n\nHong and Lee (2024) presented ORPO, an innovative approach that combines instruction tuning and preference alignment into a single training framework. ORPO modifies the conventional language modeling objective by incorporating the negative log-likelihood loss with an odds ratio (OR) component. This OR loss applies a mild penalty to rejected responses while greatly rewarding preferred ones, allowing the model to simultaneously learn the target task and align with human preferences.\n\\mathscr{L}{ORPO} = \\mathbb{E}{(x, y_{w}, y_l)}[\\mathscr{L}{SFT} + \\lambda \\cdot \\mathscr{L}{OR}]\nORPO has been integrated into key fine-tuning libraries such as TRL, Axolotl, and LLaMA-Factory. The following section will demonstrate its usage with TRL."
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#fine-tuning-mistral-v0.3-with-orpo-and-unsoth",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#fine-tuning-mistral-v0.3-with-orpo-and-unsoth",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Fine-Tuning Mistral v0.3 with ORPO and Unsoth",
    "text": "Fine-Tuning Mistral v0.3 with ORPO and Unsoth\nMistral AI‚Äôs v0.3 is a significant update to their AI model, introducing improved performance and efficiency. This version includes enhanced instruction-following capabilities, making interactions more intuitive. Additionally, Mistral v0.3 incorporates advanced reasoning skills, enabling it to tackle complex tasks more effectively. The update also extends the context length to 32768 tokens, allowing for more detailed and coherent conversations. Technical details include an extended vocabulary (32000 to 32768), a new tokenizer, and support for function calling.\nORPO necessitates a preference dataset that includes a prompt, a selected answer, and a discarded answer. To achieve this, we will utilize llmat/dpo-orpo-mix-38k-balanced, a dataset that merges high-quality DPO datasets and has been further balanced using a clustering-based approach.\nTo efficiently fine-tune our model we will use the unlsoth library. Unsloth significantly improves speed and efficiency in the training of Large Language Models (LLMs). The speed and efficiency gains are achieved through several optimizations, including manual autograd and chained matrix multiplication. Furthermore, it utilizes Flash Attention via xformers and Tri Dao‚Äôs implementation, which is a highly optimized approach to handling attention mechanisms in transformer models. Unsloth makes fine-tuning 2 times faster with 50% less memory usage.\nLet‚Äôs start by installing the required libraries:\n\n!pip install python-dotenv\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers&lt;0.0.27\" \"trl&lt;0.9.0\" peft accelerate bitsandbytes\n\nNow let‚Äôs login to our W&B workspace\n\nimport wandb\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\n%env WANDB_NOTEBOOK_NAME = $Fine_tune_Mistral_with_ORPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\n\nLoad the Model and Tokenizer for LoRA\nIn the following, we will load the Mistral 7B v0.3 model in 4-bit precision using bitsandbytes.\n\ncache_dir = './model'\nmodel_id = 'mistralai/Mistral-7B-v0.3'\n\n\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_id,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n\n\nLoading Checks\nAfter loading the model, it‚Äôs crucial to ensure that all parameters are correctly placed on the GPU and that none are overflowing onto the CPU. This can be particularly important for large models where memory management is critical.\nTo verify this, you can iterate through the model‚Äôs named parameters and check their device type. If any parameter is on the CPU (indicated by the device type ‚Äòmeta‚Äô), it will be printed out.\nHere is the code to perform this check:\n\n# Check there are no parameters overflowing onto cpu (meta).\nfor n, p in model.named_parameters():\n    if p.device.type=='meta':\n        print(f\"{n} is on meta!\")"
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#prepare-for-lora-fine-tuning",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#prepare-for-lora-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Prepare for LoRA fine-tuning",
    "text": "Prepare for LoRA fine-tuning\nBefore starting the LoRA (Low-Rank Adaptation) fine-tuning process, it‚Äôs essential to understand which parameters in your model are trainable and which are not. This helps in ensuring that only the desired parameters are updated during training, which is crucial for efficient and effective fine-tuning.\nTo achieve this, you can use the following function to print the number of trainable parameters in the model and list which parameters are trainable and which are not.\nHere is the code to perform this check:\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainabl√∂e parameters in the model and lists which parameters\n    \"\"\"\n    trainable_params = 0\n    non_trainable_params = 0\n    all_params = 0\n\n    print(\"Trainable Parameters\")\n    for name, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n            print(f\" {name}\")\n        else:\n            non_trainable_params += param.numel()\n\n    print(\"\\nNon-Trainable Parameters:\")\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            print(f\" {name}\")\n\n    print(\n        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n        \n\nLet‚Äôs take a look a the model\n\nprint(model)"
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-lora-fine-tuning",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-lora-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up LoRA Fine-Tuning",
    "text": "Setting Up LoRA Fine-Tuning\nTo prepare your model for LoRA (Low-Rank Adaptation) fine-tuning, you need to configure it properly. This involves setting up the LoRA configuration. Here‚Äôs a brief overview of the parameters and their best settings:\n\nr: This parameter controls the rank of the low-rank adaptation matrices. It‚Äôs suggested to choose a value greater than 0, with common choices being 8, 16, 32, 64, or 128. The best setting depends on the specific use case and computational resources, but a good starting point is 8 or 16.\nlora_alpha: This parameter scales the magnitude of the LoRA update. A higher value can lead to more significant changes in the model‚Äôs behavior. The best setting is typically 32, as used in the code.\ntarget_modules: This list specifies which modules in the model should be fine-tuned. The best settings include key modules like \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", and \"down_proj\". If the task involves chat fine-tuning, it‚Äôs also beneficial to set \"lm_head\" (language model head) as trainable.\nuse_gradient_checkpointing: This parameter activates gradient checkpointing to conserve memory. It is managed by Unsloth, which offloads input and output embeddings to disk, thereby saving VRAM.\nrandom_state: This parameter sets the seed for random number generation, ensuring reproducibility. The best setting is any integer value; in the code, it‚Äôs set to 3407.\nuse_rslora: This parameter activates RSLoRA, which adjusts the scaling factor of LoRA adapters to be proportional to 1/‚àör instead of 1/r. This adjustment enhances the stability of learning, particularly for higher adapter ranks, and improves fine-tuning performance as the rank increases.\n\nThese settings provide a good starting point for fine-tuning a language model using PEFT. However, the optimal settings may vary depending on the specific task and dataset, so some experimentation may be necessary.\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128\n    lora_alpha = 32,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n        \n    ],\n    \n    lora_dropout = 0, \n    bias = \"none\",    \n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = True,\n    \n)\n\n\nSet up Tokenizer and Padding\nBefore starting the fine-tuning process, it‚Äôs essential to configure the tokenizer and set up padding correctly. This ensures that the model can handle input sequences efficiently and that special tokens are properly managed.\nHere is a step-by-step guide to setting up the tokenizer and padding:\n\nInspect the Tokenizer: Print out the tokenizer details, including the vocabulary size, beginning-of-sequence (BOS) token, end-of-sequence (EOS) token, and chat template.\nOptionally Set the Chat Template Manually: If needed, you can manually set the chat template. This is useful for ensuring that the conversation starts correctly depending on the initial message role.\nApply the Chat Template: Use the chat template to format a list of messages.\nSet the Pad Token: Determine the appropriate pad token based on the tokenizer‚Äôs vocabulary and set it accordingly.\nUpdate the Model Configuration: Ensure that the model and its configuration are updated with the correct pad token ID.\n\nHere is the code to perform these steps:\n\nprint(tokenizer)\nprint(tokenizer.vocab_size)\n\n\nprint(tokenizer.bos_token)\nprint(tokenizer.eos_token)\n\n\nprint(tokenizer.chat_template)\n\nA custom chat template for a tokenizer, specifically designed for Llama/Mistral models is created. This template ensures that conversations start correctly by conditionally adding a beginning-of-sequence token (bos_token) if the first message is not from the assistant. This is particularly useful when formatting chosen and rejected responses separately, as it avoids adding an extra bos_token before the response.\nThe template is defined using a Jinja-like syntax, which iterates through the messages and formats them based on their roles (user or assistant). For user messages, it wraps the content with [INST] and [/INST] tags, while for assistant messages, it appends an end-of-sequence token (eos_token).\n\ntokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n\"\"\"\n\n# Test chat template\nmessages = [\n    {'role': 'user', 'content': 'write a quick sorf algorithm in python.'},\n    {'role': 'assistant', 'content': 'here you are.'},\n    {'role': 'user', 'content': 'great.'},\n]\n\ninputs = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(inputs)\n\n\n##¬†set the pad token to &lt;pad&gt;, if not &lt;|pad|&gt;, if not &lt;unk&gt; if &lt;unk&gt;\nif '&lt;pad&gt;' in tokenizer.get_vocab():\n    print('&lt;pad&gt; token is is in the tokenizer. Usinh &lt;pad&gt; for pad')\n    #Set the pad token\n    tokenizer.pad_token = '&lt;pad&gt;'\nelif '&lt;|pad|&gt;' in tokenizer.get_vocab():\n    print('&lt;|pad|&gt; token is in the tokenizer. Using for &lt;|pad|&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;|pad|&gt;'\nelif '&lt;unk&gt;' in tokenizer.get_vocab():\n    print('&lt;unk&gt; token is in the tokenizer. Using for &lt;unk&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;unk&gt;'\nelse:\n    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n    tokenizer.pad_token = tokenizer.eos_token\n\n\n# Update pad token id in model and its config\nmodel.pad_token_id = tokenizer.pad_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n#¬†Check if they are equal\nassert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n\n# Print the pad token ids\nprint('Tokenizer pad token ID:', tokenizer.pad_token_id)\nprint('Model pad token ID:', model.pad_token_id)\nprint('Model config pad token ID:', model.config.pad_token_id)\nprint('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n\n\nprint('Special tokens map:', tokenizer.special_tokens_map)\nprint('All special tokens:', tokenizer.all_special_tokens)\n\n\nprint(tokenizer)"
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-chat-template-has-been-changed",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-chat-template-has-been-changed",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)",
    "text": "Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)\nWhen fine-tuning a model for chat applications, it‚Äôs often beneficial to set specific layers to be trainable, especially if you are changing the chat template. This ensures that the model can adapt to the new input format more effectively.\nHere is a step-by-step guide to setting specific layers to trainable:\n\nIdentify Trainable Parameters: Create a list of the names of the layers you want to set as trainable.\nSet Modules to Trainable: Iterate through the model‚Äôs parameters and set the requires_grad attribute to True for the specified layers. Optionally, set the rest to False.\nCreate a Dictionary of Trainable Parameters: Collect the trainable parameters into a dictionary for easy access.\nConvert to State Dict Format: Convert the trainable parameters to a state dictionary format, which can be useful for saving and loading the model‚Äôs state.\nPrint Trainable Parameters: Use a function to print the trainable parameters to verify the setup.\n\nHere is the code to perform these steps:\n\n# List to hold the names of the trainable parameters\ntrainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n\n# Set modules to be trainable\nfor n, p in model.named_parameters():\n    if any(k in n for k in trainable_params_names):\n        p.requires_grad_(True)\n    else:\n        p.requires_grad_(False) # Optional: Set the rest to be trainable\n\n# Make a dictionary of trainable parameters\ntrainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n\n# Convert trainable_params to state_dict format\ntrainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}\n\n\nprint_trainable_parameters(model)"
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#loading-and-preparing-the-dataset-for-fine-tuning",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#loading-and-preparing-the-dataset-for-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Loading and Preparing the Dataset for Fine-Tuning",
    "text": "Loading and Preparing the Dataset for Fine-Tuning\nIn this code, we will guide you through the process of loading and preparing a dataset for fine-tuning a model. This involves loading the dataset, shuffling it, splitting it into training and test sets, and applying a specific template to format the data correctly.\nHere is a step-by-step guide to loading and preparing the dataset:\n\nImport Necessary Libraries: Import the required libraries, including json for handling JSON data and datasets for loading and manipulating the dataset.\nDefine Dataset Parameters: Set the dataset name and the maximum number of samples to use. If you want to use the full dataset, set max_num_samples to None.\nDefine the build_dataset Function: Create a function called build_dataset that takes a tokenizer, dataset name, cache directory, maximum number of samples, and other parameters as inputs. This function will load the dataset, shuffle it, split it into training and test sets, and apply a specific template to format the data.\nLoad the Dataset: Use the load_dataset function from the datasets library to load the dataset. The dataset is split based on the max_num_samples parameter.\nShuffle the Dataset: If max_num_samples is not None, shuffle the dataset to ensure randomness.\nSplit the Dataset: Determine the number of test samples and split the dataset into training and test sets using the train_test_split method.\nApply the DPO Template: Define a function called apply_dpo_template that formats the data according to the DPO (Direct Preference Optimization) template. This function extracts the necessary information from the dataset and applies the chat template using the tokenizer.\nMap the Dataset: Use the map method to apply the apply_dpo_template function to the dataset. Remove the original columns and rename the new columns accordingly.\nReturn the Dataset: Return the training and test datasets.\nCheck the Chat Template: Ensure that the chat template is correctly applied and that special tokens are not included when tokenizing the responses.\n\nHere is the code to perform these steps:\n\n#¬†Prepared with the help of code from: https://github.com/xfactlab/orpo/blob/main...\nimport json\n\n# Load the dataset\ndataset_name = 'llmat/dpo-orpo-mix-38k-balanced' #¬†Ensure this is defined\n\nmax_num_samples = None #¬†Set to None to use the full dataset\n#max_num_samples = 10000 #¬†set to None to use the full dataset\n\nfrom datasets import load_dataset\n\ndef build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_size_ratio=0.1):\n    # Determin the split specification based on max_num samples\n    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n\n    # Load the dataset\n    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n\n    # Shuffle the dataset\n    if max_num_samples is not None:\n        full_data = full_data.shuffle(seed=42)\n    else:\n        full_data = full_data\n\n    # Determine the number of test samples\n    num_total_samples = len(full_data)\n    test_size = int(test_size_ratio * num_total_samples)\n\n    # Randomly split the data into training and test sets\n    dataset = full_data.train_test_split(test_size=test_size)\n\n    column_names = list(dataset['train'].features)\n\n    def apply_dpo_template(example):\n        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n        if all(k in example.keys() for k in ('chosen', 'rejected')):\n            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n            # We therefore need to extract the N-1 turns to form the prompt\n            prompt_messages = example['chosen'][:-1]\n            example['messages'] = example['chosen']\n\n            # Now we extract the final turn to define chosen/rejected responses\n            chosen_messages = example['chosen'][-1:]\n            rejected_messages = example['rejected'][-1:]\n            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n        return example\n\n    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n                desc='Formatting comparisons with prompt template',)\n\n    for split in ['train', 'test']:\n        dataset[split] = dataset[split].rename_columns(\n            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected', 'messages': 'messages'}\n        )\n\n    return dataset['train'], dataset['test']\n\n# Assuming 'tokenizer' and 'dataset_name' are already defined\ntrain, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n\n# Check the chat template!!! &lt;s&gt; should not be included when tokenizing the respones\n\nAfter preparing and formatting your dataset for fine-tuning, it‚Äôs crucial to inspect the data to ensure that it has been correctly processed. This step helps you verify that the prompt, chosen, rejected, and messages fields are properly formatted and contain the expected information.\n\nprint('Prompt:', train['prompt'][0])\nprint('\\n\\nChosen:', train['chosen'][0])\nprint('\\n\\nRejected:', train['rejected'][0])\nprint('\\n\\nMessages (incl. prompt):', train['messages'][0])"
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-and-running-training",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-and-running-training",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up and Running Training",
    "text": "Setting Up and Running Training\nIn this tutorial, we will go through the process of setting up and running the training for your model. This includes configuring training parameters, creating a custom logging callback, and initiating the training process.\nHere is a step-by-step guide to setting up and running the training:\n\nSet Training Parameters: Define the training parameters such as the model name, number of epochs, gradient accumulation steps, batch size, and the directory to save the results.\nCreate a Custom Logging Callback: Implement a custom callback to log training metrics to a file. This callback will write the training and evaluation loss to a log file and save the trainable parameters at checkpoint steps.\nInitialize the Logging Callback: Create an instance of the custom logging callback with the specified log file path.\n\nHere is the code to perform these steps:\n\nmodel_name = model_id.split('/')[-1]\n\nepochs=1\ngrad_accum=4\nbatch_size=8\nfine_tune_tag='ORPO'\nsave_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\nprint(save_dir)\n\n\nimport transformers\nimport os\nimport torch\n\n# Custom callback to log metrics\nclass LoggingCallback(transformers.TrainerCallback):\n    def __init__(self, log_file_path):\n        self.log_file_path = log_file_path\n\n    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n        with open(self.log_file_path, 'a') as f:\n            if 'loss' in logs:\n                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n                if 'eval_loss' in logs:\n                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n                f.flush()  # Force flush the buffered data to file\n\n        # Check if the current step is a checkpoint step\n        if state.global_step % int(args.save_steps) == 0:\n            # Check if the last checkpoint path exists\n            if state.best_model_checkpoint:\n                checkpoint_dir = state.best_model_checkpoint\n            else:\n                # If not, construct the checkpoint directory path\n                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n\n            # Ensure the checkpoint directory exists\n            os.makedirs(checkpoint_dir, exist_ok=True)\n\n            # Save trainable params in the checkpoint directory\n            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n            torch.save(current_trainable_params_state_dict, file_path)\n\n# Log file path\ncache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\nlog_file_path = os.path.join(cache_dir, 'training_logs.txt')\n\n# Create an instance of the custom callback\nlogging_callback = LoggingCallback(log_file_path)"
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-orpo-training",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-orpo-training",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up ORPO Training",
    "text": "Setting Up ORPO Training\nIn this section, we‚Äôll walk through setting up and training a model using the ORPOTrainer from the trl library.\nI trained the model on the entire dataset (38k samples) using an RTX 4090 GPU (24 GB of VRAM). The training took 7 hours and 35 minutes. You can use smaller GPUs with less VRAM and a smaller batch size. In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like ‚Äòmax_num_samples = 10000‚Äô to only load 10k samples.\n\nConfigure ORPO\nDefine the configuration for the ORPO training. This configuration includes various hyperparameters and settings for training.\n\nfrom trl import ORPOTrainer, ORPOConfig\nfrom unsloth import is_bfloat16_supported\n\norpo_config = ORPOConfig(\n    beta=0.2,\n    save_steps=500, \n    logging_steps=1,\n    num_train_epochs=epochs,\n    output_dir=save_dir,\n    evaluation_strategy='steps', \n    do_eval=True,\n    eval_steps=0.2,\n    per_device_eval_batch_size=batch_size,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=grad_accum,\n    log_level='debug',\n    optim='paged_adamw_8bit',\n    fp16 = not is_bfloat16_supported(),\n    bf16 = is_bfloat16_supported(),\n    max_grad_norm=0.3,\n    lr_scheduler_type='linear',\n    warmup_ratio=0.03,\n    learning_rate=1e-4, \n\n    max_prompt_length=512,\n    max_length=1024,\n\n    max_completion_length=1024,\n    remove_unused_columns=True,\n    \n)\n\n\n\nInitialize ORPOTrainer\nCreate an instance of ORPOTrainer with the model, datasets, tokenizer, and the configuration defined earlier.\n\norpo_trainer = ORPOTrainer(\n    model,\n    args=orpo_config,\n    train_dataset=train,\n    eval_dataset=test,\n    tokenizer=tokenizer,\n\n    callbacks=[logging_callback], # Add custom callback here\n)\n\n\n\nTrain the Model\nSet the model configuration to avoid cache warnings and start the training process.\n\nmodel.config.use_cache = False # silence the warnings\norpo_trainer.train()"
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#plotting-training-and-evaluation-losses-with-matplotlib",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#plotting-training-and-evaluation-losses-with-matplotlib",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Plotting Training and Evaluation Losses with Matplotlib",
    "text": "Plotting Training and Evaluation Losses with Matplotlib\nAfter training your model, it‚Äôs important to visualize the training and evaluation losses to understand how well your model is performing and to identify any potential issues. Visualizing the losses can help you diagnose problems such as overfitting or underfitting and make informed decisions about further training or model adjustments.\n\nimport matplotlib.pyplot as plt\n\n# Initialize lists to hold training and evaluation losses and steps\ntrain_losses = []\neval_losses = []\ntrain_steps = []\neval_steps = []\n\n# Populate the lists from the log history\nfor entry in orpo_trainer.state.log_history:\n    if 'loss' in entry:\n        train_losses.append(entry['loss'])\n        train_steps.append(entry['step'])\n    if 'eval_loss' in entry:\n        eval_losses.append(entry['eval_loss'])\n        eval_steps.append(entry['step'])\n\n# Plot the losses\nplt.plot(train_steps, train_losses, label='Train Loss')\nplt.plot(eval_steps, eval_losses, label='Eval Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\nLet‚Äôs now check the W&B plots. While the loss goes down, we also can see that the difference between the chosen and rejects answers becomes clearer."
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#merging-adapters-and-saving-the-model-to-hugging-face-hub",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#merging-adapters-and-saving-the-model-to-hugging-face-hub",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Merging Adapters and Saving the Model to Hugging Face Hub",
    "text": "Merging Adapters and Saving the Model to Hugging Face Hub\nIn the subsequent steps, we merge the adapters with the original model using 16-bit precision to enhance quality. Initially, we save it locally in the ‚Äúmodel‚Äù directory before uploading it to the Hugging Face Hub. The trained model is available at llmat/Mistral-v0.3-7B-ORPO.\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"llmat/Mistral-v0.3-7B-ORPO\", tokenizer, save_method=\"merged_16bit\")"
  }
]