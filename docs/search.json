[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Articles",
    "section": "",
    "text": "LLM Evaluation Framework\n\n\nReplicate Huggingface Open LLM Leaderboard Locally\n\n\n\nLLM Evaluation\n\n\n\n\n\n\nApr 22, 2025\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuild a High-Quality DPO Dataset\n\n\nBuilding a High-Quality DPO Dataset by Aggregating Best Answers\n\n\n\nPost-Training LLM\n\n\n\n\n\n\nNov 1, 2024\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonte Carlo Tree Self-Refine\n\n\nUnlocking Advanced Mathematical Reasoning with Monte Carlo Tree Self-Refine\n\n\n\nInference\n\n\n\n\n\n\nSep 30, 2024\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Fine-Tuning to Deployment\n\n\nHarnessing Custom LLMs with Ollama and Quantization\n\n\n\nPost-Training LLM\n\n\n\n\n\n\nAug 31, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-Tune Mistral v0.3 with ORPO and Unsloth\n\n\nLow-Rank Adapter Model Fine-tuning\n\n\n\nPost-Training LLM\n\n\n\n\n\n\nJul 30, 2024\n\n\n11 min\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Matthias De Paolis",
    "section": "",
    "text": "Matthias De Paolis is a Machine Learning Engineer and Data Scientist at Yarowa AG, serving as the lead AI Engineer. He holds a Master in Applied Information and Data Science from the university of applied science in Lucerne.\nConnect with him on LinkedIn @matthiasdepaolis.\n\nCredits:\n\nEmojis used in figures are designed by OpenMoji, the open-source emoji and icon project. License: CC BY-SA 4.0.\nVector icons are provided by Streamline (https://streamlinehq.com). License: CC BY-SA 4.0."
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "",
    "text": "Supervised fine-tuning of large language models is a crucial step in natural language processing. It‚Äôs about taking a pre-trained model and making it really good at specific tasks. You start with a model that‚Äôs been trained on a massive amount of text. This gives it a solid understanding of language, including grammar, syntax, and semantics. However, while these models can generate text that sounds human, they often need more training to excel at particular tasks like sentiment analysis, question answering, or named entity recognition. This is where supervised fine-tuning comes in. You take the pre-trained model and train it further on a labeled dataset that‚Äôs specific to the task. The labeled data provides the guidance the model needs to learn how to perform the task well.\nAfter pretraining, auto-regressive models like TinyLlama can predict the next token in a sequence. However, this capability alone doesn‚Äôt make them particularly useful as assistants since they don‚Äôt inherently respond to instructions. That‚Äôs why we use instruction tuning to align their responses with human expectations. Instruction tuning involves two main techniques:\nThe data typically has two columns: User and Assistant. The rows are created by humans or other language models to help the model get used to the flow of conversation.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-model",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-model",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Load Model",
    "text": "Load Model\n\n%env HF_HUB_ENABLEHF_TRANSFER = True\n\nenv: HF_HUB_ENABLEHF_TRANSFER=True\n\n\n\n#!pip install wandb -q -U\nimport wandb\nimport os\n\n#%env WANDB_NOTEBOOK_NAME = $Fine_tune_tinyllama_with_DPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\n\n#!python -m pip install --upgrade pip\n#!pip install -U -q transformers\n#!pip install -U -q bitsandbytes\n#!pip install -U -q peft\n#!pip install -U -q accelerate\n#!pip install -U -q scipy\n#!pip install -U -q trl\n#!pip install -U -q torch\n\n\ncache_dir = ''\n\nmodel_id = \"./TinyLlama/TinyLlama_v1.1\"\nnew_model = \"./llmat/TinyLlama-1.1B_SFT\"",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-model-and-tokenizer-for-lora",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-model-and-tokenizer-for-lora",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Load the Model and Tokenizer for LoRA",
    "text": "Load the Model and Tokenizer for LoRA\nIn this example, we are using the TinyLlama base model (1.1B parameters). If you have a smaller GPU, such as those available in a Google Colab notebook, you can use quantization to make the model fit. Quantization reduces the size of the model by representing its weights with fewer bits.\nWhen specifying the data type:\n\nUse bfloat16 for newer GPUs like the A6000 or A100.\nUse float16 for other GPUs. Also, if you are using Colab, you might need to disable Flash Attention, which is an optimization technique for faster processing.\n\nCode Explanation: 1. Set the directories for the model: Define where your models are located.\n\nmodel_id is the path to the TinyLlama base model.\nnew_model is the path to the modified TinyLlama model.\n\n\nImport necessary libraries:\n\n\nAutoTokenizer and AutoModelForCausalLM from the transformers library are used to load the tokenizer and model.\ntorch is the PyTorch library, used for tensor computations and model handling.\nBitsAndBytesConfig is used for model quantization settings.\n\n\nConfigure quantization settings: Create a configuration object bnb_config to specify quantization parameters:\n\n\nload_in_4bit=True: Load the model weights in 4-bit precision.\nbnb_4bit_use_double_quant=True: Use double quantization for better precision.\nbnb_4bit_quant_type=‚Äònf4‚Äô: Specify the type of quantization.\nbnb_4bit_compute_dtype=torch.bfloat16: Set the data type for computation to bfloat16.\n\n\nLoad the model: Use the AutoModelForCausalLM.from_pretrained function to load the model with the specified configurations.\n\n\nmodel_id: Path to the TinyLlama base model.\ndevice_map=‚Äòauto‚Äô: Automatically map the model to available devices (GPUs/CPUs).\ntorch_dtype=torch.bfloat16: Set the data type for the model.\n\n\nLoad the tokenizer: Use the AutoTokenizer.from_pretrained function to load the tokenizer.\n\n\ncache_dir = ''\n\nmodel_id = \"./TinyLlama/TinyLlama_v1.1\"\nnew_model = \"./llmat/TinyLlama-1.1B_SFT\"\n\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    #config=config,\n    #quantization_config=bnb_config,\n    #rope_scaling={'type': 'Linear', 'factor': 2.0},\n    device_map='auto',\n    #attn_implementation = \"flash_attention_2\",\n    torch_dtype=torch.bfloat16,\n    cache_dir=cache_dir\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True, cache_dir=cache_dir)\n\nin oss file",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#loading-checks",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#loading-checks",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Loading Checks",
    "text": "Loading Checks\nAfter loading the model, it‚Äôs crucial to ensure that all the model‚Äôs parameters are correctly loaded onto the GPU. If any parameters are still on the CPU (referred to as the ‚Äúmeta device‚Äù), it means they haven‚Äôt been properly moved to the GPU.\nCode Explanation: 1. Check parameter locations: Iterate through all the parameters of the model to verify their device location.\n\nmodel.named_parameters(): This function returns an iterator over model parameters, providing both the name (n) and the parameter itself (p).\np.device.type==‚Äòmeta‚Äô: This condition checks if a parameter is on the ‚Äúmeta‚Äù device, which indicates it is not on the GPU.\nIf any parameter is found on the ‚Äúmeta‚Äù device, a message is printed to identify which parameter is not correctly loaded.\n\n\nPrint model configurations: Display important configurations of the model.\n\n\nmodel.config.max_position_embeddings: This prints the maximum number of positions the model can handle, which relates to the input sequence length.\nmodel.config.eos_token_id: This prints the ID of the end-of-sequence token, which the model uses to identify the end of a text sequence.\n\n\n# Check there are no parameters overflowing onto cpu (meta).\nfor n, p in model.named_parameters():\n    if p.device.type=='meta':\n        print(f\"{n} is on meta!\")\n\n\nprint(model.config.max_position_embeddings)\nprint(model.config.eos_token_id)\n\n2048\n2",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-and-run-training",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-and-run-training",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Set up and run Training",
    "text": "Set up and run Training\n\nmodel_name = model_id.split('/')[-1]\n\nepochs=1\ngrad_accum=4\nbatch_size=8\nfine_tune_tag='SFT'\nsave_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\nprint(save_dir)\n\n./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT\n\n\n\nimport transformers\nimport os\nimport torch\n\n# Custom callback to log metrics\nclass LoggingCallback(transformers.TrainerCallback):\n    def __init__(self, log_file_path):\n        self.log_file_path = log_file_path\n\n    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n        with open(self.log_file_path, 'a') as f:\n            if 'loss' in logs:\n                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n                if 'eval_loss' in logs:\n                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n                f.flush()  # Force flush the buffered data to file\n\n        # Check if the current step is a checkpoint step\n        if state.global_step % int(args.save_steps) == 0:\n            # Check if the last checkpoint path exists\n            if state.best_model_checkpoint:\n                checkpoint_dir = state.best_model_checkpoint\n            else:\n                # If not, construct the checkpoint directory path\n                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n\n            # Ensure the checkpoint directory exists\n            os.makedirs(checkpoint_dir, exist_ok=True)\n\n            # Save trainable params in the checkpoint directory\n            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n            torch.save(current_trainable_params_state_dict, file_path)\n\n# Log file path\ncache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\nlog_file_path = os.path.join(cache_dir, 'training_logs.txt')\n\n# Create an instance of the custom callback\nlogging_callback = LoggingCallback(log_file_path)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#sft",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#sft",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "SFT",
    "text": "SFT\nSetting a fine-tuning tag (either SFT or ORPO), we then set up a logging callback, connect weights and biases, and clean the dataset for supervised fine-tuning. The SFT trainer automatically uses the chat template of the tokenizer, allowing for consistent formatting. We use a maximum sequence length of 1,000 tokens and a learning rate of 1e-4, with an optional warm-up period.\n\n# Remove unnecessary columns\ntrain = train.remove_columns(['prompt', 'chosen', 'rejected'])\n\nprint(train)\n\nDataset({\n    features: ['messages'],\n    num_rows: 43245\n})\n\n\n\nfrom transformers import Trainer\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    # peft_config=peft_config, # comment out if passing a peft model\n    max_seq_length=2048,\n    tokenizer=tokenizer, # Trainer uses the chat template passsed by the tokenizer. If data consist only of the column messages, this is fine\n    model=model,\n    train_dataset=train,\n    eval_dataset=test,\n    args=transformers.TrainingArguments(\n        save_steps=150,\n        logging_steps=1,\n        num_train_epochs=epochs,\n        output_dir=save_dir,\n        eval_strategy='steps',\n        do_eval=True,\n        eval_steps=0.2,\n        per_device_eval_batch_size=batch_size,\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=grad_accum,\n        log_level='debug',\n        #optim='paged_adamw_8bit',\n        #fp16=True, # For non-Ampere GPUs\n        bf16=True, # For Ampere GPUs or later\n        max_grad_norm=0.3,\n        lr_scheduler_type='linear',\n        #hub_private_repo=True,\n        warmup_ratio=0.03, # optional, may help stability at the (learning rate is lower for the first steps)\n        optim='adamw_torch', \n        learning_rate=1e-4, \n    ),\n\n    callbacks=[logging_callback] # Add custom callback here\n    # neftune_noise_alpha=5 # Aff in noise to embeddings to improve...\n)\n        \n\nPyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\nPyTorch: setting up devices\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\nUsing auto half precision backend\n\n\n\nmodel.config.use_cache=False # silence the warnings\ntrainer.train()\n\nCurrently training with a batch size of: 8\n***** Running training *****\n  Num examples = 43,245\n  Num Epochs = 1\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 4\n  Total optimization steps = 1,351\n  Number of trainable parameters = 65,628,160\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\nwandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\n\nTracking run with wandb version 0.17.1\n\n\nRun data is saved locally in /workspace/_LEARNING/me/FineTuning/wandb/run-20240612_091154-2f6so3xa\n\n\nSyncing run ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/demaz/huggingface\n\n\n View run at https://wandb.ai/demaz/huggingface/runs/2f6so3xa\n\n\n\n    \n      \n      \n      [1351/1351 1:11:40, Epoch 0/1]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n271\n1.450700\n1.353275\n\n\n542\n1.271700\n1.335227\n\n\n813\n1.317600\n1.329108\n\n\n1084\n1.406600\n1.327019\n\n\n\n\n\n\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050/special_tokens_map.json\n***** Running Evaluation *****\n  Num examples = 1000\n  Batch size = 8\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200/special_tokens_map.json\nSaving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350/tokenizer_config.json\nSpecial tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n\nTrainOutput(global_step=1351, training_loss=1.3279773617214312, metrics={'train_runtime': 4304.6658, 'train_samples_per_second': 10.046, 'train_steps_per_second': 0.314, 'total_flos': 2.616444791832576e+17, 'train_loss': 1.3279773617214312, 'epoch': 0.9996300406955235})\n\n\n\ntrainer.save_model(new_model)\n\nSaving model checkpoint to ./llmat/TinyLlama-1.1B_SFT\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\ntokenizer config file saved in ./llmat/TinyLlama-1.1B_SFT/tokenizer_config.json\nSpecial tokens file saved in ./llmat/TinyLlama-1.1B_SFT/special_tokens_map.json",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#plotting",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#plotting",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Plotting",
    "text": "Plotting\n\nimport matplotlib.pyplot as plt\n\n# Initialize lists to hold training and evaluation losses and steps\ntrain_losses = []\neval_losses = []\ntrain_steps = []\neval_steps = []\n\n# Populate the lists from the log history\nfor entry in trainer.state.log_history:\n    if 'loss' in entry:\n        train_losses.append(entry['loss'])\n        train_steps.append(entry['step'])\n    if 'eval_loss' in entry:\n        eval_losses.append(entry['eval_loss'])\n        eval_steps.append(entry['step'])\n\n# Plot the losses\nplt.plot(train_steps, train_losses, label='Train Loss')\nplt.plot(eval_steps, eval_losses, label='Eval Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nTraining results show a decrease in training loss and validation loss, indicating effective fine-tuning.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#save-trainable-params-if-training-non-lora-modules",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#save-trainable-params-if-training-non-lora-modules",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Save trainable params if training non-LoRA modules",
    "text": "Save trainable params if training non-LoRA modules\nAfter training, we save the trainable parameters and evaluate the model.\n\n# Update the dictionary to reflect the final state of the model's\ntrainable_params_state_dict = {n: p.data for n, p in model.named_parameters() if p.requires_grad}\n\n# Save the final state of the trainable parameters (ONLY RELEVANT FOR NON-LORA ADAPTERS)\nfinal_save_path = os.path.join(save_dir, 'trainable_params_final.pt')\ntorch.save(trainable_params_state_dict, final_save_path)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#install-libraries",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#install-libraries",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Install Libraries",
    "text": "Install Libraries\nOur first step involves installing the necessary libraries. Once the installation is complete, we can proceed to load the model.\n\n#!python -m pip install --upgrade pip\n#!pip install -U -q transformers\n#!pip install -U -q bitsandbytes\n#!pip install -U -q peft\n#!pip install -U -q accelerate\n#!pip install -U -q scipy\n#!pip install -U -q trl\n#!pip install -U -q torch\n#!pip install wandb -q -U\n\n\n%env HF_HUB_ENABLEHF_TRANSFER = True\n\nenv: HF_HUB_ENABLEHF_TRANSFER=True\n\n\n\nimport wandb\nimport os\n\n#%env WANDB_NOTEBOOK_NAME = $Fine_tune_tinyllama_with_DPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#prepare-for-lora-fine-tuning",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#prepare-for-lora-fine-tuning",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Prepare for LoRA fine-tuning",
    "text": "Prepare for LoRA fine-tuning\nWe will fine-tune the model using Low-Rank Adaptation (LoRA) to reduce the VRAM (Video RAM) usage. LoRA helps to make the training process more efficient by focusing on specific parts of the model. In this case, we‚Äôll train the attention layers and the language model head (LM head) to ensure that the model can handle chat templating effectively. The chosen configuration uses a rank of 8 and a LoRA alpha of 32, which are parameters that control the adaptation process.\nThe function below prints the number of trainable parameters:\nCode Explanation: 1. Print trainable parameters: This function helps to understand which parts of the model can be trained and how many parameters are involved. - model.named_parameters(): This function returns an iterator over model parameters, providing both the name (name) and the parameter itself (param). - param.requires_grad: This condition checks if a parameter is trainable. - The function prints the names of all trainable parameters and counts the total number of trainable and non-trainable parameters.\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainabl√∂e parameters in the model and lists which parameters\n    \"\"\"\n    trainable_params = 0\n    non_trainable_params = 0\n    all_params = 0\n\n    print(\"Trainable Parameters\")\n    for name, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n            print(f\" {name}\")\n        else:\n            non_trainable_params += param.numel()\n\n    print(\"\\nNon-Trainable Parameters:\")\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            print(f\" {name}\")\n\n    print(\n        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n        \n\nSpecifically, we‚Äôll train the attention layers, the up and down layers, and the language model head (LM head) to ensure proper chat templating. The chosen configuration uses a rank of 8 and a LoRA alpha of 32.\nCode Explanation: 1. Enable gradient checkpointing: This feature saves VRAM by storing the intermediate activations during training.\n\nmodel.gradient_checkpointing_enable(): Enable this if you want to save VRAM.\n\n\nPrepare model for LoRA fine-tuning: Convert the model to use LoRA.\n\n\nprepare_model_for_kbit_training(model): Uncomment this if you are using quantization.\n\n\nDefine LoRA configuration: Set up the parameters for LoRA fine-tuning.\n\n\nLoraConfig: Create a configuration object for LoRA.\nr=8: Set the rank to 8.\nlora_alpha=32: Set the LoRA alpha to 32.\ntarget_modules: Specify the layers of the model to be fine-tuned.\nlora_dropout=0.1: Set the dropout rate for LoRA.\nbias=‚Äúnone‚Äù: Do not include additional biases.\ntask_type=‚ÄúCAUSAL_LM‚Äù: Specify the task type as causal language modeling.\n\n\nApply LoRA to the model: Convert the model to use the defined LoRA configuration.\n\n\nget_peft_model(model, peft_config): Apply the LoRA configuration to the model.\n\n\nfrom peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable() # Comment this in to save VRAM\n# model = prepare_model_for_kbit_training(model) # only set this if using quantization\n\nfrom peft import LoraConfig, get_peft_model\n\npeft_config = LoraConfig( # matching the Llama recipe\n    r=8,\n    lora_alpha=32,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        # \"self_attn.rotary_emb.inv_freq\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n        #\"input_layernorm.weight\", #can't be lora fine-tuned as it's not a linear layer\n        #\"post_attention_layernorm.weights, #can't be lora fine-tuned as it's not a linear layer\n        #\"model.norm.weight\", #can't be lora fine-tuned as it's not a linear layer\n    ],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, peft_config) # move to a peft model",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-you-are-changing-the-template",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-you-are-changing-the-template",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Set embed and norm layers to trainable (recommended for chat fine-tuning if you are changing the template)",
    "text": "Set embed and norm layers to trainable (recommended for chat fine-tuning if you are changing the template)\nAfter setting the pad token to the UN token already present in the tokenizer, we enable the trainability of the embedding and normalization layers. These layers cannot be adapted with LoRA, but fine-tuning them is crucial for adjusting the chat templating. Since we‚Äôre fine-tuning a base model unfamiliar with our chat format, making these layers trainable increases the number of trainable parameters, typically by 1-5%.\n\n# List to hold the names of the trainable parameters\ntrainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n\n# Set modules to be trainable\nfor n, p in model.named_parameters():\n    if any(k in n for k in trainable_params_names):\n        p.requires_grad_(True)\n    else:\n        p.requires_grad_(False) # Optional: Set the rest to be trainable\n\n# Make a dictionary of trainable parameters\ntrainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n\n# Convert trainable_params to state_dict format\ntrainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}\n\n\nprint_trainable_parameters(model)\n\nTrainable Parameters\n base_model.model.model.embed_tokens.weight\n base_model.model.model.layers.0.input_layernorm.weight\n base_model.model.model.layers.0.post_attention_layernorm.weight\n base_model.model.model.layers.1.input_layernorm.weight\n base_model.model.model.layers.1.post_attention_layernorm.weight\n base_model.model.model.layers.2.input_layernorm.weight\n base_model.model.model.layers.2.post_attention_layernorm.weight\n base_model.model.model.layers.3.input_layernorm.weight\n base_model.model.model.layers.3.post_attention_layernorm.weight\n base_model.model.model.layers.4.input_layernorm.weight\n base_model.model.model.layers.4.post_attention_layernorm.weight\n base_model.model.model.layers.5.input_layernorm.weight\n base_model.model.model.layers.5.post_attention_layernorm.weight\n base_model.model.model.layers.6.input_layernorm.weight\n base_model.model.model.layers.6.post_attention_layernorm.weight\n base_model.model.model.layers.7.input_layernorm.weight\n base_model.model.model.layers.7.post_attention_layernorm.weight\n base_model.model.model.layers.8.input_layernorm.weight\n base_model.model.model.layers.8.post_attention_layernorm.weight\n base_model.model.model.layers.9.input_layernorm.weight\n base_model.model.model.layers.9.post_attention_layernorm.weight\n base_model.model.model.layers.10.input_layernorm.weight\n base_model.model.model.layers.10.post_attention_layernorm.weight\n base_model.model.model.layers.11.input_layernorm.weight\n base_model.model.model.layers.11.post_attention_layernorm.weight\n base_model.model.model.layers.12.input_layernorm.weight\n base_model.model.model.layers.12.post_attention_layernorm.weight\n base_model.model.model.layers.13.input_layernorm.weight\n base_model.model.model.layers.13.post_attention_layernorm.weight\n base_model.model.model.layers.14.input_layernorm.weight\n base_model.model.model.layers.14.post_attention_layernorm.weight\n base_model.model.model.layers.15.input_layernorm.weight\n base_model.model.model.layers.15.post_attention_layernorm.weight\n base_model.model.model.layers.16.input_layernorm.weight\n base_model.model.model.layers.16.post_attention_layernorm.weight\n base_model.model.model.layers.17.input_layernorm.weight\n base_model.model.model.layers.17.post_attention_layernorm.weight\n base_model.model.model.layers.18.input_layernorm.weight\n base_model.model.model.layers.18.post_attention_layernorm.weight\n base_model.model.model.layers.19.input_layernorm.weight\n base_model.model.model.layers.19.post_attention_layernorm.weight\n base_model.model.model.layers.20.input_layernorm.weight\n base_model.model.model.layers.20.post_attention_layernorm.weight\n base_model.model.model.layers.21.input_layernorm.weight\n base_model.model.model.layers.21.post_attention_layernorm.weight\n base_model.model.model.norm.weight\n\nNon-Trainable Parameters:\n base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.0.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.0.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.1.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.1.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.2.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.2.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.3.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.3.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.4.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.4.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.5.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.5.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.6.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.6.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.7.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.7.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.8.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.8.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.9.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.9.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.10.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.10.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.11.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.11.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.12.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.12.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.13.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.13.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.14.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.14.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.15.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.15.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.16.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.16.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.17.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.17.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.18.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.18.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.19.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.19.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.20.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.20.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight\n base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight\n base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight\n base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n base_model.model.model.layers.21.mlp.up_proj.base_layer.weight\n base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n base_model.model.model.layers.21.mlp.down_proj.base_layer.weight\n base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n base_model.model.lm_head.base_layer.weight\n base_model.model.lm_head.lora_A.default.weight\n base_model.model.lm_head.lora_B.default.weight\n\nSummary:\n Trainable params: 65628160\n Non-Trainable params: 1041000448\n All Parameters: 1106628608",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-evaluation",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-evaluation",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Set up Evaluation",
    "text": "Set up Evaluation\nTo evaluate the model‚Äôs performance, we define simple evaluation questions, such as ‚ÄúWhat is 1+1?‚Äù and ‚ÄúGive me some Python code to add the first five Fibonacci numbers.‚Äù Running this evaluation on the base model shows that it isn‚Äôt familiar with the chat template, as it repeats the question and provides suboptimal code responses.\n\nfrom transformers import TextStreamer\nfrom peft import PeftModel\nimport torch\nimport gc # import Python's garbage collection module\n\n# Define a stream\ndef stream(user_prompt, model_type, tokenizer, checkpoint=''):\n    if model_type == 'base':\n        eval_model = model\n    elif model_type == 'fine-tuned':\n        eval_model = PeftModel.from_pretrained(model, checkpoint)\n        eval_model = eval_model.to('cuda')\n\n        for n, p in eval_model.named_parameters():\n            if p.device.type == 'cpu':\n                print(f'{n} is on cpu!')\n    \n    else:\n        print(\"You must set the model_type to base or fine-tuned\")\n        exit()\n    \n    print(f'Proceeding to inference with peft adapters from {checkpoint}')\n    \n    eval_model.config.use_cache = True\n\n    messages = [\n        { 'role': 'user', 'content': f\"{user_prompt.strip()}\"},\n    ]\n\n    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    inputs = tokenizer([inputs], return_tensors='pt', add_special_tokens=False).to('cuda')\n\n    if \"token_type_ids\" in inputs:\n        del inputs[\"token_type_ids\"]\n    \n    streamer = TextStreamer(tokenizer)\n    \n    print(f'eval_model is on: {next(eval_model.parameters()).device}') # Debug line\n    print(f'input_ids are on: {inputs[\"input_ids\"].device}') # Debug line\n\n    # Despite returning the usal output, the streamer will also print the generated \n    #_ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=250, do_s)\n    _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=250, do_sample=True)\n    \n    # Clear GPU cache and run garbage collection\n    torch.cuda.empty_cache()\n    gc.collect()\n    \ndef evaluation(model_type, checkpoint=''):\n    questions = [\n        \"What is one plus one?\",\n        \"Give me some python code to add the first five Fibonacci numbers.\",\n    ]\n    \n    answers = [\n        \"Two.\",\n        \"...\",\n    ]\n    \n    for question , answer in zip(questions, answers):\n        stream(question, model_type, tokenizer, checkpoint)\n        print('\\n\\n')\n        \n\n\nprint(model.config)\n\nLlamaConfig {\n  \"_name_or_path\": \"./TinyLlama/TinyLlama_v1.1\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 5632,\n  \"max_position_embeddings\": 2048,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 22,\n  \"num_key_value_heads\": 4,\n  \"pad_token_id\": 0,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.41.2\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n\n\n\nprint(model.generation_config)\n\nGenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"max_length\": 2048,\n  \"pad_token_id\": 0\n}\n\n\n\n\nevaluation('base', tokenizer)\n\nProceeding to inference with peft adapters from LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;unk&gt;'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n    0: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    1: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\neval_model is on: cuda:0\ninput_ids are on: cuda:0\n&lt;s&gt; [INST] What is one plus one? [/INST] IN TH MEMEME ME MEME MEMEME MEME MEMEME ME MEME MEME ME ME MEMEME E ME MEME MEMEME ME ME ME MEMEME MEME MEME MEMEME MEAT ME MEMEME ME DMEME MEMEME MEVER MEME EMEME TEMEME MEMEME MAR MARMAMMAMA MARMAT MARMAMMA MA MR MARMA MR M MAR MR MAR MR M MAR MR M MAR MR MR MR MR MR MR MR MR MR M MR MR MR MR MR MR MR M MR MR MR MR MAR MR MRMR MAR M MARMR MAR MAR MARMA MAR H MAR MARMA MAR Mar MAR MAR MAR MAMA RAT MR M MR MR MR MR MR MAR MAR MAR MA MR MR MR MR MR MRMMMRMMMRMRMMMR MYMA MR MR MR MR MRMRMRMPMRTR MR MRMRMM MR MR MRMA MR MRMR MR MR MRMRMRMRMR MR TUR MR MRMT MRM MRMR M TR MR MR MR MR MR MR MRMRMR MR T MR MR MR MR MR R MR MR MR MR MRMO MR MR MRMR mour MR MR MRMMR MR\n\n\n\nProceeding to inference with peft adapters from LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;unk&gt;'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n    0: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    1: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\neval_model is on: cuda:0\ninput_ids are on: cuda:0\n&lt;s&gt; [INST] Give me some python code to add the first five Fibonacci numbers. [/INST]\"]], [\"INFORMATION\"]].\n[[INFORMATION]]\nNAME: \"Alexandre\"\nPIT: -1.0\nREMARKS: \"French\", \"Dutch\", [\"German\", \"Vietnamese\", \"Scissors, Cut\", \"Teddy bear\", \"French\"]\n\n\n{\n  \"NAME\": \"Maria\",\n  \"REMARKS\": \".Japanese\",\n  \"WITHDRAW\":\n     {\"MOTHER\": \"\",\n      \"POTATOES\": \"12\"},\n  \"CLOSESURE\": {\n     \"$TYPE\": {\n       \"MOTHER\": \"'French'\"\n     },\n     \"CHILD\": [\n       {\n         \"MOTHER\": \"0\",\n         \"POTATOES\": \"122\"\n       }]\n    }\n}\n\n\n\n\n\n\n\n&lt;/s&gt;",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-dataset",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#load-the-dataset",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Load the Dataset",
    "text": "Load the Dataset\nNext, we load the dataset. The dataset we are using is designed for DPO training. Therefore it includes chosen and rejected responses. The dataset, is a combination of different high-quality datasets, already contains formatted chosen and rejected answers. This dataset, derived from high-quality responses.\nThe dataset script applies the chat template to three elements: the chosen text, rejected text, and the prompt. For the prompt message, we take all but the last message, while for the chosen and rejected responses, we take only the last message. This setup creates a dataset with formatted prompts, chosen responses, and rejected responses. Additionally, an extra field includes the full list of messages for supervised fine-tuning.\n\nprint(tokenizer.chat_template)\n\n{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n\n\n\n\n#¬†Prepared with the help of code from: https://github.com/xfactlab/orpo/blob/main...\nimport json\n\n# Load the dataset\ndataset_name = 'llmat/dpo-orpo-mix-50k' #¬†Ensure this is defined\n\nmax_num_samples = None #¬†Set to None to use the full dataset\n# max_num_samples = 1000 #¬†set to None to use the full dataset\n\nfrom datasets import load_dataset\n\ndef build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_split_max=1000):\n    # Determin the split specification based on max_num samples\n    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n\n    # Load the dataset\n    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n\n    # Shuffle the dataset\n    if max_num_samples is not None:\n        full_data = full_data.shuffle(seed=42)\n    else:\n        full_data = full_data\n\n    # Determine the number of test samples\n    num_total_samples = len(full_data)\n    test_size = min(test_split_max, min(1000, int(0.1 * num_total_samples)))\n\n    # Randomly split the data into training and test sets\n    dataset = full_data.train_test_split(test_size=test_size)\n\n    # ds_train = dataset['train']\n    # ds_test = dataser['test']\n\n    column_names = list(dataset['train'].features)\n\n    def apply_dpo_template(example):\n        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n        if all(k in example.keys() for k in ('chosen', 'rejected')):\n            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n            # We therefore need to extract the N-1 turns to form the prompt\n            prompt_messages = example['chosen'][:-1]\n            example['messages'] = example['chosen']\n\n            # Now we extract the final turn to define chosen/rejected responses\n            chosen_messages = example['chosen'][-1:]\n            rejected_messages = example['rejected'][-1:]\n            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n        return example\n\n    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n                desc='Formatting comparisons with prompt template',)\n\n    for split in ['train', 'test']:\n        dataset[split] = dataset[split].rename_columns(\n            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected', 'messages': 'messages'}\n        )\n\n    return dataset['train'], dataset['test']\n\n# Assuming 'tokenizer' and 'dataset_name' are already defined\ntrain, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n\n# Check the chat template!!! &lt;s&gt; should not be included when tokenizing the respones\n\n\n\n\n\n\n\nPrinting the dataset reveals the formatted prompt, chosen answer, and rejected answer, all combined in a list of dictionaries for use in supervised fine-tuning. We set training parameters, keeping them consistent for both SFT and ORPO for a fair comparison. Training runs for one epoch with a batch size of 16 and gradient accumulation of two, achieving an effective batch size of 32.\n\nprint('Prompt:', train['prompt'][0])\nprint('\\n\\nChosen:', train['chosen'][0])\nprint('\\n\\nRejected:', train['rejected'][0])\nprint('\\n\\nMessages (incl. prompt):', train['messages'][0])\n\nPrompt: &lt;s&gt;[INST] Q: Did Al-Farabi ever meet Mohammed?\nA: no\nExplanation: Al-Farabi was born in 872 AD. Mohammed died in 832 AD.\n\nQ: Can music be used as a weapon?\nA: yes\nExplanation: Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion. The military calls the use of loud disorienting music part of psychological operations.\n\nQ: Can you write a whole Haiku in a single tweet?\nA: [/INST]\n\n\nChosen: A spring breeze whispers  \nThrough cherry blossoms  \nNature's symphony  \n(5 syllables in the first line, 7 syllables in the second line, 5 syllables in the third line)&lt;/s&gt;\n\n\nRejected: No, 140 characters \n\nUnfurl the petal's secret\nA Haiku, unfolding a rustling dance\nHere's a delicate answer.&lt;/s&gt;\n\n\nMessages (incl. prompt): [{'content': 'Q: Did Al-Farabi ever meet Mohammed?\\nA: no\\nExplanation: Al-Farabi was born in 872 AD. Mohammed died in 832 AD.\\n\\nQ: Can music be used as a weapon?\\nA: yes\\nExplanation: Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion. The military calls the use of loud disorienting music part of psychological operations.\\n\\nQ: Can you write a whole Haiku in a single tweet?\\nA:', 'role': 'user'}, {'content': \"A spring breeze whispers  \\nThrough cherry blossoms  \\nNature's symphony  \\n(5 syllables in the first line, 7 syllables in the second line, 5 syllables in the third line)\", 'role': 'assistant'}]",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#train",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#train",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Train!",
    "text": "Train!",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-tokenizer-and-padding",
    "href": "posts/Fine_Tune_TinyLlama_with_SFT.html#set-up-tokenizer-and-padding",
    "title": "Fine-Tune TinyLlama with SFT",
    "section": "Set up Tokenizer and Padding",
    "text": "Set up Tokenizer and Padding\nNext, we load the tokenizer, checking the start and end tokens, and then set up a custom chat template.\n\nprint(tokenizer)\nprint(tokenizer.vocab_size)\n\nLlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n    0: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    1: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n32000\n\n\n\nprint(tokenizer.bos_token)\nprint(tokenizer.eos_token)\n\n&lt;s&gt;\n&lt;/s&gt;\n\n\n\nprint(tokenizer.chat_template)\n\nNone\n\n\nThis template includes the Beginning of Sequence (BOS) token only if there is a user message. If a message sequence starts with an assistant message, the BOS token is excluded. This is done to ensure that the conversation starts correctly depending on the initial message role.\n\ntokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\"\"\"\n\nmessages = [\n    {'role': 'user', 'content': 'write a quick sort algorithm in python.'},\n    {'role': 'assistant', 'content': 'here you are.'},\n    {'role': 'user', 'content': 'great.'},\n]\n\ninputs = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(inputs)\n\n&lt;s&gt;[INST] write a quick sorf algorithm in python. [/INST]here you are.&lt;/s&gt;[INST] great. [/INST]\n\n\nWe are setting the pad token to the ‚Äòunk‚Äô token.\n\n# set the pad token to &lt;pad&gt;, if not &lt;|pad|&gt;, if not &lt;unk&gt; if &lt;unk&gt;\nif '&lt;pad&gt;' in tokenizer.get_vocab():\n    print('&lt;pad&gt; token is is in the tokenizer. Usinh &lt;pad&gt; for pad')\n    #Set the pad token\n    tokenizer.pad_token = '&lt;pad&gt;'\nelif '&lt;|pad|&gt;' in tokenizer.get_vocab():\n    print('&lt;|pad|&gt; token is in the tokenizer. Using for &lt;|pad|&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;|pad|&gt;'\nelif '&lt;unk&gt;' in tokenizer.get_vocab():\n    print('&lt;unk&gt; token is in the tokenizer. Using for &lt;unk&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;unk&gt;'\nelse:\n    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n    tokenizer.pad_token = tokenizer.eos_token\n\n&lt;unk&gt; token is in the tokenizer. Using for &lt;unk&gt; for pad\n\n\n\n# Update pad token id in model and its config\nmodel.pad_token_id = tokenizer.pad_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n#¬†Check if they are equal\nassert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n\n# Print the pad token ids\nprint('Tokenizer pad token ID:', tokenizer.pad_token_id)\nprint('Model pad token ID:', model.pad_token_id)\nprint('Model config pad token ID:', model.config.pad_token_id)\nprint('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n\nTokenizer pad token ID: 0\nModel pad token ID: 0\nModel config pad token ID: 0\nNumber of tokens now in tokenizer: 32000\n\n\n\nprint('Special tokens map:', tokenizer.special_tokens_map)\nprint('All special tokens:', tokenizer.all_special_tokens)\n\nSpecial tokens map: {'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;unk&gt;'}\nAll special tokens: ['&lt;s&gt;', '&lt;/s&gt;', '&lt;unk&gt;']\n\n\n\nprint(tokenizer)\n\nLlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '&lt;s&gt;', 'eos_token': '&lt;/s&gt;', 'unk_token': '&lt;unk&gt;', 'pad_token': '&lt;unk&gt;'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n    0: AddedToken(\"&lt;unk&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    1: AddedToken(\"&lt;s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    2: AddedToken(\"&lt;/s&gt;\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-tune TinyLlama with SFT"
    ]
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html",
    "href": "posts/ORPO_Mistral-unsloth-publish.html",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "",
    "text": "ORPO is an innovative fine-tuning method that merges traditional supervised fine-tuning with preference alignment into a unified process. This approach decreases the computational resources and time needed for training. Additionally, empirical evidence shows that ORPO surpasses other alignment techniques across different model sizes and benchmarks.\nIn this article, we will fine-tune the newest Mistral 7B model using ORPO and the TRL library. The code is accessible on Google Colab and in the LLM Tutorial on GitHub."
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#orpo",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#orpo",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "ORPO",
    "text": "ORPO\nInstruction tuning and preference alignment are crucial methods for customizing Large Language Models (LLMs) for particular tasks. Typically, this entails a multi-step process: first, Supervised Fine-Tuning (SFT) on instructions to tailor the model to the desired domain, and second, applying preference alignment techniques such as Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to enhance the probability of producing preferred responses over less desirable ones.\nResearchers have discovered a drawback in this method. Although SFT successfully adjusts the model to the target domain, it also unintentionally raises the chances of producing both unwanted and desired answers. Therefore, the preference alignment stage is essential to enlarge the disparity between the probabilities of accepted and rejected outputs.\n\n\n\nHong and Lee (2024) presented ORPO, an innovative approach that combines instruction tuning and preference alignment into a single training framework. ORPO modifies the conventional language modeling objective by incorporating the negative log-likelihood loss with an odds ratio (OR) component. This OR loss applies a mild penalty to rejected responses while greatly rewarding preferred ones, allowing the model to simultaneously learn the target task and align with human preferences.\n\\mathscr{L}{ORPO} = \\mathbb{E}{(x, y_{w}, y_l)}[\\mathscr{L}{SFT} + \\lambda \\cdot \\mathscr{L}{OR}]\nORPO has been integrated into key fine-tuning libraries such as TRL, Axolotl, and LLaMA-Factory. The following section will demonstrate its usage with TRL."
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#fine-tuning-mistral-v0.3-with-orpo-and-unsoth",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#fine-tuning-mistral-v0.3-with-orpo-and-unsoth",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Fine-Tuning Mistral v0.3 with ORPO and Unsoth",
    "text": "Fine-Tuning Mistral v0.3 with ORPO and Unsoth\nMistral AI‚Äôs v0.3 is a significant update to their AI model, introducing improved performance and efficiency. This version includes enhanced instruction-following capabilities, making interactions more intuitive. Additionally, Mistral v0.3 incorporates advanced reasoning skills, enabling it to tackle complex tasks more effectively. The update also extends the context length to 32768 tokens, allowing for more detailed and coherent conversations. Technical details include an extended vocabulary (32000 to 32768), a new tokenizer, and support for function calling.\nORPO necessitates a preference dataset that includes a prompt, a selected answer, and a discarded answer. To achieve this, we will utilize llmat/dpo-orpo-mix-38k-balanced, a dataset that merges high-quality DPO datasets and has been further balanced using a clustering-based approach.\nTo efficiently fine-tune our model we will use the unlsoth library. Unsloth significantly improves speed and efficiency in the training of Large Language Models (LLMs). The speed and efficiency gains are achieved through several optimizations, including manual autograd and chained matrix multiplication. Furthermore, it utilizes Flash Attention via xformers and Tri Dao‚Äôs implementation, which is a highly optimized approach to handling attention mechanisms in transformer models. Unsloth makes fine-tuning 2 times faster with 50% less memory usage.\nLet‚Äôs start by installing the required libraries:\n\n!pip install python-dotenv\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers&lt;0.0.27\" \"trl&lt;0.9.0\" peft accelerate bitsandbytes\n\nNow let‚Äôs login to our W&B workspace\n\nimport wandb\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\n%env WANDB_NOTEBOOK_NAME = $Fine_tune_Mistral_with_ORPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\n\nLoad the Model and Tokenizer for LoRA\nIn the following, we will load the Mistral 7B v0.3 model in 4-bit precision using bitsandbytes.\n\ncache_dir = './model'\nmodel_id = 'mistralai/Mistral-7B-v0.3'\n\n\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_id,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n\n\nLoading Checks\nAfter loading the model, it‚Äôs crucial to ensure that all parameters are correctly placed on the GPU and that none are overflowing onto the CPU. This can be particularly important for large models where memory management is critical.\nTo verify this, you can iterate through the model‚Äôs named parameters and check their device type. If any parameter is on the CPU (indicated by the device type ‚Äòmeta‚Äô), it will be printed out.\nHere is the code to perform this check:\n\n# Check there are no parameters overflowing onto cpu (meta).\nfor n, p in model.named_parameters():\n    if p.device.type=='meta':\n        print(f\"{n} is on meta!\")"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#prepare-for-lora-fine-tuning",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#prepare-for-lora-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Prepare for LoRA fine-tuning",
    "text": "Prepare for LoRA fine-tuning\nBefore starting the LoRA (Low-Rank Adaptation) fine-tuning process, it‚Äôs essential to understand which parameters in your model are trainable and which are not. This helps in ensuring that only the desired parameters are updated during training, which is crucial for efficient and effective fine-tuning.\nTo achieve this, you can use the following function to print the number of trainable parameters in the model and list which parameters are trainable and which are not.\nHere is the code to perform this check:\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainabl√∂e parameters in the model and lists which parameters\n    \"\"\"\n    trainable_params = 0\n    non_trainable_params = 0\n    all_params = 0\n\n    print(\"Trainable Parameters\")\n    for name, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n            print(f\" {name}\")\n        else:\n            non_trainable_params += param.numel()\n\n    print(\"\\nNon-Trainable Parameters:\")\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            print(f\" {name}\")\n\n    print(\n        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n        \n\nLet‚Äôs take a look a the model\n\nprint(model)"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-lora-fine-tuning",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-lora-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up LoRA Fine-Tuning",
    "text": "Setting Up LoRA Fine-Tuning\nTo prepare your model for LoRA (Low-Rank Adaptation) fine-tuning, you need to configure it properly. This involves setting up the LoRA configuration. Here‚Äôs a brief overview of the parameters and their best settings:\n\nr: This parameter controls the rank of the low-rank adaptation matrices. It‚Äôs suggested to choose a value greater than 0, with common choices being 8, 16, 32, 64, or 128. The best setting depends on the specific use case and computational resources, but a good starting point is 8 or 16.\nlora_alpha: This parameter scales the magnitude of the LoRA update. A higher value can lead to more significant changes in the model‚Äôs behavior. The best setting is typically 32, as used in the code.\ntarget_modules: This list specifies which modules in the model should be fine-tuned. The best settings include key modules like \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", and \"down_proj\". If the task involves chat fine-tuning, it‚Äôs also beneficial to set \"lm_head\" (language model head) as trainable.\nuse_gradient_checkpointing: This parameter activates gradient checkpointing to conserve memory. It is managed by Unsloth, which offloads input and output embeddings to disk, thereby saving VRAM.\nrandom_state: This parameter sets the seed for random number generation, ensuring reproducibility. The best setting is any integer value; in the code, it‚Äôs set to 3407.\nuse_rslora: This parameter activates RSLoRA, which adjusts the scaling factor of LoRA adapters to be proportional to 1/‚àör instead of 1/r. This adjustment enhances the stability of learning, particularly for higher adapter ranks, and improves fine-tuning performance as the rank increases.\n\nThese settings provide a good starting point for fine-tuning a language model using PEFT. However, the optimal settings may vary depending on the specific task and dataset, so some experimentation may be necessary.\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128\n    lora_alpha = 32,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n        \n    ],\n    \n    lora_dropout = 0, \n    bias = \"none\",    \n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = True,\n    \n)\n\n\nSet up Tokenizer and Padding\nBefore starting the fine-tuning process, it‚Äôs essential to configure the tokenizer and set up padding correctly. This ensures that the model can handle input sequences efficiently and that special tokens are properly managed.\nHere is a step-by-step guide to setting up the tokenizer and padding:\n\nInspect the Tokenizer: Print out the tokenizer details, including the vocabulary size, beginning-of-sequence (BOS) token, end-of-sequence (EOS) token, and chat template.\nOptionally Set the Chat Template Manually: If needed, you can manually set the chat template. This is useful for ensuring that the conversation starts correctly depending on the initial message role.\nApply the Chat Template: Use the chat template to format a list of messages.\nSet the Pad Token: Determine the appropriate pad token based on the tokenizer‚Äôs vocabulary and set it accordingly.\nUpdate the Model Configuration: Ensure that the model and its configuration are updated with the correct pad token ID.\n\nHere is the code to perform these steps:\n\nprint(tokenizer)\nprint(tokenizer.vocab_size)\n\n\nprint(tokenizer.bos_token)\nprint(tokenizer.eos_token)\n\n\nprint(tokenizer.chat_template)\n\nA custom chat template for a tokenizer, specifically designed for Llama/Mistral models is created. This template ensures that conversations start correctly by conditionally adding a beginning-of-sequence token (bos_token) if the first message is not from the assistant. This is particularly useful when formatting chosen and rejected responses separately, as it avoids adding an extra bos_token before the response.\nThe template is defined using a Jinja-like syntax, which iterates through the messages and formats them based on their roles (user or assistant). For user messages, it wraps the content with [INST] and [/INST] tags, while for assistant messages, it appends an end-of-sequence token (eos_token).\n\ntokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n\"\"\"\n\n# Test chat template\nmessages = [\n    {'role': 'user', 'content': 'write a quick sorf algorithm in python.'},\n    {'role': 'assistant', 'content': 'here you are.'},\n    {'role': 'user', 'content': 'great.'},\n]\n\ninputs = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(inputs)\n\n\n##¬†set the pad token to &lt;pad&gt;, if not &lt;|pad|&gt;, if not &lt;unk&gt; if &lt;unk&gt;\nif '&lt;pad&gt;' in tokenizer.get_vocab():\n    print('&lt;pad&gt; token is is in the tokenizer. Usinh &lt;pad&gt; for pad')\n    #Set the pad token\n    tokenizer.pad_token = '&lt;pad&gt;'\nelif '&lt;|pad|&gt;' in tokenizer.get_vocab():\n    print('&lt;|pad|&gt; token is in the tokenizer. Using for &lt;|pad|&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;|pad|&gt;'\nelif '&lt;unk&gt;' in tokenizer.get_vocab():\n    print('&lt;unk&gt; token is in the tokenizer. Using for &lt;unk&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;unk&gt;'\nelse:\n    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n    tokenizer.pad_token = tokenizer.eos_token\n\n\n# Update pad token id in model and its config\nmodel.pad_token_id = tokenizer.pad_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n#¬†Check if they are equal\nassert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n\n# Print the pad token ids\nprint('Tokenizer pad token ID:', tokenizer.pad_token_id)\nprint('Model pad token ID:', model.pad_token_id)\nprint('Model config pad token ID:', model.config.pad_token_id)\nprint('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n\n\nprint('Special tokens map:', tokenizer.special_tokens_map)\nprint('All special tokens:', tokenizer.all_special_tokens)\n\n\nprint(tokenizer)"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-chat-template-has-been-changed",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-chat-template-has-been-changed",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)",
    "text": "Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)\nWhen fine-tuning a model for chat applications, it‚Äôs often beneficial to set specific layers to be trainable, especially if you are changing the chat template. This ensures that the model can adapt to the new input format more effectively.\nHere is a step-by-step guide to setting specific layers to trainable:\n\nIdentify Trainable Parameters: Create a list of the names of the layers you want to set as trainable.\nSet Modules to Trainable: Iterate through the model‚Äôs parameters and set the requires_grad attribute to True for the specified layers. Optionally, set the rest to False.\nCreate a Dictionary of Trainable Parameters: Collect the trainable parameters into a dictionary for easy access.\nConvert to State Dict Format: Convert the trainable parameters to a state dictionary format, which can be useful for saving and loading the model‚Äôs state.\nPrint Trainable Parameters: Use a function to print the trainable parameters to verify the setup.\n\nHere is the code to perform these steps:\n\n# List to hold the names of the trainable parameters\ntrainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n\n# Set modules to be trainable\nfor n, p in model.named_parameters():\n    if any(k in n for k in trainable_params_names):\n        p.requires_grad_(True)\n    else:\n        p.requires_grad_(False) # Optional: Set the rest to be trainable\n\n# Make a dictionary of trainable parameters\ntrainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n\n# Convert trainable_params to state_dict format\ntrainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}\n\n\nprint_trainable_parameters(model)"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#loading-and-preparing-the-dataset-for-fine-tuning",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#loading-and-preparing-the-dataset-for-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Loading and Preparing the Dataset for Fine-Tuning",
    "text": "Loading and Preparing the Dataset for Fine-Tuning\nIn this code, we will guide you through the process of loading and preparing a dataset for fine-tuning a model. This involves loading the dataset, shuffling it, splitting it into training and test sets, and applying a specific template to format the data correctly.\nHere is a step-by-step guide to loading and preparing the dataset:\n\nImport Necessary Libraries: Import the required libraries, including json for handling JSON data and datasets for loading and manipulating the dataset.\nDefine Dataset Parameters: Set the dataset name and the maximum number of samples to use. If you want to use the full dataset, set max_num_samples to None.\nDefine the build_dataset Function: Create a function called build_dataset that takes a tokenizer, dataset name, cache directory, maximum number of samples, and other parameters as inputs. This function will load the dataset, shuffle it, split it into training and test sets, and apply a specific template to format the data.\nLoad the Dataset: Use the load_dataset function from the datasets library to load the dataset. The dataset is split based on the max_num_samples parameter.\nShuffle the Dataset: If max_num_samples is not None, shuffle the dataset to ensure randomness.\nSplit the Dataset: Determine the number of test samples and split the dataset into training and test sets using the train_test_split method.\nApply the DPO Template: Define a function called apply_dpo_template that formats the data according to the DPO (Direct Preference Optimization) template. This function extracts the necessary information from the dataset and applies the chat template using the tokenizer.\nMap the Dataset: Use the map method to apply the apply_dpo_template function to the dataset. Remove the original columns and rename the new columns accordingly.\nReturn the Dataset: Return the training and test datasets.\nCheck the Chat Template: Ensure that the chat template is correctly applied and that special tokens are not included when tokenizing the responses.\n\nHere is the code to perform these steps:\n\n#¬†Prepared with the help of code from: https://github.com/xfactlab/orpo/blob/main...\nimport json\n\n# Load the dataset\ndataset_name = 'llmat/dpo-orpo-mix-38k-balanced' #¬†Ensure this is defined\n\nmax_num_samples = None #¬†Set to None to use the full dataset\n#max_num_samples = 10000 #¬†set to None to use the full dataset\n\nfrom datasets import load_dataset\n\ndef build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_size_ratio=0.1):\n    # Determin the split specification based on max_num samples\n    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n\n    # Load the dataset\n    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n\n    # Shuffle the dataset\n    if max_num_samples is not None:\n        full_data = full_data.shuffle(seed=42)\n    else:\n        full_data = full_data\n\n    # Determine the number of test samples\n    num_total_samples = len(full_data)\n    test_size = int(test_size_ratio * num_total_samples)\n\n    # Randomly split the data into training and test sets\n    dataset = full_data.train_test_split(test_size=test_size)\n\n    column_names = list(dataset['train'].features)\n\n    def apply_dpo_template(example):\n        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n        if all(k in example.keys() for k in ('chosen', 'rejected')):\n            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n            # We therefore need to extract the N-1 turns to form the prompt\n            prompt_messages = example['chosen'][:-1]\n            example['messages'] = example['chosen']\n\n            # Now we extract the final turn to define chosen/rejected responses\n            chosen_messages = example['chosen'][-1:]\n            rejected_messages = example['rejected'][-1:]\n            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n        return example\n\n    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n                desc='Formatting comparisons with prompt template',)\n\n    for split in ['train', 'test']:\n        dataset[split] = dataset[split].rename_columns(\n            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected', 'messages': 'messages'}\n        )\n\n    return dataset['train'], dataset['test']\n\n# Assuming 'tokenizer' and 'dataset_name' are already defined\ntrain, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n\n# Check the chat template!!! &lt;s&gt; should not be included when tokenizing the respones\n\nAfter preparing and formatting your dataset for fine-tuning, it‚Äôs crucial to inspect the data to ensure that it has been correctly processed. This step helps you verify that the prompt, chosen, rejected, and messages fields are properly formatted and contain the expected information.\n\nprint('Prompt:', train['prompt'][0])\nprint('\\n\\nChosen:', train['chosen'][0])\nprint('\\n\\nRejected:', train['rejected'][0])\nprint('\\n\\nMessages (incl. prompt):', train['messages'][0])"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-and-running-training",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-and-running-training",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up and Running Training",
    "text": "Setting Up and Running Training\nIn this tutorial, we will go through the process of setting up and running the training for your model. This includes configuring training parameters, creating a custom logging callback, and initiating the training process.\nHere is a step-by-step guide to setting up and running the training:\n\nSet Training Parameters: Define the training parameters such as the model name, number of epochs, gradient accumulation steps, batch size, and the directory to save the results.\nCreate a Custom Logging Callback: Implement a custom callback to log training metrics to a file. This callback will write the training and evaluation loss to a log file and save the trainable parameters at checkpoint steps.\nInitialize the Logging Callback: Create an instance of the custom logging callback with the specified log file path.\n\nHere is the code to perform these steps:\n\nmodel_name = model_id.split('/')[-1]\n\nepochs=1\ngrad_accum=4\nbatch_size=8\nfine_tune_tag='ORPO'\nsave_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\nprint(save_dir)\n\n\nimport transformers\nimport os\nimport torch\n\n# Custom callback to log metrics\nclass LoggingCallback(transformers.TrainerCallback):\n    def __init__(self, log_file_path):\n        self.log_file_path = log_file_path\n\n    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n        with open(self.log_file_path, 'a') as f:\n            if 'loss' in logs:\n                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n                if 'eval_loss' in logs:\n                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n                f.flush()  # Force flush the buffered data to file\n\n        # Check if the current step is a checkpoint step\n        if state.global_step % int(args.save_steps) == 0:\n            # Check if the last checkpoint path exists\n            if state.best_model_checkpoint:\n                checkpoint_dir = state.best_model_checkpoint\n            else:\n                # If not, construct the checkpoint directory path\n                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n\n            # Ensure the checkpoint directory exists\n            os.makedirs(checkpoint_dir, exist_ok=True)\n\n            # Save trainable params in the checkpoint directory\n            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n            torch.save(current_trainable_params_state_dict, file_path)\n\n# Log file path\ncache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\nlog_file_path = os.path.join(cache_dir, 'training_logs.txt')\n\n# Create an instance of the custom callback\nlogging_callback = LoggingCallback(log_file_path)"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-orpo-training",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#setting-up-orpo-training",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up ORPO Training",
    "text": "Setting Up ORPO Training\nIn this section, we‚Äôll walk through setting up and training a model using the ORPOTrainer from the trl library.\nI trained the model on the entire dataset (38k samples) using an RTX 4090 GPU (24 GB of VRAM). The training took 7 hours and 35 minutes. You can use smaller GPUs with less VRAM and a smaller batch size. In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like ‚Äòmax_num_samples = 10000‚Äô to only load 10k samples.\n\nConfigure ORPO\nDefine the configuration for the ORPO training. This configuration includes various hyperparameters and settings for training.\n\nfrom trl import ORPOTrainer, ORPOConfig\nfrom unsloth import is_bfloat16_supported\n\norpo_config = ORPOConfig(\n    beta=0.2,\n    save_steps=500, \n    logging_steps=1,\n    num_train_epochs=epochs,\n    output_dir=save_dir,\n    evaluation_strategy='steps', \n    do_eval=True,\n    eval_steps=0.2,\n    per_device_eval_batch_size=batch_size,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=grad_accum,\n    log_level='debug',\n    optim='paged_adamw_8bit',\n    fp16 = not is_bfloat16_supported(),\n    bf16 = is_bfloat16_supported(),\n    max_grad_norm=0.3,\n    lr_scheduler_type='linear',\n    warmup_ratio=0.03,\n    learning_rate=1e-4, \n\n    max_prompt_length=512,\n    max_length=1024,\n\n    max_completion_length=1024,\n    remove_unused_columns=True,\n    \n)\n\n\n\nInitialize ORPOTrainer\nCreate an instance of ORPOTrainer with the model, datasets, tokenizer, and the configuration defined earlier.\n\norpo_trainer = ORPOTrainer(\n    model,\n    args=orpo_config,\n    train_dataset=train,\n    eval_dataset=test,\n    tokenizer=tokenizer,\n\n    callbacks=[logging_callback], # Add custom callback here\n)\n\n\n\nTrain the Model\nSet the model configuration to avoid cache warnings and start the training process.\n\nmodel.config.use_cache = False # silence the warnings\norpo_trainer.train()"
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#plotting-training-and-evaluation-losses-with-matplotlib",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#plotting-training-and-evaluation-losses-with-matplotlib",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Plotting Training and Evaluation Losses with Matplotlib",
    "text": "Plotting Training and Evaluation Losses with Matplotlib\nAfter training your model, it‚Äôs important to visualize the training and evaluation losses to understand how well your model is performing and to identify any potential issues. Visualizing the losses can help you diagnose problems such as overfitting or underfitting and make informed decisions about further training or model adjustments.\n\nimport matplotlib.pyplot as plt\n\n# Initialize lists to hold training and evaluation losses and steps\ntrain_losses = []\neval_losses = []\ntrain_steps = []\neval_steps = []\n\n# Populate the lists from the log history\nfor entry in orpo_trainer.state.log_history:\n    if 'loss' in entry:\n        train_losses.append(entry['loss'])\n        train_steps.append(entry['step'])\n    if 'eval_loss' in entry:\n        eval_losses.append(entry['eval_loss'])\n        eval_steps.append(entry['step'])\n\n# Plot the losses\nplt.plot(train_steps, train_losses, label='Train Loss')\nplt.plot(eval_steps, eval_losses, label='Eval Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\nLet‚Äôs now check the W&B plots. While the loss goes down, we also can see that the difference between the chosen and rejects answers becomes clearer."
  },
  {
    "objectID": "posts/ORPO_Mistral-unsloth-publish.html#merging-adapters-and-saving-the-model-to-hugging-face-hub",
    "href": "posts/ORPO_Mistral-unsloth-publish.html#merging-adapters-and-saving-the-model-to-hugging-face-hub",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Merging Adapters and Saving the Model to Hugging Face Hub",
    "text": "Merging Adapters and Saving the Model to Hugging Face Hub\nIn the subsequent steps, we merge the adapters with the original model using 16-bit precision to enhance quality. Initially, we save it locally in the ‚Äúmodel‚Äù directory before uploading it to the Hugging Face Hub. The trained model is available at llmat/Mistral-v0.3-7B-ORPO.\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"llmat/Mistral-v0.3-7B-ORPO\", tokenizer, save_method=\"merged_16bit\")"
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "",
    "text": "The field of artificial intelligence and machine learning is marked by constant innovation, with new tools and methodologies emerging to expand the horizons of what these technologies can achieve. Recently, significant upgrades were introduced in popular AI model series, enhancing their capabilities and setting new benchmarks in AI development.\nHowever, to fully leverage the potential of these advanced models, it‚Äôs essential to employ sophisticated fine-tuning techniques like ORPO (Odds Ratio Preference Optimization) and Unsloth. ORPO simplifies the alignment process by integrating preference optimization directly into the training phase, eliminating the need for a separate alignment step. Unsloth, on the other hand, offers groundbreaking advancements in training efficiency, significantly speeding up the process while reducing memory consumption without compromising accuracy.\nIn this article, we will explore how to fine-tune Mistral v0.3 using ORPO and Unsloth, demonstrating how these techniques can enhance model performance and efficiency. By understanding and applying these methods, you can unlock new levels of capability and efficiency in your AI projects. The code for this process can be found on Google Colab and in the LLM Tutorial on GitHub.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#orpo",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#orpo",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "ORPO",
    "text": "ORPO\nInstruction tuning and preference alignment are crucial for customizing Large Language Models (LLMs) for specific tasks. This typically involves a multi-step process: first, Supervised Fine-Tuning (SFT) on instructions to tailor the model to the desired domain, and second, applying preference alignment techniques such as Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to enhance the probability of producing preferred responses over less desirable ones. Researchers have found that although SFT adjusts the model to the target domain, it also raises the chances of producing both unwanted and desired answers. Therefore, the preference alignment stage is essential to enlarge the disparity between the probabilities of accepted and rejected outputs.\n\n\n\nHong and Lee (2024) introduced ORPO (Odds Ratio Preference Optimization), a groundbreaking method that aligns the language model without a reference model in a single-step manner by assigning a weak penalty to the rejected responses and a strong adaptation signal to the chosen responses with a simple log odds ratio term appended to the negative log-likelihood loss.\n\n\n\nThis approach enhances the traditional language modeling objective by integrating the negative log-likelihood (NLL) loss with an odds ratio (OR) component. The OR loss imposes a slight penalty on disfavored responses while significantly rewarding favored ones, enabling the model to concurrently master the target task and align with human preferences. The objective function for ORPO is defined as follows:\n\\mathscr{L}{ORPO} = \\mathbb{E}{(x, y_{w}, y_l)}[\\mathscr{L}{SFT} + \\lambda \\cdot \\mathscr{L}{OR}]\nIn this formula, SFT represents the conventional supervised fine-tuning loss, OR denotes the odds ratio loss, and Lambda is a weighting factor that balances these two components. This integration ensures that the model adapts effectively to the desired domain while minimizing the generation of undesired outputs.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#fine-tuning-mistral-v0.3-with-orpo-and-unsoth",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#fine-tuning-mistral-v0.3-with-orpo-and-unsoth",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Fine-Tuning Mistral v0.3 with ORPO and Unsoth",
    "text": "Fine-Tuning Mistral v0.3 with ORPO and Unsoth\nMistral AI‚Äôs v0.3 is a significant update to their AI model, introducing improved performance and efficiency. This version includes enhanced instruction-following capabilities, making interactions more intuitive. Additionally, Mistral v0.3 incorporates advanced reasoning skills, enabling it to tackle complex tasks more effectively. The update also extends the context length to 32768 tokens, allowing for more detailed and coherent conversations. Technical details include an extended vocabulary (32000 to 32768), a new tokenizer, and support for function calling.\nORPO necessitates a preference dataset that includes a prompt, a selected answer, and a discarded answer. To achieve this, we will utilize llmat/dpo-orpo-mix-38k-balanced, a dataset that merges high-quality DPO datasets and has been further balanced using a clustering-based approach.\nTo efficiently fine-tune our model we will use the unlsoth library. Unsloth significantly improves speed and efficiency in the training of Large Language Models (LLMs). The speed and efficiency gains are achieved through several optimizations, including manual autograd and chained matrix multiplication. Furthermore, it utilizes Flash Attention via xformers and Tri Dao‚Äôs implementation, which is a highly optimized approach to handling attention mechanisms in transformer models. Unsloth makes fine-tuning 2 times faster with 50% less memory usage.\nLet‚Äôs start by installing the required libraries:\n\n!pip install python-dotenv\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers&lt;0.0.27\" \"trl&lt;0.9.0\" peft accelerate bitsandbytes\n\nNow let‚Äôs login to our W&B workspace\n\nimport wandb\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\n%env WANDB_NOTEBOOK_NAME = $Fine_tune_Mistral_with_ORPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\n\nLoad the Model and Tokenizer for LoRA\nIn the following, we will load the Mistral 7B v0.3 model in 4-bit precision using bitsandbytes.\n\ncache_dir = './model'\nmodel_id = 'mistralai/Mistral-7B-v0.3'\n\n\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_id,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n\n\nLoading Checks\nAfter loading the model, it‚Äôs crucial to ensure that all parameters are correctly placed on the GPU and that none are overflowing onto the CPU. This can be particularly important for large models where memory management is critical.\nTo verify this, you can iterate through the model‚Äôs named parameters and check their device type. If any parameter is on the CPU (indicated by the device type ‚Äòmeta‚Äô), it will be printed out.\nHere is the code to perform this check:\n\n# Check there are no parameters overflowing onto cpu (meta).\nfor n, p in model.named_parameters():\n    if p.device.type=='meta':\n        print(f\"{n} is on meta!\")",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#prepare-for-lora-fine-tuning",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#prepare-for-lora-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Prepare for LoRA fine-tuning",
    "text": "Prepare for LoRA fine-tuning\nBefore starting the LoRA (Low-Rank Adaptation) fine-tuning process, it‚Äôs essential to understand which parameters in your model are trainable and which are not. This helps in ensuring that only the desired parameters are updated during training, which is crucial for efficient and effective fine-tuning.\nTo achieve this, you can use the following function to print the number of trainable parameters in the model and list which parameters are trainable and which are not.\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model and lists which parameters\n    \"\"\"\n    trainable_params = 0\n    non_trainable_params = 0\n    all_params = 0\n\n    print(\"Trainable Parameters\")\n    for name, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n            print(f\" {name}\")\n        else:\n            non_trainable_params += param.numel()\n\n    print(\"\\nNon-Trainable Parameters:\")\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            print(f\" {name}\")\n\n    print(\n        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n\nPrint the trainable parameters to verify the setup.\n\nprint_trainable_parameters(model)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-lora-fine-tuning",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-lora-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up LoRA Fine-Tuning",
    "text": "Setting Up LoRA Fine-Tuning\nTo prepare your model for LoRA (Low-Rank Adaptation) fine-tuning, you need to configure it properly. This involves setting up the LoRA configuration. Here‚Äôs a brief overview of the parameter settings:\n\nr: This parameter controls the rank of the low-rank adaptation matrices. It‚Äôs suggested to choose a value greater than 0, with common choices being 8, 16, 32, 64, or 128. The best setting depends on the specific use case and computational resources, but a good starting point is 8 or 16.\nlora_alpha: This parameter scales the magnitude of the LoRA update. A higher value can lead to more significant changes in the model‚Äôs behavior. In our example we are setting lora_alpha to 32.\ntarget_modules: This list specifies which modules in the model should be fine-tuned. The settings include key modules like \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", and \"down_proj\". If the task involves chat fine-tuning, it‚Äôs also beneficial to set \"lm_head\" (language model head) as trainable.\nuse_gradient_checkpointing: This parameter activates gradient checkpointing to conserve memory. It is managed by Unsloth, which offloads input and output embeddings to disk, thereby saving VRAM.\nrandom_state: This parameter sets the seed for random number generation, ensuring reproducibility. The best setting is any integer value; in the code, it‚Äôs set to 3407.\nuse_rslora: This parameter activates RSLoRA, which adjusts the scaling factor of LoRA adapters to be proportional to 1/‚àör instead of 1/r. This adjustment enhances the stability of learning, particularly for higher adapter ranks, and improves fine-tuning performance as the rank increases.\n\nThese settings provide a good starting point for fine-tuning a language model using PEFT. However, the optimal settings may vary depending on the specific task and dataset, so some experimentation may be necessary.\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number &gt; 0 ! Suggested 8, 16, 32, 64, 128\n    lora_alpha = 32,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n        \n    ],\n    \n    lora_dropout = 0, \n    bias = \"none\",    \n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = True,\n    \n)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-chat-template-has-been-changed",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#set-embed-and-norm-layers-to-trainable-recommended-for-chat-fine-tuning-if-chat-template-has-been-changed",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)",
    "text": "Set embed and norm layers to trainable (recommended for chat fine-tuning if chat template has been changed)\nWhen fine-tuning a model for chat applications, it‚Äôs often beneficial to set specific layers to be trainable, especially if you are changing the chat template. This ensures that the model can adapt to the new input format more effectively.\nHere is a step-by-step guide to setting specific layers to trainable:\n\nIdentify Trainable Parameters: Create a list of the names of the layers you want to set as trainable.\nSet Modules to Trainable: Iterate through the model‚Äôs parameters and set the requires_grad attribute to True for the specified layers. Optionally, set the rest to False.\nCreate a Dictionary of Trainable Parameters: Collect the trainable parameters into a dictionary for easy access.\nConvert to State Dict Format: Convert the trainable parameters to a state dictionary format, which can be useful for saving and loading the model‚Äôs state.\nPrint Trainable Parameters: Use a function to print the trainable parameters to verify the setup.\n\nHere is the code to perform these steps:\n\n# List to hold the names of the trainable parameters\ntrainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n\n# Set modules to be trainable\nfor n, p in model.named_parameters():\n    if any(k in n for k in trainable_params_names):\n        p.requires_grad_(True)\n    else:\n        p.requires_grad_(False) # Optional: Set the rest to be trainable\n\n# Make a dictionary of trainable parameters\ntrainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n\n# Convert trainable_params to state_dict format\ntrainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}\n\n\nprint_trainable_parameters(model)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#loading-and-preparing-the-dataset-for-fine-tuning",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#loading-and-preparing-the-dataset-for-fine-tuning",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Loading and Preparing the Dataset for Fine-Tuning",
    "text": "Loading and Preparing the Dataset for Fine-Tuning\nWhen working with large datasets, it‚Äôs essential to streamline the process of loading, splitting, and formatting the data to ensure efficient model training and testing. The following Python code demonstrates how to achieve this using the Hugging Face datasets library, along with a tokenizer for text processing.\n\n#¬†Prepared with the help of code from: https://github.com/xfactlab/orpo/.\nimport json\n\n# Load the dataset\ndataset_name = 'llmat/dpo-orpo-mix-38k-balanced' #¬†Ensure this is defined\n\nmax_num_samples = None #¬†Set to None to use the full dataset\n#max_num_samples = 10000 #¬†set to None to use the full dataset\n\nfrom datasets import load_dataset\n\ndef build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_size_ratio=0.1):\n    # Determin the split specification based on max_num samples\n    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n\n    # Load the dataset\n    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n\n    # Shuffle the dataset\n    if max_num_samples is not None:\n        full_data = full_data.shuffle(seed=42)\n    else:\n        full_data = full_data\n\n    # Determine the number of test samples\n    num_total_samples = len(full_data)\n    test_size = int(test_size_ratio * num_total_samples)\n\n    # Randomly split the data into training and test sets\n    dataset = full_data.train_test_split(test_size=test_size)\n\n    column_names = list(dataset['train'].features)\n\n    def apply_dpo_template(example):\n        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n        if all(k in example.keys() for k in ('chosen', 'rejected')):\n            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n            # We therefore need to extract the N-1 turns to form the prompt\n            prompt_messages = example['chosen'][:-1]\n\n            # Now we extract the final turn to define chosen/rejected responses\n            chosen_messages = example['chosen'][-1:]\n            rejected_messages = example['rejected'][-1:]\n            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n        return example\n\n    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n                desc='Formatting comparisons with prompt template',)\n\n    for split in ['train', 'test']:\n        dataset[split] = dataset[split].rename_columns(\n            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected'}\n        )\n\n    return dataset['train'], dataset['test']\n\n# Assuming 'tokenizer' and 'dataset_name' are already defined\ntrain, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n\nAfter preparing and formatting your dataset for fine-tuning, let‚Äôs inspect the data to ensure that it has been correctly processed. This step helps you verify that the prompt, chosen, rejected, and messages fields are properly formatted and contain the expected information.\n\nprint('Prompt:', train['prompt'][0])\nprint('\\n\\nChosen:', train['chosen'][0])\nprint('\\n\\nRejected:', train['rejected'][0])\nprint('\\n\\nMessages (incl. prompt):', train['messages'][0])",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-and-running-training",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-and-running-training",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up and Running Training",
    "text": "Setting Up and Running Training\nIn this tutorial, we will go through the process of setting up and running the training for your model. This includes configuring training parameters, creating a custom logging callback, and initiating the training process.\n\nSet Training Parameters\nDefine the training parameters such as the model name, number of epochs, gradient accumulation steps, batch size, and the directory to save the results.\n\nmodel_name = model_id.split('/')[-1]\n\nepochs=1\ngrad_accum=4\nbatch_size=8\nfine_tune_tag='ORPO'\nsave_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\nprint(save_dir)\n\n\n\nCreate a Custom Logging Callback\nImplement a custom callback to log training metrics to a file. This callback will write the training and evaluation loss to a log file and save the trainable parameters at checkpoint steps. Create an instance of the custom logging callback with the specified log file path.\n\nimport transformers\nimport os\nimport torch\n\n# Custom callback to log metrics\nclass LoggingCallback(transformers.TrainerCallback):\n    def __init__(self, log_file_path):\n        self.log_file_path = log_file_path\n\n    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n        with open(self.log_file_path, 'a') as f:\n            if 'loss' in logs:\n                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n                if 'eval_loss' in logs:\n                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n                f.flush()  # Force flush the buffered data to file\n\n        # Check if the current step is a checkpoint step\n        if state.global_step % int(args.save_steps) == 0:\n            # Check if the last checkpoint path exists\n            if state.best_model_checkpoint:\n                checkpoint_dir = state.best_model_checkpoint\n            else:\n                # If not, construct the checkpoint directory path\n                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n\n            # Ensure the checkpoint directory exists\n            os.makedirs(checkpoint_dir, exist_ok=True)\n\n            # Save trainable params in the checkpoint directory\n            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n            torch.save(current_trainable_params_state_dict, file_path)\n\n# Log file path\ncache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\nlog_file_path = os.path.join(cache_dir, 'training_logs.txt')\n\n# Create an instance of the custom callback\nlogging_callback = LoggingCallback(log_file_path)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-orpo-training",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#setting-up-orpo-training",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Setting Up ORPO Training",
    "text": "Setting Up ORPO Training\nIn this section, we‚Äôll walk through setting up and training a model using the ORPOTrainer from the trl library.\nI trained the model on the entire dataset (38k samples) using an RTX 4090 GPU (24 GB of VRAM). The training took 7 hours and 35 minutes. You can use smaller GPUs with less VRAM and a smaller batch size. In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like ‚Äòmax_num_samples = 10000‚Äô to only load 10k samples.\n\nConfigure ORPO\nWe define the configuration for the ORPO training. This configuration includes various hyperparameters and settings for training. An important parameter to set is beta. beta is the constant Œª of the loss function in the paper. It controls how much weight we give to the preference part vs.¬†the cross-entropy part. In our example, we set the value to 0.2.\n\nfrom trl import ORPOTrainer, ORPOConfig\nfrom unsloth import is_bfloat16_supported\n\norpo_config = ORPOConfig(\n    beta=0.2,\n    save_steps=500, \n    logging_steps=1,\n    num_train_epochs=epochs,\n    output_dir=save_dir,\n    evaluation_strategy='steps', \n    do_eval=True,\n    eval_steps=0.2,\n    per_device_eval_batch_size=batch_size,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=grad_accum,\n    log_level='debug',\n    optim='paged_adamw_8bit',\n    fp16 = not is_bfloat16_supported(),\n    bf16 = is_bfloat16_supported(),\n    max_grad_norm=0.3,\n    lr_scheduler_type='linear',\n    warmup_ratio=0.03,\n    learning_rate=1e-4, \n\n    max_prompt_length=512,\n    max_length=1024,\n\n    max_completion_length=1024,\n    remove_unused_columns=True,\n    \n)\n\n\n\nInitialize ORPOTrainer\nCreate an instance of ORPOTrainer with the model, datasets, tokenizer, and the configuration defined earlier.\n\norpo_trainer = ORPOTrainer(\n    model,\n    args=orpo_config,\n    train_dataset=train,\n    eval_dataset=test,\n    tokenizer=tokenizer,\n\n    callbacks=[logging_callback], # Add custom callback here\n)\n\n\n\nTrain the Model\nSet the model configuration to avoid cache warnings and start the training process.\n\nmodel.config.use_cache = False # silence the warnings\norpo_trainer.train()",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#plotting-training-and-evaluation-losses-with-matplotlib",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#plotting-training-and-evaluation-losses-with-matplotlib",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Plotting Training and Evaluation Losses with Matplotlib",
    "text": "Plotting Training and Evaluation Losses with Matplotlib\nAfter training your model, it‚Äôs important to visualize the training and evaluation losses to understand how well your model is performing and to identify any potential issues. Visualizing the losses can help you diagnose problems such as overfitting or underfitting and make informed decisions about further training or model adjustments.\n\nimport matplotlib.pyplot as plt\n\n# Initialize lists to hold training and evaluation losses and steps\ntrain_losses = []\neval_losses = []\ntrain_steps = []\neval_steps = []\n\n# Populate the lists from the log history\nfor entry in orpo_trainer.state.log_history:\n    if 'loss' in entry:\n        train_losses.append(entry['loss'])\n        train_steps.append(entry['step'])\n    if 'eval_loss' in entry:\n        eval_losses.append(entry['eval_loss'])\n        eval_steps.append(entry['step'])\n\n# Plot the losses\nplt.plot(train_steps, train_losses, label='Train Loss')\nplt.plot(eval_steps, eval_losses, label='Eval Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\nLet‚Äôs now check the W&B plots. While the loss goes down, we also can see that the difference between the chosen and rejects answers becomes clearer.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **Large Language Models**",
      "1. Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#merging-adapters-and-saving-the-model-to-hugging-face-hub",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#merging-adapters-and-saving-the-model-to-hugging-face-hub",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Merging Adapters and Saving the Model to Hugging Face Hub",
    "text": "Merging Adapters and Saving the Model to Hugging Face Hub\nAs a last step, we merge the adapters with the original model using 16-bit precision to enhance quality. Initially, we save it locally in the ‚Äúmodel‚Äù directory before uploading it to the Hugging Face Hub. The trained model is available at llmat/Mistral-v0.3-7B-ORPO.\n\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"llmat/Mistral-v0.3-7B-ORPO\", tokenizer, save_method=\"merged_16bit\")",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#conclusion",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#conclusion",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Conclusion",
    "text": "Conclusion\nThis article presented a thorough overview of ORPO fine-tuning and its practical application to a Mistral v0.3 7B model. Utilizing QLoRA‚Äôs efficient memory management, we successfully fine-tuned a 7B LLM on a high-quality dataset with minimal GPU resources.\nI hope you found this guide helpful. If you liked this article, follow me on Hugging Face @llmat. Best of luck with your model fine-tuning!",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#fine-tuning-mistral-v0.3-with-orpo-and-unsloth",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#fine-tuning-mistral-v0.3-with-orpo-and-unsloth",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Fine-Tuning Mistral v0.3 with ORPO and Unsloth",
    "text": "Fine-Tuning Mistral v0.3 with ORPO and Unsloth\nIn this example we will QLoRA fine-tune the Mistral v0.3 7B model using ORPO and the Unsloth framework. ORPO necessitates a preference dataset that includes a prompt, a selected answer, and a discarded answer. To achieve this, we will utilize llmat/dpo-orpo-mix-38k-balanced, a dataset that merges high-quality DPO datasets and has been further balanced using a clustering-based approach.\nLet‚Äôs start by installing the required libraries:\n\n!pip install python-dotenv\n# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n!pip install \"unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers&lt;0.0.27\" \"trl&lt;0.9.0\" peft accelerate bitsandbytes\n\nNow let‚Äôs login to our W&B workspace\n\nimport wandb\nimport os\nimport dotenv\n\ndotenv.load_dotenv()\n%env WANDB_NOTEBOOK_NAME = $Fine_tune_Mistral_with_ORPO\nwandb.login(key=os.environ[\"WANDB_API_KEY\"])\n\n\nLoad the Model and Tokenizer for LoRA\nIn the following, we will load the Mistral 7B v0.3 model in 4-bit precision.\n\ncache_dir = './model'\nmodel_id = 'mistralai/Mistral-7B-v0.3'\n\n\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048 \ndtype = None \nload_in_4bit = True \n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_id,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n\n\nLoading Checks\nAfter loading the model, it‚Äôs crucial to ensure that all parameters are correctly placed on the GPU and that none are overflowing onto the CPU. This can be particularly important for large models where memory management is critical.\nTo verify the placement of the model‚Äôs parameters, you can iterate through the model‚Äôs named parameters and check their device type. If any parameter is on the CPU (indicated by the device type ‚Äòmeta‚Äô), it will be printed out. This ensures that your model is fully utilizing the GPU resources and avoiding any potential performance bottlenecks.\nHere is the code to perform this check:\n\n# Check there are no parameters overflowing onto cpu (meta).\nfor n, p in model.named_parameters():\n    if p.device.type=='meta':\n        print(f\"{n} is on meta!\")",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#unsloth",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#unsloth",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Unsloth",
    "text": "Unsloth\nUnsloth is a fine-tuning framework designed to accelerate the training of large language models (LLMs) like Llama and Mistral, while drastically reducing memory usage. It achieves this through several optimizations:\n\nManual Derivation and Handwritten GPU Kernels: Unsloth optimizes computational steps by manually deriving and handwriting GPU kernels, bypassing inefficiencies in general-purpose libraries.\nQuantization Techniques: Utilizing 4-bit and 16-bit quantization (QLoRA) reduces memory requirements without compromising model accuracy.\nOptimized Attention Mechanisms: Integrating Flash Attention v2 for faster attention calculations and reduced memory usage.\nEnhanced Memory Management: Efficient memory allocation and data transfer processes optimize VRAM usage.\n\nUnsloth can make training up to 2 times faster on single GPUs and reduces memory usage by up to 60% without degrading accuracy, supporting diverse fine-tuning use cases, including instructional fine-tuning and direct preference optimization (DPO)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#set-embed-and-norm-layers-to-trainable",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#set-embed-and-norm-layers-to-trainable",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Set embed and norm layers to trainable",
    "text": "Set embed and norm layers to trainable\n\n(recommended for chat fine-tuning if the chat template has been changed)\nWhen fine-tuning a model for chat applications, it‚Äôs often beneficial to set specific layers to be trainable, especially if you are changing the chat template. This ensures that the model can adapt to the new input format more effectively.\n\n# List to hold the names of the trainable parameters\ntrainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n\n# Set modules to be trainable\nfor n, p in model.named_parameters():\n    if any(k in n for k in trainable_params_names):\n        p.requires_grad_(True)\n    else:\n        p.requires_grad_(False) # Optional: Set the rest to be trainable\n\n# Make a dictionary of trainable parameters\ntrainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n\n# Convert trainable_params to state_dict format\ntrainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#plotting-training-and-evaluation-losses",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#plotting-training-and-evaluation-losses",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Plotting Training and Evaluation Losses",
    "text": "Plotting Training and Evaluation Losses\nAfter training your model, it‚Äôs important to visualize the training and evaluation losses to understand how well your model is performing and to identify any potential issues. Visualizing the losses can help you diagnose problems such as overfitting or underfitting and make informed decisions about further training or model adjustments.\n\nimport matplotlib.pyplot as plt\n\n# Initialize lists to hold training and evaluation losses and steps\ntrain_losses = []\neval_losses = []\ntrain_steps = []\neval_steps = []\n\n# Populate the lists from the log history\nfor entry in orpo_trainer.state.log_history:\n    if 'loss' in entry:\n        train_losses.append(entry['loss'])\n        train_steps.append(entry['step'])\n    if 'eval_loss' in entry:\n        eval_losses.append(entry['eval_loss'])\n        eval_steps.append(entry['step'])\n\n# Plot the losses\nplt.plot(train_steps, train_losses, label='Train Loss')\nplt.plot(eval_steps, eval_losses, label='Eval Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n\n\n\nLet‚Äôs now check the W&B plots. While the loss goes down, we also can see that the difference between the chosen and rejects answers becomes clearer.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#set-up-tokenizer-and-padding",
    "href": "posts/Fine-Tune_Mistral_7B_with_ORPO.html#set-up-tokenizer-and-padding",
    "title": "Fine-Tune Mistral v0.3 with ORPO and Unsloth",
    "section": "Set up Tokenizer and Padding",
    "text": "Set up Tokenizer and Padding\nBefore starting the fine-tuning process, it‚Äôs essential to configure the tokenizer and set up padding correctly. This ensures that the model can handle input sequences efficiently and that special tokens are properly managed.\n\nInspect the Tokenizer\nPrint out the tokenizer details, including the vocabulary size, beginning-of-sequence (BOS) token, end-of-sequence (EOS) token, and chat template.\n\nprint(tokenizer)\nprint(tokenizer.vocab_size)\n\n\nprint(tokenizer.bos_token)\nprint(tokenizer.eos_token)\n\n\nprint(tokenizer.chat_template)\n\n\n\nCustomize Chat Template\nWhen working with Llama/Mistral models, it‚Äôs sometimes necessary to customize the chat template to ensure the conversation is formatted correctly. This customization is particularly useful when handling cases where the initial message in the conversation might not be from the assistant. By ensuring the beginning-of-sequence token (bos_token) is correctly placed, we can maintain the proper structure and flow of the conversation.\nThe following code snippet demonstrates how to set the chat template manually for such scenarios. This template checks if the first message is from the assistant. If not, it adds the bos_token at the beginning. This step is crucial because we format the chosen and rejected responses separately, and we want to avoid adding an extra bos_token before the response when there‚Äôs no initial user message.\nThe template is defined using a Jinja-like syntax, which iterates through the messages and formats them based on their roles (user or assistant). For user messages, it wraps the content with [INST] and [/INST] tags, while for assistant messages, it appends an end-of-sequence token (eos_token).\n\ntokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n\"\"\"\n\n# Test chat template\nmessages = [\n    {'role': 'user', 'content': 'write a quick sorf algorithm in python.'},\n    {'role': 'assistant', 'content': 'here you are.'},\n    {'role': 'user', 'content': 'great.'},\n]\n\ninputs = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(inputs)\n\n\n\nSet the Pad Token\nWhen working with tokenizers, it‚Äôs essential to designate a token for padding sequences to ensure they all have the same length. This padding token helps maintain the consistency of input shapes when batching data for training models. The following code snippet demonstrates how to set the padding token (pad_token) in your tokenizer by checking for the presence of specific tokens in its vocabulary.\n\n##¬†set the pad token to &lt;pad&gt;, if not &lt;|pad|&gt;, if not &lt;unk&gt; if &lt;unk&gt;\nif '&lt;pad&gt;' in tokenizer.get_vocab():\n    print('&lt;pad&gt; token is is in the tokenizer. Usinh &lt;pad&gt; for pad')\n    #Set the pad token\n    tokenizer.pad_token = '&lt;pad&gt;'\nelif '&lt;|pad|&gt;' in tokenizer.get_vocab():\n    print('&lt;|pad|&gt; token is in the tokenizer. Using for &lt;|pad|&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;|pad|&gt;'\nelif '&lt;unk&gt;' in tokenizer.get_vocab():\n    print('&lt;unk&gt; token is in the tokenizer. Using for &lt;unk&gt; for pad')\n    # Set the pad token\n    tokenizer.pad_token = '&lt;unk&gt;'\nelse:\n    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n    tokenizer.pad_token = tokenizer.eos_token\n\n\n\nUpdate the Model Configuration\nThe following code snippet demonstrates how to update the pad token ID in both the model and its configuration to match the tokenizer‚Äôs pad token ID. Additionally, it includes checks and print statements to verify the consistency of these IDs and provides information about the tokenizer‚Äôs special tokens.\n\n# Update pad token id in model and its config\nmodel.pad_token_id = tokenizer.pad_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n#¬†Check if they are equal\nassert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n\n# Print the pad token ids\nprint('Tokenizer pad token ID:', tokenizer.pad_token_id)\nprint('Model pad token ID:', model.pad_token_id)\nprint('Model config pad token ID:', model.config.pad_token_id)\nprint('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n\n\nprint('Special tokens map:', tokenizer.special_tokens_map)\nprint('All special tokens:', tokenizer.all_special_tokens)\n\n\nprint(tokenizer)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Fine-Tune Mistral with ORPO"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html",
    "href": "posts/From_Fine-Tuning_to_Deployment.html",
    "title": "From Fine-Tuning to Deployment",
    "section": "",
    "text": "Imagine unlocking the full potential of large language models (LLMs) right on your local machine, without relying on costly cloud services. This is where Ollama shines by allowing users to harness the power of LLMs on their machines. While Ollama offers a range of ready-to-use models, there are times when a custom model is necessary, whether it‚Äôs fine-tuned on specific data or designed for a particular task. Efficiently deploying these custom models on local hardware often requires optimization techniques like quantization. In this article, we will explore the concept of quantization and demonstrate how to apply it to a fine-tuned model from Huggingface. We‚Äôll then cover how to install Ollama, create a corresponding Modelfile for a custom model, and integrate this custom model into Ollama, proving how easy it is to bring AI capabilities in-house. All the code used in this article is available on Google Colab and in the LLM Tutorial.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#ollama",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#ollama",
    "title": "From Fine-Tuning to Deployment",
    "section": "Ollama",
    "text": "Ollama\nOllama is an open-source platform that empowers users to run large language models (LLMs) locally, bypassing the need for cloud-based services. Designed with accessibility in mind, Ollama simplifies the installation and management of a wide range of pre-trained LLMs and embedding models, enabling easy deployment without requiring extensive technical expertise. The platform provides a local API for seamless application integration and supports frameworks like LangChain. Recently tool-calling functionality has been introduced. This feature allows models to interact with external tools‚Ää-‚Äälike APIs, web browsers, and code interpreters‚Ää-‚Ääenabling them to perform complex tasks and interact with the outside world more effectively. Thanks to a large open-source community, Ollama continues evolving its capabilities, making it a robust, cost-effective solution for local AI deployment.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#quantization-in-large-language-models",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#quantization-in-large-language-models",
    "title": "From Fine-Tuning to Deployment",
    "section": "Quantization in Large Language¬†Models",
    "text": "Quantization in Large Language¬†Models\nQuantization is a crucial technique in machine learning that involves reducing the precision of a model‚Äôs weights and activations, without significantly impacting the model‚Äôs performance. Traditionally, these models operate using 32-bit floating point (FP32) formats, but quantization allows for the conversion of these weights to lower precision formats such as 16-bit (FP16), 8-bit (INT8), 4-bit, or even 2-bit. The primary goals of quantization are to reduce the model‚Äôs memory footprint and computational demands, thereby making it possible to deploy the model on resource-constrained hardware. There are two types of quantization techniques: post-training quantization and quantization-aware training.\n\nTypes of Quantization\n‚Ä¢ Post-Training Quantization (PTQ): PTQ is a straightforward technique in which the model is quantized after it has been fully trained. This method is quick to implement and does not require retraining the model, making it ideal for scenarios where time or resources are limited. However, it may result in a slight decrease in accuracy since the model was not trained with quantization in mind. ‚Ä¢ Quantization-Aware Training (QAT): QAT integrates quantization into the training process, allowing the model to learn to compensate for the reduced precision. This approach generally results in better performance compared to PTQ, as the model adapts to the quantized environment during training. However, QAT requires more computational resources during training and is more complex to implement.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#quantizing-a-custom-model",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#quantizing-a-custom-model",
    "title": "From Fine-Tuning to Deployment",
    "section": "Quantizing a Custom¬†Model",
    "text": "Quantizing a Custom¬†Model\nIn our example, we will use the GGUF (GPT-Generated Unified Format) quantization format, released by Georgi Gerganov and the llama.cpp team. GGUF employs the post-training quantization technique and supports a range of quantization methods, allowing developers to balance model accuracy and efficiency based on their specific needs. This format is particularly favored by the community for its ability to run efficiently on both CPU and Apple devices, making it an excellent choice for local testing and deployment.\n\nInstalling the llama.cpp library\nTo start quantizing our model, we need to install the llama.cpp library. The library includes utilities to convert models into GGUF format and tools to quantize these models into various bit-widths depending on the hardware constraints.\n\n!git clone https://github.com/ggerganov/llama.cpp\n!pip install -r llama.cpp/requirements.txt -q\n!cd llama.cpp && make -j 8",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#downloading-and-preparing-the-model",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#downloading-and-preparing-the-model",
    "title": "From Fine-Tuning to Deployment",
    "section": "Downloading and Preparing the¬†Model",
    "text": "Downloading and Preparing the¬†Model\nOnce we have the necessary tools, the next step is to download the model we want to quantize from Huggingface. In this example, we are using the Mistral-v0.3‚Äì7B-ORPO model that we fine-tuned in the last article. We download the model, rename it locally, and move it to the¬†./model folder.\n\n!git lfs install\n!git clone https://huggingface.co/llmat/Mistral-v0.3-7B-ORPO Mistral-v0.3-7B-ORPO\n!mv Mistral-v0.3-7B-ORPO model/\n\nOnce we have our model we need to convert it to the GGUF F16 format.\n\n!python ./llama.cpp/convert_hf_to_gguf.py ./model --outfile ./model/Mistral-v0.3-7B-ORPO-f16.gguf --outtype f16\n\nNow, we can choose the method by which we want our model to be quantized. In the context of llama.cpp, quantization methods are typically named following a specific convention: **Q#_K_M Let‚Äôs break down what each component means:  ‚Ä¢ Q:** Stands for ‚ÄúQuantization,‚Äù indicating that the model has undergone a process to reduce its numerical precision.  ‚Ä¢ #: Refers to the number of bits used in the quantization process. For example, 4 in Q4_K_M indicates that the model has been quantized using 4-bit integers.  ‚Ä¢ K: Denotes the use of k-means clustering in the quantization process. K-means clustering is a technique used to group similar weights, reducing the variation between them and allowing for more efficient quantization with minimal loss of accuracy. ‚Ä¢ M: Indicates the size category of the model after quantization, where: ‚Ä¢ S = Small ‚Ä¢ M = Medium ‚Ä¢ L = Large",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#quantization-methods-explained",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#quantization-methods-explained",
    "title": "From Fine-Tuning to Deployment",
    "section": "Quantization Methods Explained",
    "text": "Quantization Methods Explained\nHere‚Äôs a closer look at the different quantization methods supported by llama.cpp and Ollama, following the Q#_K_M naming convention: Q2_K: This method uses 2-bit quantization, offering the most significant size reduction but with a considerable loss in accuracy. It‚Äôs mainly used in highly constrained environments where memory and processing power are extremely limited. Q3_K_S: A 3-bit quantization method using k-means clustering, optimized for small models. This method provides significant memory savings and is used when accuracy can be somewhat compromised. Q3_K_M: Similar to Q3_K_S but optimized for medium-sized models. This method offers a balanced trade-off between memory usage and accuracy. Q3_K_L: This method is tailored for larger models, using 3-bit quantization with k-means clustering to reduce size while maintaining as much accuracy as possible. Q4_0: A standard 4-bit quantization method that does not use k-means clustering. This is the default method, offering a good balance between size reduction and maintaining model accuracy. It‚Äôs suitable for general use cases where memory is limited but accuracy is still important. Q4_1: Similar to Q4_0 but with slight variations in how quantization is applied, potentially offering slightly better accuracy at the cost of a small increase in resource usage. Q4_K_S: A variation of 4-bit quantization optimized for smaller models. It reduces the model size significantly while preserving reasonable accuracy. Q4_K_M: This method applies 4-bit quantization with k-means clustering to medium-sized models, offering an excellent balance between size and accuracy. It‚Äôs one of the most recommended methods for general use. Q5_0: Uses 5-bit quantization, which offers higher precision than 4-bit methods, resulting in better accuracy. This method is a good choice when you have slightly more memory available and need to maintain higher accuracy. Q5_1: A refinement of Q5_0, providing even greater accuracy by applying more sophisticated quantization techniques, though at the cost of increased resource requirements. Q5_K_S: This method uses 5-bit quantization with k-means clustering, optimized for smaller models, providing higher accuracy than 4-bit methods with only a slight increase in resource use. Q5_K_M: An advanced 5-bit quantization technique optimized for medium-sized models, providing high accuracy with reasonable memory efficiency. This method is often recommended for scenarios where accuracy is critical but resources are still somewhat limited. Q6_K: This method uses 6-bit quantization, providing a middle ground between 4-bit and 8-bit methods. It‚Äôs suitable when you need more accuracy than what 4-bit offers but can‚Äôt afford the higher resource demands of 8-bit quantization. Q8_0: Uses 8-bit quantization, which is nearly as accurate as the original float16 model. This method is best for scenarios where you need to preserve as much accuracy as possible while still reducing the model size. In our example we choose to quantize our model to 4-bit, using the Q4_K_M method.\n\n!mkdir Mistral-v0.3-7B-ORPO_Q4_K_M\n!./llama.cpp/llama-quantize ./model/Mistral-v0.3-7B-ORPO-f16.gguf ./Mistral-v0.3-7B-ORPO_Q4_K_M/Mistral-v0.3-7B-ORPO_Q4_K_M.gguf Q4_K_M",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#push-model-to-hub-optional",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#push-model-to-hub-optional",
    "title": "From Fine-Tuning to Deployment",
    "section": "Push model to hub (Optional)",
    "text": "Push model to hub (Optional)\n\n# imports from huggingface\nfrom huggingface_hub import create_repo, HfApi\nimport dotenv\nimport os\n\ndotenv.load_dotenv()\nusername = \"llmat\"\nHUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\nMODEL_NAME = \"Mistral-v0.3-7B-ORPO_Q4_K_M\"\n\n\n# Defined in the .env\napi = HfApi(token=HUGGINGFACE_TOKEN)\n\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n    repo_type=\"model\",\n    exist_ok=True,\n)\n\n# Upload gguf files\napi.upload_folder(\n    folder_path=MODEL_NAME,\n    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n    allow_patterns=f\"*.gguf\",\n)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#install-ollama",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#install-ollama",
    "title": "From Fine-Tuning to Deployment",
    "section": "Install Ollama",
    "text": "Install Ollama\nWith our model now quantized, the next step is to install and start Ollama. To begin, install Ollama by using this link:\nDownload link: https://ollama.com/download\nFor Windows Installation: After downloading the executable file, simply run it, and Ollama will be installed automatically.\nFor MacOS Installation: After the download completes on MacOS, unzip the downloaded file. Then, simply drag the Ollama.app folder into your Applications folder.\nLinux installation: Just run below command in your terminal. Ollama will be installed.\n\n!curl -fsSL https://ollama.com/install.sh | sh\n\nOnce the installation is complete, start the Ollama server. If using Google Colab then execute the following commands to start Ollama:\n\n!pip install colab-xterm #https://pypi.org/project/colab-xterm/\n%load_ext colabxterm\n%xterm\n\n## A Terminal window pops up\n## Add command 'ollama serve'\n\nIf running on a local environment use this command to start the Ollama serve:\n\n!ollama serve\n\nBefore we can add our quantized model to the Ollama server, Ollama requires us to create a Modelfile.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#modelfile-for-ollama",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#modelfile-for-ollama",
    "title": "From Fine-Tuning to Deployment",
    "section": "Modelfile for¬†Ollama",
    "text": "Modelfile for¬†Ollama\nOllama‚Äôs Modelfile is a developing syntax designed to act as a blueprint, defining key components and parameters to customize model behavior within the Ollama ecosystem. The Modelfile includes several key instructions: ‚Ä¢ FROM (Required): Specifies the base model or file to build from. ‚Ä¢ PARAMETER: Sets various operational parameters like temperature, context window size, and stopping conditions, influencing model output and behavior. ‚Ä¢ TEMPLATE: Defines the prompt structure, including system messages and user prompts, which guides the model‚Äôs responses. ‚Ä¢ SYSTEM: Sets the system message to dictate the model‚Äôs behavior. ‚Ä¢ ADAPTER: Applies LoRA adapters to the model for further customization. ‚Ä¢ LICENSE: Specifies the legal license under which the model is shared. ‚Ä¢ MESSAGE: Provides a message history to influence how the model generates responses.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#create-custom-modelfile",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#create-custom-modelfile",
    "title": "From Fine-Tuning to Deployment",
    "section": "Create Custom Modelfile",
    "text": "Create Custom Modelfile\nIn our example, we only need to set the path, define the template, and set parameters for the stopping conditions. Path to the Quantized Model: The Modelfile needs to specify the path to the quantized model stored on our system. This ensures the correct model is loaded for processing. Template for Message Processing: The template within the Modelfile is based on the chat template used in the base model. It is responsible for processing and formatting messages according to their roles, such as ‚Äúuser‚Äù or ‚Äúassistant.‚Äù This structure guarantees that the model‚Äôs output adheres to the dialogue format it was fine-tuned for. Stop Parameters: The stop parameters identify the boundaries of the instructions provided to the model and the responses generated by it. The markers ‚Äú[INST]‚Äù and ‚Äú[/INST]‚Äù signal the start and end of the user‚Äôs input, respectively. These delimiters ensure the model recognizes where the user‚Äôs message begins and ends. Below is how we define the path to our quantized model, construct the template content and stopping parameters within the Modelfile for our example:\n\n# Creating the content for the Modelfile\ntemplate_content = \"\"\"TEMPLATE \"\"\"\ntemplate_content += '''\"\"\"\n{{- if .Messages }}\n    {{- range $index, $_ := .Messages }}\n        {{- if eq .Role \"user\" }}\n            [INST] \n            {{ .Content }}[/INST]\n        {{- else if eq .Role \"assistant\" }}\n            {{- if .Content }} {{ .Content }}\n            {{- end }}&lt;/s&gt;\n        {{- end }}\n    {{- end }}\n{{- else }}\n    [INST] \n    {{ .Prompt }}[/INST]\n{{- end }} \n{{ .Response }}\n{{- if .Response }}&lt;/s&gt;\n{{- end }}\n\"\"\"'''\n\n# Write the rest of the parameters to the file\nwith open('./Mistral-v0.3-7B-ORPO_Q4_K_M/modelfile', 'w') as file:\n    file.write('FROM ./Mistral-v0.3-7B-ORPO_Q4_K_M.gguf\\n\\n')\n    file.write('PARAMETER stop \"[INST]\"\\n')\n    file.write('PARAMETER stop \"[/INST]\"\\n\\n')\n    file.write(template_content)\n\nLet‚Äôs break down the template content: ‚Ä¢ Processing Messages: The template processes the list of messages (.Messages) by identifying the role of each sender (.Role), effectively structuring the conversation. ‚Ä¢ Formatting User Messages: Messages from the ‚Äúuser‚Äù are enclosed within [INST] tags. If the message is the user‚Äôs only input and a system message exists, it is included at the beginning. ‚Ä¢ Formatting Assistant Messages: Messages from the ‚Äúassistant‚Äù are output directly without additional tags, with a  tag appended to signify the end of the response. ‚Ä¢ Handling Edge Cases: If no messages are present, the template provides a fallback instruction within [INST] tags to ensure that the model still generates meaningful content. ‚Ä¢ Final Response Handling: The final response is appended and closed with a  tag, ensuring the conversation is properly terminated. After creating the Modelfile, you can display the file content to verify:\n\n# Display the file content\nwith open('./Mistral-v0.3-7B-ORPO_Q4_K_M/modelfile', 'r') as file:\n    content = file.read()\n\nWith the Modelfile ready, we can now create and add our quantized model to Ollama. This command registers the quantized model with Ollama using the configurations specified in the Modelfile:\n\n!ollama create mistral-v0.3-7B-orpo_Q4_K_M -f ./Mistral-v0.3-7B-ORPO_Q4_K_M/modelfile\n\nWe can now check if our quantized model is now listed and ready to use.\n\n!ollama list\n\nNext, we install the necessary library to test the model using the LangChain framework:\n\n!pip install langchain-community langchain-core\n\nNow we run and test the model on Ollama:\n\nfrom langchain.llms import Ollama\n\nollama = Ollama(base_url=\"http://localhost:11434\", model=\"mistral-v0.3-7B-orpo_Q4_K_M\")\n\nTEXT_PROMPT = \"What is one plus one?\"\n\nprint(ollama(TEXT_PROMPT))\n\nOne plus one equals two.\n\n\nThe model should return a correct response.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#conclusion",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#conclusion",
    "title": "From Fine-Tuning to Deployment",
    "section": "Conclusion",
    "text": "Conclusion\nThis article has walked you through the process of quantizing a custom model, integrating it with Ollama, and testing it locally. By leveraging the llama.cpp framework we quantized our custom model in the Q4_K_M format and pushed it to Hugging Face Hub. We then discussed how to create the corresponding Modelfile and how to integrate our model into the Ollama framework. Quantization, offers significant benefits, including reduced memory footprint, faster inference times, and lower power consumption. These advantages make it feasible to deploy sophisticated AI models across a variety of hardware configurations, from high-performance servers to low-power edge devices, broadening the scope of where and how AI can be applied. I hope you enjoyed reading this article and learned something new. You can find the quantized model from this example on Huggingface.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#references",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#references",
    "title": "From Fine-Tuning to Deployment",
    "section": "References",
    "text": "References\nBrev.dev. (2024). Convert a fine-tuned model to GGUF format and run on Ollama. https://brev.dev/blog/convert-to-llamacpp IBM. (2024). GGUF versus GGML. IBM. https://www.ibm.com/think/topics/gguf-versus-ggml Ollama. (2024). Retrieved from https://ollama.com/blog PatrickPT‚Äôs Blog. (2024). LLM Quantization in a nutshell. https://patrickpt.github.io/posts/quantllm/",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/From_Fine-Tuning_to_Deployment.html#push-model-to-hub",
    "href": "posts/From_Fine-Tuning_to_Deployment.html#push-model-to-hub",
    "title": "From Fine-Tuning to Deployment",
    "section": "",
    "text": "# imports from huggingface\nfrom huggingface_hub import create_repo, HfApi\n\nusername = \"llmat\"\nHUGGINGFACE_TOKEN = \"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\"\nMODEL_NAME = \"Mistral-v0.3-7B-ORPO_Q8_0\"\n\n\n# Defined in the .env\napi = HfApi(token=HUGGINGFACE_TOKEN)\n\n# Create empty repo\ncreate_repo(\n    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n    repo_type=\"model\",\n    exist_ok=True,\n)\n\n# Upload gguf files\napi.upload_folder(\n    folder_path=MODEL_NAME,\n    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n    allow_patterns=f\"*.gguf\",\n)\n\n\n\n\nCommitInfo(commit_url='https://huggingface.co/llmat/Mistral-v0.3-7B-ORPO_q8_0-GGUF/commit/7d6b711f87115356c13a8fcdcc1887c4406d07b2', commit_message='Upload folder using huggingface_hub', commit_description='', oid='7d6b711f87115356c13a8fcdcc1887c4406d07b2', pr_url=None, pr_revision=None, pr_num=None)",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "From Fine-Tuning to Deployment"
    ]
  },
  {
    "objectID": "posts/Dataset Creation.html",
    "href": "posts/Dataset Creation.html",
    "title": "From Fine-Tuning to Deployment",
    "section": "",
    "text": "Imagine unlocking the full potential of large language models (LLMs) right on your local machine, without relying on costly cloud services. This is where Ollama shines by allowing users to harness the power of LLMs on their machines. While Ollama offers a range of ready-to-use models, there are times when a custom model is necessary, whether it‚Äôs fine-tuned on specific data or designed for a particular task. Efficiently deploying these custom models on local hardware often requires optimization techniques like quantization. In this article, we will explore the concept of quantization and demonstrate how to apply it to a fine-tuned model from Huggingface. We‚Äôll then cover how to install Ollama, create a corresponding Modelfile for a custom model, and integrate this custom model into Ollama, proving how easy it is to bring AI capabilities in-house. All the code used in this article is available on Google Colab and in the LLM Tutorial."
  },
  {
    "objectID": "posts/Dataset Creation.html#conclusion",
    "href": "posts/Dataset Creation.html#conclusion",
    "title": "From Fine-Tuning to Deployment",
    "section": "Conclusion",
    "text": "Conclusion\nThis article has walked you through the process of quantizing a custom model, integrating it with Ollama, and testing it locally. By leveraging the llama.cpp framework we quantized our custom model in the Q4_K_M format and pushed it to Hugging Face Hub. We then discussed how to create the corresponding Modelfile and how to integrate our model into the Ollama framework. Quantization, offers significant benefits, including reduced memory footprint, faster inference times, and lower power consumption. These advantages make it feasible to deploy sophisticated AI models across a variety of hardware configurations, from high-performance servers to low-power edge devices, broadening the scope of where and how AI can be applied. I hope you enjoyed reading this article and learned something new. You can find the quantized model from this example on Huggingface."
  },
  {
    "objectID": "posts/Dataset Creation.html#references",
    "href": "posts/Dataset Creation.html#references",
    "title": "From Fine-Tuning to Deployment",
    "section": "References",
    "text": "References\nBrev.dev. (2024). Convert a fine-tuned model to GGUF format and run on Ollama. https://brev.dev/blog/convert-to-llamacpp IBM. (2024). GGUF versus GGML. IBM. https://www.ibm.com/think/topics/gguf-versus-ggml Ollama. (2024). Retrieved from https://ollama.com/blog PatrickPT‚Äôs Blog. (2024). LLM Quantization in a nutshell. https://patrickpt.github.io/posts/quantllm/"
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html",
    "href": "posts/monte_carlo_self_refine.html",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "",
    "text": "The rapid advancement of artificial intelligence has ushered in a new era of large language models (LLMs) like GPT-4 and LLaMA. These models have demonstrated remarkable abilities in natural language understanding, generation, and even exhibited emergent properties such as reasoning and in-context learning. They have been deployed across various domains, from generating coherent narratives to assisting in programming tasks. However, when it comes to complex mathematical reasoning, especially at the level of mathematical Olympiads, these models often fall short.\nDespite their impressive linguistic capabilities, LLMs struggle with the precision and logical rigor required for high-level mathematics. They are prone to generating ‚Äúhallucinations‚Äù‚Ää-‚Ääplausible-sounding but incorrect or irrelevant outputs‚Ää-‚Ääwhich can be particularly problematic in mathematical contexts where accuracy is paramount. Traditional methods to mitigate these issues, such as self-refinement techniques, provide some relief but do not fully bridge the gap.\nEnter the Monte Carlo Tree Self-Refine (MCTSr) algorithm‚Ää-‚Ääa novel integration of LLMs with Monte Carlo Tree Search (MCTS). This innovative approach systematically explores the solution space and employs heuristic self-refinement mechanisms to enhance decision-making within LLMs. By constructing a Monte Carlo search tree through iterative processes of selection, self-refinement, self-evaluation, and backpropagation, MCTSr optimizes the balance between exploration and exploitation. The result is a significant improvement in the LLM‚Äôs ability to solve complex mathematical problems, reaching success rates comparable to GPT-4 on Olympiad-level benchmarks.\nIn this article, we delve into the theoretical underpinnings of the MCTSr technique, explore its implementation through code examples, and demonstrate its effectiveness in enhancing mathematical reasoning in LLMs. Whether you‚Äôre an AI researcher, a data scientist, or simply curious about the intersection of machine learning and mathematics, this exploration offers valuable insights into pushing the boundaries of what LLMs can achieve.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#theoretical-foundations-of-monte-carlo-tree-self-refine",
    "href": "posts/monte_carlo_self_refine.html#theoretical-foundations-of-monte-carlo-tree-self-refine",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Theoretical Foundations of Monte Carlo Tree Self-Refine",
    "text": "Theoretical Foundations of Monte Carlo Tree Self-Refine\nTo understand the Monte Carlo Tree Self-Refine (MCTSr) technique, it‚Äôs essential to grasp the fundamentals of both Monte Carlo Tree Search (MCTS) and the limitations of LLMs in mathematical reasoning. MCTSr marries these two domains to overcome the challenges inherent in each.\n\nMonte Carlo Tree Search (MCTS)¬†Primer\nMCTS is a heuristic search algorithm used extensively in decision-making processes, particularly in game playing (like Go and chess) and complex problem-solving scenarios. The algorithm incrementally builds a search tree, guided by the outcomes of random simulations (also known as rollouts), to evaluate the potential of different actions.\n\n\nMCTS operates through four primary¬†phases:\n\nSelection: Starting from the root node, the algorithm selects child nodes based on a policy (e.g., the Upper Confidence Bound for Trees, or UCT) until it reaches a leaf node.\nExpansion: If the leaf node is not a terminal state, the algorithm adds one or more child nodes, representing possible future states.\nSimulation (Rollout): From the new node, the algorithm performs a simulation to the end of the game or task, using a default policy to make decisions.\nBackpropagation: The results of the simulation are propagated back up the tree, updating the statistics of the nodes involved (e.g., win/loss records).\n\nThe key to MCTS‚Äôs success is balancing exploration (trying new, untested actions) and exploitation (choosing actions known to yield good results). This balance is often achieved using the Upper Confidence Bound for Trees algorithm (UCT), which selects child nodes that maximize the following equation:",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#challenges-in-applying-mcts-to-llms",
    "href": "posts/monte_carlo_self_refine.html#challenges-in-applying-mcts-to-llms",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Challenges in Applying MCTS to¬†LLMs",
    "text": "Challenges in Applying MCTS to¬†LLMs\nWhile MCTS is powerful, directly applying it to LLMs isn‚Äôt straightforward. LLMs generate outputs in a continuous and infinite space, making the action space vast and unbounded. Traditional MCTS relies on discrete, finite action spaces. Moreover, LLMs can produce inconsistent outputs due to their generative nature, complicating the evaluation of states and actions.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#introducing-monte-carlo-tree-self-refine-mctsr",
    "href": "posts/monte_carlo_self_refine.html#introducing-monte-carlo-tree-self-refine-mctsr",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Introducing Monte Carlo Tree Self-Refine (MCTSr)",
    "text": "Introducing Monte Carlo Tree Self-Refine (MCTSr)\nMCTSr addresses these challenges by integrating self-refinement and self-evaluation mechanisms into the MCTS framework. It adapts the traditional MCTS algorithm to operate effectively within the context of LLMs tackling mathematical problems.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#key-components-and-notations",
    "href": "posts/monte_carlo_self_refine.html#key-components-and-notations",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Key Components and Notations",
    "text": "Key Components and Notations\nBefore diving into the algorithm, let‚Äôs define the key components and notations:\n\nProblem Instance (P): The mathematical problem to be solved.\nAnswer Nodes (A): Each node represents a potential solution to P.\nActions (M): Possible self-refinement modifications to an answer.\nReward Function (R): Samples self-rewards based on the quality of the modifications.\nReward Set (Ra): Stores all reward samples for node a.\nQuality Function (Q(a)): Estimates the value of an answer node a, derived from accumulated rewards.\nUpper Confidence Bound (U(a)): Balances exploration and exploitation in node selection.\nParent Node (Father(a)): The node from which a was derived.\nChild Nodes (Children(a)): Nodes derived from a through actions m ‚àà M.\nVisit Count (N(a)): The number of times node a has been visited.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#algorithm-workflow",
    "href": "posts/monte_carlo_self_refine.html#algorithm-workflow",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Algorithm Workflow",
    "text": "Algorithm Workflow\n\nInitialization: Start with a root node representing an initial answer to P. This could be a naive solution or even a placeholder response like ‚ÄúI don‚Äôt know.‚Äù\nSelection:\n\n\nUse the quality function Q to evaluate all expandable nodes.\nSelect the node with the highest U(a) value for further exploration.\n\n\nSelf-Refinement:\n\n\nApply a self-refinement action m to the selected node a.\nThe LLM generates feedback or criticism about a and produces an improved answer a‚Äô guided by this feedback.\n\n\nSelf-Evaluation:\n\n\nThe LLM evaluates a‚Äô to assign a reward, sampling multiple times to reduce variance.\nApply constraints to ensure meaningful rewards:\n\nPrompt Constraint: Instruct the model to adhere to strict evaluation standards.\nFull Score Suppression: Discourage perfect scores to maintain discernment.\nRepeated Sampling: Collect multiple reward samples to improve reliability.¬†\n\n\n\nBackpropagation:\n\n\nUpdate the Q values of a and its ancestors based on the rewards obtained.\nThe updated Q value of a node a considers both its own reward and the maximum Q value among its children.\n\n\nUCT Update:\n\n\nRecalculate U(a) for all candidate nodes.\n\n\nTermination:\n\n\nThe process repeats until a termination condition is met (e.g., maximum iterations, satisfactory solution quality).\nUpon termination, select the best answer based on the highest Q(a) value.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#detailed-explanation-of-components",
    "href": "posts/monte_carlo_self_refine.html#detailed-explanation-of-components",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Detailed Explanation of Components",
    "text": "Detailed Explanation of Components\n\nSelf-Refinement\nSelf-refinement leverages the LLM‚Äôs capability to critique and improve upon its own outputs. The model generates feedback m on the current answer a and then refines a into a‚Äô based on this feedback. This process simulates a human iteratively improving a solution by self-critique.\n###Self-Evaluation In self-evaluation, the LLM assesses the quality of the refined answer a‚Äô and assigns a reward. Since LLMs tend to produce overly generous evaluations, constraints are necessary: - Prompt Constraint: The model is instructed to be strict and critical in its evaluation. - Full Score Suppression: Perfect scores are penalized to encourage meaningful differentiation. - Repeated Sampling: Multiple evaluations are averaged to reduce bias.\nThe quality function Q(a) is computed as:\n\n\n\nThis formula balances the worst-case scenario (min Ra) with the average reward, providing a more robust estimate of the answer‚Äôs quality.\n\n\nBackpropagation\nAfter updating the Q(a) of a, the algorithm backpropagates this information up the tree. The Q value of each parent node is updated based on its own Q(a) and the maximum Q value among its children:\n\n\n\nThis ensures that the parent nodes are aware of the best potential outcomes from their descendants.\n\n\nUCT Update and Selection\nThe updated UCT value for each candidate node guides the selection of the next node to explore. By balancing the estimated quality of the node (Q(a)) and the need to explore less-visited nodes, the algorithm efficiently navigates the search space.\n\n\nTermination Conditions\nThe algorithm can terminate based on various criteria: - Early Stopping: If improvements become negligible or solutions converge. - Resource Constraints: Maximum number of iterations or time limits. - Solution Quality: If a solution meets a predefined quality threshold.\n\n\nAdvantages of¬†MCTSr\n\nSystematic Exploration: By structuring the search as a tree, the algorithm systematically explores possible refinements.\nBalanced Decision-Making: The UCT formula ensures a balance between exploiting known good solutions and exploring new possibilities.\nEnhanced Accuracy: Self-evaluation and refinement lead to higher-quality solutions, reducing errors common in LLM outputs.\nScalability: The framework can adapt to various problem complexities by adjusting parameters like the exploration constant c and termination conditions.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#practical-implementation-of-mctsr",
    "href": "posts/monte_carlo_self_refine.html#practical-implementation-of-mctsr",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Practical Implementation of¬†MCTSr",
    "text": "Practical Implementation of¬†MCTSr\nTo bring the theoretical concepts of MCTSr to life, let‚Äôs delve into a practical implementation using Python code. This implementation demonstrates how the algorithm can be applied to improve the performance of an LLM in solving mathematical problems.\n\nSetting Up Seed Answers\n\n# Seed answers to initiate the MCTS\nseed_answers = [\n    \"I don't know the answer\",\n    \"I'm not sure\",\n    \"I can't say\",\n]\n\n\n\nCritiquing an Answer\nThe critique_answer function prompts the LLM to analyze a given answer and provide a detailed critique. This critique will guide the refinement process.\n\n# Get Critique\ndef critique_answer(question, answer):\n    prompt = (\n        f\"Question: {question}\\n\"\n        f\"Answer Attempt: {answer}\\n\"\n        \"Please analyze the answer above. \"\n        \"Identify any inaccuracies or areas lacking detail. \"\n        \"Provide a thorough critique, highlighting specific flaws and suggesting improvements. \"\n        \"Your critique should be detailed and step-by-step. \"\n        \"Do not provide a revised answer.\"\n    )\n    # Request critique from the language model\n    return chat_completion_request(prompt)\n\nExplanation: This function constructs a prompt that includes the question and the current answer. It instructs the LLM to provide a detailed critique without offering a revised answer.\n\n\nRefining the Answer\nUsing the critique, the refine_answer function prompts the LLM to generate an improved answer.\n\n# Improve the answer\ndef refine_answer(question, answer, critique):\n    prompt = (\n        f\"Question: {question}\\n\"\n        f\"Current Answer: {answer}\\n\"\n        f\"Feedback: {critique}\\n\\n\"\n        \"Based on the feedback, refine the answer to address all the points raised. \"\n        \"Ensure the new answer is accurate, detailed, and well-structured. \"\n        \"Present your reasoning process and verification steps before providing the final answer.\"\n    )\n    # Request refined answer from the language model\n    return chat_completion_request(prompt)\n\nExplanation: This function constructs a prompt that includes the question, the current answer, and the critique. It instructs the LLM to refine the answer based on the feedback.\n\n\nEvaluating the Answer\nThe evaluate_answer function asks the LLM to assess the refined answer, provide a critique, and assign a numerical rating.\n\ndef evaluate_answer(question, answer):\n    prompt = (\n        f\"Question: {question}\\n\"\n        f\"Answer: {answer}\\n\"\n        \"As an expert, assess the answer above for correctness and completeness. \"\n        \"Provide a detailed critique, pointing out any issues. \"\n        \"Then, assign a rating between 0 and 100, where 100 represents a perfect answer. \"\n        \"Format:\\n\"\n        \"Critique: &lt;Your detailed critique&gt;\\n\"\n        \"Rating: &lt;Numerical rating&gt;\"\n    )\n    # Request evaluation from the language model\n    evaluation = chat_completion_request(prompt)\n    \n    # Extract the rating from the evaluation\n    try:\n        match = re.search(r'Rating:\\s*(\\d+\\.?\\d*)', evaluation)\n        if match:\n            rating = float(match.group(1))\n            rating = min(rating, 95)  # Cap the rating at 95\n            rating /= 100.0\n        else:\n            raise ValueError(\"Rating not found in the evaluation.\")\n    except Exception as e:\n        print(f\"Error extracting rating: {e}\")\n        print(f\"Evaluation response: {evaluation}\")\n        rating = 0.0\n    \n    print(f\"\\nEvaluation Response:\\n{evaluation}\")\n    return rating\n\nExplanation: The function prompts the LLM to critique the answer and assign a rating. It then parses the LLM‚Äôs response to extract the numerical rating, capping it at 95 to prevent overconfidence and normalizing it to a value between 0 and 1.\n\n\nDefining the Tree Node Structure\nWe define a TreeNode class to represent nodes in the MCTS tree. Each node contains an answer and references to its parent and children.\n\nimport math\nimport random\nimport numpy as np\nimport re\n\n# Define the maximum number of children per node\nMAX_CHILDREN = 3\n\nclass TreeNode:\n    def __init__(self, question, answer, parent=None):\n        self.question = question\n        self.answer = answer\n        self.parent = parent\n        self.children = []\n        self.visits = 0\n        self.value = 0.0\n        self.Ra = []  # List to store all reward samples\n        self.Q = 0.0  # Quality value Q(a)\n\n    def fully_expanded(self):\n        return len(self.children) &gt;= MAX_CHILDREN\n\n    def select_promising_child(self, exploration_param=1.41):\n        best_score = float('-inf')\n        best_child = None\n        for child in self.children:\n            if child.visits == 0:\n                ucb_score = float('inf')\n            else:\n                # Compute Q(a) using the exact formula\n                min_ra = min(child.Ra)\n                avg_ra = child.value / child.visits\n                child.Q = 0.5 * (min_ra + avg_ra)\n                \n                exploration = exploration_param * math.sqrt(2 * math.log(self.visits) / child.visits)\n                ucb_score = child.Q + exploration\n            if ucb_score &gt; best_score:\n                best_score = ucb_score\n                best_child = child\n        return best_child\n\n    def most_visited_child(self):\n        return max(self.children, key=lambda c: c.visits, default=None)\n\n    def add_child(self, child_node):\n        self.children.append(child_node)\n\nExplanation: - fully_expanded: Checks if the node has reached the maximum number of child nodes. - select_promising_child: Implements the UCT formula to select the most promising child node. - most_visited_child: Retrieves the child node with the highest visit count. - add_child: Adds a new child node to the current node.\n\n\nImplementing the Monte Carlo Tree Search\nThe MonteCarloTreeSearch class orchestrates the MCTS process, integrating the functions defined earlier.\n\nclass MonteCarloTreeSearch:\n    def __init__(self, question, initial_answers, iterations=3):\n        self.question = question\n        self.iterations = iterations\n        # Initialize the root with a random seed answer\n        self.root = TreeNode(question, random.choice(initial_answers))\n\n    def perform_search(self):\n        for iteration in range(self.iterations):\n            print(f\"\\nIteration {iteration + 1}/{self.iterations}\")\n            node = self._tree_policy(self.root)\n            print(f\"Selected Node Answer: {node.answer}\")\n            reward = self._default_policy(node)\n            print(f\"Simulated Reward: {reward}\")\n            self._backpropagate(node, reward)\n        best_child = self.root.most_visited_child()\n        if best_child:\n            print(f\"Most Visited Child Visits: {best_child.visits}\")\n            return best_child.answer\n        else:\n            return self.root.answer\n\n    def _tree_policy(self, node):\n        while not node.fully_expanded():\n            return self._expand(node)\n        return self._best_child(node)\n\n    def _expand(self, node):\n        for _ in range(MAX_CHILDREN - len(node.children)):\n            # Generate a critique and refine the answer\n            critique = critique_answer(node.question, node.answer)\n            print(f\"\\nCritique:\\n{critique}\")\n            refined_answer = refine_answer(node.question, node.answer, critique)\n            print(f\"\\nRefined Answer:\\n{refined_answer}\")\n            # Create a new child node with the refined answer\n            child_node = TreeNode(node.question, refined_answer, parent=node)\n            node.add_child(child_node)\n        # Return one of the newly added children\n        return random.choice(node.children)\n\n    def _default_policy(self, node):\n        return evaluate_answer(node.question, node.answer)\n\n    def _backpropagate(self, node, reward):\n        while node is not None:\n            node.visits += 1\n            node.value += reward\n            node.Ra.append(reward)  # Store the reward sample\n\n            # Update Q(a) using the existing formula\n            if node.Ra:\n                min_ra = min(node.Ra)\n                avg_ra = node.value / node.visits\n                node.Q = 0.5 * (min_ra + avg_ra)\n            \n            # If the node has a parent, update the parent's Q(a) based on the formula\n            if node.parent is not None:\n                parent = node.parent\n                # Compute the maximum Q among the parent's children\n                if parent.children:\n                    max_child_Q = max(child.Q for child in parent.children if child.Q is not None)\n                    # Update the parent's Q(a)\n                    parent.Q = 0.5 * (parent.Q + max_child_Q)\n            node = node.parent\n\n    def _best_child(self, node, exploration_param=1.41):\n        return node.select_promising_child(exploration_param)\n\nExplanation: - perform_search: Runs the MCTS for a specified number of iterations and returns the best answer found. - _tree_policy: Decides whether to expand a node or move to the best child. - _expand: Generates critiques and refines the answer to create child nodes. - _default_policy: Evaluates the node‚Äôs answer to simulate the reward. - _backpropagate: Updates the visit count and value of the nodes up the tree.\n\n\nRunning the MCTS\nFinally, we initialize the MCTS with a question and perform the search to find the best answer.\n\nmcts = MonteCarloTreeSearch(question, seed_answers, iterations=10)\nbest_answer = mcts.perform_search()\n\nExplanation: We create an instance of MonteCarloTreeSearch with the question and seed answers. We specify the number of iterations (rollouts) for the search.\n\n\nApplying MCTSr to a Mathematical Problem\nSuppose we have the following mathematical question: Question: ‚ÄúCalculate the sum of the interior angles of a 12-sided polygon.‚Äù We can use the MCTSr implementation to find an accurate answer.\n\nquestion = \"Calculate the sum of the interior angles of a 12-sided polygon.\"\nmcts = MonteCarloTreeSearch(question, seed_answers, iterations=10)\nbest_answer = mcts.perform_search()\nprint(f\"\\nBest Answer:\\n{best_answer}\")\n\n\n\nOutput:\n(Snippet showing only the final anwer)\n‚Ä¶\nBest Answer: Refined Answer: Calculating the sum of interior angles of a 12-sided polygon\nStep-by-step calculation:\nA 12-sided polygon is called a dodecagon. Using the formula for the sum of interior angles of a polygon, we can calculate the sum:\nSum of interior angles = (n-2) √ó 180\nWhere n is the number of sides.\nIn this case, n = 12, so: Sum of interior angles = (12 - 2) √ó 180 = 10 √ó 180 = 1800\nReasoning and Verification:\nThe formula for the sum of interior angles of a polygon is derived from the fact that each interior angle is supplementary to its adjacent exterior angle. By applying this formula to a dodecagon, we can calculate the sum of its interior angles.\nTo verify the answer, we can also use a different approach. The sum of interior angles of a polygon is also equal to (n-2) √ó 180, where n is the number of sides. Since we have already shown that n = 12, we can substitute this value into the formula:\nSum of interior angles = (12 - 2) √ó 180 = 10 √ó 180 = 1800\nFinal Answer: The sum of the interior angles of a 12-sided polygon is 1800 degrees.\nThis refined answer demonstrates a clear understanding of the problem and provides a step-by-step calculation using relevant mathematical concepts. It also includes a verification step to ensure the answer is accurate.\n‚Ä¶\nExplanation: The MCTS starts with a seed answer and iteratively refines it. After ten iterations, it arrives at a correct and well-explained solution.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#advantages-of-the-implementation",
    "href": "posts/monte_carlo_self_refine.html#advantages-of-the-implementation",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Advantages of the Implementation",
    "text": "Advantages of the Implementation\n\nIterative Improvement: The algorithm systematically improves the answer through self-critique and refinement.\nBalanced Exploration: Using UCT, the search balances exploring new refinements and exploiting known good answers.\nAutomatic Evaluation: The self-evaluation step allows the model to assess the quality of answers without external input.\n\n\nLimitations\n\nComputational Resources: Each iteration involves multiple calls to the LLM, which can be time-consuming and resource-intensive.\nModel Dependence: The quality of the final answer heavily relies on the LLM‚Äôs capability to critique and refine effectively.\nParameter Tuning: Parameters like MAX_CHILDREN and the exploration constant need to be tuned for optimal performance.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#conclusion",
    "href": "posts/monte_carlo_self_refine.html#conclusion",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Conclusion",
    "text": "Conclusion\nThe Monte Carlo Tree Self-Refine technique represents a significant step forward in enhancing LLMs‚Äô ability to tackle complex reasoning tasks. By integrating MCTS with self-refinement strategies, MCTSr offers a robust framework for improving decision-making and solution quality in AI applications. The practical implementation demonstrates how theoretical concepts can be applied to achieve tangible improvements in mathematical problem-solving.\nAs research continues, we can expect further refinements and broader applications of this innovative approach, potentially extending beyond mathematics to other domains requiring complex reasoning and decision-making.\nNote: The code examples provided use placeholder functions like chat_completion_request, which should be implemented using the appropriate API calls to the language model you‚Äôre interfacing with. The full code used in this article is available on Google Colab and in the LLM Tutorial.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#references",
    "href": "posts/monte_carlo_self_refine.html#references",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "References",
    "text": "References\nZhang, D., Huang, X., Zhou, D., Li, Y., & Ouyang, W. (2024). Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B. arXiv (Cornell University). https://doi.org/10.48550/arxiv.2406.07394",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Inference**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#conclusion-1",
    "href": "posts/monte_carlo_self_refine.html#conclusion-1",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Conclusion",
    "text": "Conclusion\nThe MCT Self-Refine (MCTSr) algorithm represents a significant advancement in AI-driven problem-solving. By integrating Monte Carlo Tree Search (MCTS) with large language models (LLMs), MCTSr addresses critical challenges in accuracy and reliability, particularly within reasoning tasks.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Prompting**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#conclusion-2",
    "href": "posts/monte_carlo_self_refine.html#conclusion-2",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article we discussed the the Monte Carlo Search Refine and we implemented it.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Prompting**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/monte_carlo_self_refine.html#references-1",
    "href": "posts/monte_carlo_self_refine.html#references-1",
    "title": "Monte Carlo Tree Self-Refine",
    "section": "References",
    "text": "References",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **Prompting**",
      "Monte Carlo Tree Self-Refine"
    ]
  },
  {
    "objectID": "posts/Build_DPO_Dataset.html",
    "href": "posts/Build_DPO_Dataset.html",
    "title": "Build a High-Quality DPO Dataset",
    "section": "",
    "text": "Direct Preference Optimization (DPO) is a technique in fine-tuning language models using human preference data. In this article, we‚Äôll explore how to build a high-quality DPO dataset by consolidating multiple existing datasets, focusing on extracting only the best answers. We‚Äôll walk through the code implementation step-by-step to demonstrate this process.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Build a High-Quality DPO Dataset"
    ]
  },
  {
    "objectID": "posts/Build_DPO_Dataset.html#what-is-direct-preference-optimization-dpo",
    "href": "posts/Build_DPO_Dataset.html#what-is-direct-preference-optimization-dpo",
    "title": "Build a High-Quality DPO Dataset",
    "section": "What is Direct Preference Optimization (DPO)?",
    "text": "What is Direct Preference Optimization (DPO)?\nAs language models advance, aligning them with human preferences has become increasingly important. Direct Preference Optimization (DPO) is a method that directly integrates human preferences into the training process, eliminating the need for separate reward models or reinforcement learning.\nKey Concepts of DPO:\n\nDirect Optimization: DPO adjusts the language model to favor responses preferred by humans, streamlining the alignment process.\nSimplified Training: By removing the need for auxiliary models and reinforcement learning, DPO reduces computational overhead and complexity.\nEnhanced Alignment: This approach produces outputs more in line with human expectations and values, improving user satisfaction.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Build a High-Quality DPO Dataset"
    ]
  },
  {
    "objectID": "posts/Build_DPO_Dataset.html#dpo-fine-tuning",
    "href": "posts/Build_DPO_Dataset.html#dpo-fine-tuning",
    "title": "Build a High-Quality DPO Dataset",
    "section": "DPO Fine-tuning",
    "text": "DPO Fine-tuning\nAt the core of the DPO fine-tuning process lies the concept of creating an exact duplicate of the language model (LM) being trained, with its parameters set to remain unchanged. This ‚Äúfrozen‚Äù model serves as a reference point during training.\nFor each data point, both the trained (policy) and frozen (reference) language models evaluate the chosen and rejected responses. The evaluation score is calculated as the product of the probabilities assigned to each token in the target response at every step. Thanks to the causal decoder architecture of these generative language models, we can compute this score in a single forward pass.\n\n\n\nTo score a chosen or rejected response for a given prompt, the LM calculates the probability of generating each response token sequentially. These probabilities are multiplied together to obtain the final score for that response.\nWith scores for both the chosen and rejected responses from both models, we compute the ratio between the scores from the trained language model (ùëÖ_policy‚Äã) and those from the frozen language model (ùëÖ_reference‚Äã). These ratios are then used to calculate the final loss, guiding the model‚Äôs weight adjustments during the gradient descent update:\n\n\n\nwhere Œ≤ is a hyperparameter and œÉ is the sigmoid function.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Build a High-Quality DPO Dataset"
    ]
  },
  {
    "objectID": "posts/Build_DPO_Dataset.html#building-a-high-quality-dpo-dataset",
    "href": "posts/Build_DPO_Dataset.html#building-a-high-quality-dpo-dataset",
    "title": "Build a High-Quality DPO Dataset",
    "section": "Building a High-Quality DPO Dataset",
    "text": "Building a High-Quality DPO Dataset\nTo effectively implement Direct Preference Optimization (DPO), it‚Äôs essential to curate a dataset of high-quality preference pairs. Several notable datasets can serve as valuable resources:\n\nargilla/distilabel-math-preference-dpo: Developed by Argilla using the Distilabel framework, this dataset comprises approximately 2,418 entries. Each entry includes a math-related instruction, two model-generated responses, and corresponding quality ratings, facilitating the enhancement of mathematical reasoning in language models.\nargilla/distilabel-intel-orca-dpo-pairs: This dataset is a ‚Äúdistilabeled‚Äù version of the widely used Intel/orca_dpo_pairs. It has been improved using the Distilabel framework to enhance the quality of preference pairs, making it suitable for fine-tuning models with diverse preference data.\nargilla/ultrafeedback-binarized-preferences-cleaned: This dataset offers cleaned and binarized preference pairs, providing a refined resource for training models to understand and prioritize user preferences effectively.\nM4-ai/prm_dpo_pairs_cleaned: Containing cleaned DPO pairs, this dataset aids in fine-tuning models to align with preferred responses, enhancing their decision-making capabilities.\njondurbin/truthy-dpo-v0.1: Focused on truthfulness, this dataset provides preference pairs that help models discern and prioritize truthful information, crucial for maintaining accuracy and reliability.\nunalignment/toxic-dpo-v0.2: This dataset addresses toxicity by offering preference pairs that guide models to avoid generating harmful or offensive content, promoting safer AI interactions.\nargilla/Capybara-Preferences: A collection of preference pairs tailored to specific tasks, this dataset assists in fine-tuning models for specialized applications, enhancing their adaptability and performance.\n\nBy selecting the highest-rated responses from these datasets, we can curate a collection of superior preference pairs, thereby enhancing the effectiveness of DPO fine-tuning.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Build a High-Quality DPO Dataset"
    ]
  },
  {
    "objectID": "posts/Build_DPO_Dataset.html#code-implementation",
    "href": "posts/Build_DPO_Dataset.html#code-implementation",
    "title": "Build a High-Quality DPO Dataset",
    "section": "Code Implementation",
    "text": "Code Implementation\nLet‚Äôs dive into the code to see how we can achieve this. We‚Äôll use the datasets library from Hugging Face to handle dataset loading and manipulation.\n\n1. Import Necessary Libraries\n\nfrom datasets import load_dataset, Dataset, concatenate_datasets as hf_concatenate_datasets, DatasetDict, Features, Value\n\nExplanation:\n‚Ä¢   load_dataset: Loads datasets from the Hugging Face Hub.\n‚Ä¢   Dataset: A class representing a dataset.\n‚Ä¢   concatenate_datasets: Function to concatenate multiple datasets.\n‚Ä¢   DatasetDict: A dictionary-like class for datasets with multiple splits.\n‚Ä¢   Features, Value: Used to define a consistent schema for our datasets.\n\n\n2. Load Datasets\n\n# Load datasets\ndatasets = {\n    \"math_preference\": load_dataset(\"argilla/distilabel-math-preference-dpo\"),\n    \"intel_orca\": load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\"),\n    \"ultrafeedback_binarized\": load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\"),\n    \"prm_dpo\": load_dataset(\"M4-ai/prm_dpo_pairs_cleaned\"),\n    \"truthy_dpo\": load_dataset(\"jondurbin/truthy-dpo-v0.1\"),\n    \"toxic_dpo\": load_dataset(\"unalignment/toxic-dpo-v0.2\"),\n    \"capybara\": load_dataset(\"argilla/Capybara-Preferences\"),\n}\n\nExplanation:\nWe load several DPO datasets from the Hugging Face Hub and store them in a dictionary for easy access. Each key corresponds to a dataset name, and the value is the loaded dataset.\n\n\n3. Define a Consistent Schema\n\n# Define the consistent schema\nconsistent_features = Features({\n    \"origin\": Value(\"string\"),\n    \"chosen\": [{\"content\": Value(\"string\"), \"role\": Value(\"string\")}],\n    \"rejected\": [{\"content\": Value(\"string\"), \"role\": Value(\"string\")}],\n    \"prompt\": Value(\"string\"),\n})\n\nExplanation:\nWe define a consistent schema (consistent_features) to standardize the datasets. This schema includes:\n‚Ä¢   origin: The source of the data.\n‚Ä¢   chosen: The preferred answer, along with its role.\n‚Ä¢   rejected: The less preferred answer.\n‚Ä¢   prompt: The input prompt to which the answers respond.\n\n\n4. Transform Examples Function\n\n# Function to transform the 'chosen' and 'rejected' features into lists of dictionaries\ndef transform_example(example):\n    if 'prompt' in example and 'chosen' in example:\n        example['chosen'] = [\n            {\"content\": example['prompt'], \"role\": \"user\"},\n            {\"content\": example['chosen'], \"role\": \"assistant\"}\n        ]\n    if 'prompt' in example and 'rejected' in example:\n        example['rejected'] = [\n            {\"content\": example['prompt'], \"role\": \"user\"},\n            {\"content\": example['rejected'], \"role\": \"assistant\"}\n        ]\n    return example\n\nExplanation:\nThis function transforms each example to match our consistent schema. It structures the ‚Äòchosen‚Äô and ‚Äòrejected‚Äô responses as lists of dictionaries, pairing the prompt with the assistant‚Äôs response.\n\n\n5. Align Dataset Features\n\n# Align dataset features\ndef align_features(dataset, source_name):\n    aligned_data = {\n        feature: dataset[feature] if feature in dataset.column_names else [None] * len(dataset)\n        for feature in consistent_features\n    }\n    aligned_data[\"origin\"] = [source_name] * len(dataset)\n    return Dataset.from_dict(aligned_data, features=consistent_features)\n\nExplanation:\nThe align_features function ensures that each dataset conforms to the consistent schema. It fills in missing features with None and adds the origin field.\n\n\n6. Preprocess Datasets\nWe preprocess each dataset individually to filter and transform the data according to our requirements.\n6.1 Capybara Dataset\n\n# Capybara dataset\ndatasets['capybara']['train'] = datasets['capybara']['train']\\\n    .filter(lambda x: x['chosen_rating'] is float(x['chosen_rating']) &gt;= 5)\\\n    .map(lambda x: {'prompt': x['chosen'][0]['content'] if x['chosen'] else \"\", **x})\n\nExplanation:\n‚Ä¢   Filter: Keeps entries where the chosen_rating is None or greater than or equal to 5.\n‚Ä¢   Map: Extracts the prompt from the chosen response.\n6.2 PRM DPO Dataset\n\n# PRM DPO dataset\ndatasets['prm_dpo']['train'] = datasets['prm_dpo']['train']\\\n    .filter(lambda x: x['is_chosen_correct'])\\\n    .map(transform_example)\n\nExplanation:\n‚Ä¢   Filter: Keeps entries where the chosen answer is marked as correct.\n‚Ä¢   Map: Applies the transform_example function to standardize the data.\n6.3 Ultrafeedback Binarized Dataset\n\n# Ultrafeedback binarized dataset\ndatasets['ultrafeedback_binarized']['train'] = datasets['ultrafeedback_binarized']['train']\\\n    .filter(lambda x: x['chosen-rating'] is x['chosen-rating'] &gt;= 5)\n\nExplanation:\nFilters out entries where the chosen answer has a rating less than 5.\n6.4 Intel ORCA Dataset\n\n# Intel ORCA dataset\ndatasets['intel_orca']['train'] = datasets['intel_orca']['train']\\\n    .rename_column('input', 'prompt')\\\n    .filter(lambda x: x['rating'] is not None and x['rating'][0] &gt;= 10 and x['rating'][1] &gt;= 10)\\\n    .filter(lambda x: not x.get('in_gsm8k_train', False))\\\n    .map(transform_example)\n\nExplanation:\n‚Ä¢   Rename Column: Renames ‚Äòinput‚Äô to ‚Äòprompt‚Äô for consistency.\n‚Ä¢   Filter: Keeps entries with high ratings (&gt;=10) for both choices and excludes those in the GSM8K train set.\n‚Ä¢   Map: Transforms examples to match the schema.\n6.5 Math Preference Dataset\n\n# Math preference dataset\ndatasets['math_preference']['train'] = datasets['math_preference']['train']\\\n    .rename_column('instruction', 'prompt')\\\n    .rename_column('chosen_response', 'chosen')\\\n    .rename_column('rejected_response', 'rejected')\\\n    .filter(lambda x: x['chosen_rating'] is x['chosen_rating'] &gt;= 9)\\\n    .map(transform_example)\n\nExplanation:\n‚Ä¢   Rename Columns: Adjusts column names to match our schema.\n‚Ä¢   Filter: Keeps entries where the chosen rating is None or greater than or equal to 9.\n‚Ä¢   Map: Applies the transformation function.\n6.6 Truthy DPO and Toxic DPO Datasets\n\n# Truthy DPO and Toxic DPO datasets\ndatasets['truthy_dpo'] = datasets['truthy_dpo'].map(transform_example)\ndatasets['toxic_dpo'] = datasets['toxic_dpo'].map(transform_example)\n\nExplanation:\nSimply applies the transform_example function to standardize these datasets.\n\n\n7. Align and Collect All Datasets\n\n# Align and collect all datasets\nall_datasets = []\nfor name, dataset_dict in datasets.items():\n    for split, dataset in dataset_dict.items():\n        aligned_dataset = align_features(dataset, name)\n        all_datasets.append(aligned_dataset)\n\nExplanation:\nWe iterate over each dataset and its splits, align their features, and collect them into a list.\n\n\n8. Concatenate All Datasets\n\n# Concatenate all datasets\ncombined_dataset = hf_concatenate_datasets(all_datasets)\n\nExplanation:\nWe concatenate all the aligned datasets into a single dataset using concatenate_datasets.\n\n\n9. Create the Final Dataset\n\n# Create the final dataset\nfinal_dataset = DatasetDict({'train': combined_dataset})\n\nExplanation:\nWe wrap the combined dataset into a DatasetDict with a ‚Äòtrain‚Äô split.\n\nVerify the Dataset\n\n\n# Print the combined dataset schema and a few rows to verify\nprint(final_dataset)\nprint(final_dataset['train'][:1])\n\nDatasetDict({\n    train: Dataset({\n        features: ['origin', 'chosen', 'rejected', 'prompt'],\n        num_rows: 45011\n    })\n})\n{'origin': ['math_preference'], 'chosen': [[{'content': 'How can I simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`?.\\n Take a deep breath, think step by step, and give an accurate response', 'role': 'user'}, {'content': 'To simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`, we can follow a few steps:\\n\\nStep 1: Distribute the division symbol by multiplying the expression by the reciprocal of the denominator. The reciprocal of `2x` is `1/(2x)`, so the expression becomes `(3x^2 - 4y^3) * (1/(2x))`.\\n\\nStep 2: Simplify within the parentheses by dividing each term separately. \\n\\n- For the first term, `3x^2`, divide `3x^2` by `2x`. This gives us `(3x^2) / (2x) = (3/2) * (x^2 / x) = (3/2) * x`.\\n\\n- For the second term, `-4y^3`, divide `-4y^3` by `2x`. This gives us `(-4y^3) / (2x) = (-2) * (y^3 / x)`.\\n\\nStep 3: Combine the simplified terms from Step 2. The expression now becomes `(3/2) * x - 2 * (y^3 / x)`.\\n\\nSo, the simplified form of the algebraic expression `(3x^2 - 4y^3) / (2x)` is `(3/2) * x - 2 * (y^3 / x)`.', 'role': 'assistant'}]], 'rejected': [[{'content': 'How can I simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`?.\\n Take a deep breath, think step by step, and give an accurate response', 'role': 'user'}, {'content': \"To simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`, you can divide each term in the numerator by the denominator.\\n\\nFirst, let's divide `3x^2` by `2x`. Since both terms have a common factor of `x`, we can simplify this expression to `3x`.\\n\\nNext, we divide `-4y^3` by `2x`. We can simplify this expression by dividing each term separately. \\n\\nDividing `-4` by `2` gives `-2`. Then, dividing `y^3` by `x` gives `y^3/x`.\\n\\nSo, the simplified form of `(3x^2 - 4y^3) / (2x)` is `3x - 2y^3/x`.\", 'role': 'assistant'}]], 'prompt': ['How can I simplify the algebraic expression `(3x^2 - 4y^3) / (2x)`?.\\n Take a deep breath, think step by step, and give an accurate response']}\n\n\nExplanation:\nWe print the structure of the final dataset and the first few entries to ensure everything is correctly formatted.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Build a High-Quality DPO Dataset"
    ]
  },
  {
    "objectID": "posts/Build_DPO_Dataset.html#conclusion",
    "href": "posts/Build_DPO_Dataset.html#conclusion",
    "title": "Build a High-Quality DPO Dataset",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we‚Äôve journeyed through the process of building a high-quality DPO dataset by carefully aggregating and standardizing multiple existing datasets. By focusing on extracting only the best answers and ensuring a consistent schema, we‚Äôve created a robust resource that can significantly enhance the performance of language models fine-tuned using Direct Preference Optimization.\nThis approach not only streamlines the training process but also ensures that our models are more closely aligned with human preferences, leading to outputs that are more accurate, reliable, and satisfying for users. As language models continue to evolve, techniques like DPO and the careful curation of training data become ever more important.\nBy investing time in building superior datasets, we lay the groundwork for more advanced, aligned, and human-centric AI models in the future. I encourage practitioners to apply these methods, explore further optimizations, and contribute to the ongoing effort to align AI models with human values and expectations.\nI hope you found this guide helpful. If you liked this article, follow me on Hugging Face @llmat. Best of luck with your model fine-tuning!",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Build a High-Quality DPO Dataset"
    ]
  },
  {
    "objectID": "posts/Build_DPO_Dataset.html#references",
    "href": "posts/Build_DPO_Dataset.html#references",
    "title": "Build a High-Quality DPO Dataset",
    "section": "References",
    "text": "References\n\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. arXiv preprint arXiv:2305.18290.\nHugging Face. (n.d.). Datasets. Retrieved November 1, 2024, from https://huggingface.co/docs/datasets/index",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "üó£Ô∏è **LLM Post-Training**",
      "Build a High-Quality DPO Dataset"
    ]
  },
  {
    "objectID": "posts/LLM_Evaluation_Framework.html",
    "href": "posts/LLM_Evaluation_Framework.html",
    "title": "LLM Evaluation Framework",
    "section": "",
    "text": "The discontinuation of Hugging Face‚Äôs Open LLM Leaderboard has left a gap in the community for standardized evaluation of large language models (LLMs). To address this, I developed the LLM Evaluation Framework, a comprehensive and modular tool designed to facilitate reproducible and extensible benchmarking of LLMs across various tasks and benchmarks.\nThe LLM Evaluation Framework can be found on my Github account: LLM Evaluation Framework",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **LLM Evaluation**",
      "LLM Evaluation Framework"
    ]
  },
  {
    "objectID": "posts/LLM_Evaluation_Framework.html#why-this-framework-matters",
    "href": "posts/LLM_Evaluation_Framework.html#why-this-framework-matters",
    "title": "LLM Evaluation Framework",
    "section": "üß© Why This Framework Matters",
    "text": "üß© Why This Framework Matters\nThe Open LLM Leaderboard was instrumental in providing a centralized platform for evaluating and comparing LLMs. Its retirement has underscored the need for tools that allow researchers and developers to conduct their own evaluations with transparency and consistency. The LLM Evaluation Framework aims to fill this void by offering:\n\nModular Design: Inspired by microservice architecture, enabling easy integration and customization.\nMultiple Model Backends: Support for Hugging Face (hf) and vLLM backends, allowing flexibility in model loading and inference.\nQuantization Support: Evaluate quantized models (e.g., 4-bit, 8-bit with hf, AWQ with vLLM) to assess performance under resource constraints.\nComprehensive Benchmarks: Includes support for standard benchmarks like MMLU, GSM8K, BBH, and more.\nLeaderboard Replication: Easily run evaluations mimicking the Open LLM Leaderboard setup with standardized few-shot settings.\nFlexible Configuration: Customize evaluations via CLI arguments or programmatic usage.\nDetailed Reporting: Generates JSON results and Markdown reports for easy analysis.\nParallelism: Leverages vLLM for efficient inference, including tensor parallelism across multiple GPUs.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **LLM Evaluation**",
      "LLM Evaluation Framework"
    ]
  },
  {
    "objectID": "posts/LLM_Evaluation_Framework.html#getting-started",
    "href": "posts/LLM_Evaluation_Framework.html#getting-started",
    "title": "LLM Evaluation Framework",
    "section": "üöÄ Getting Started",
    "text": "üöÄ Getting Started\nInstallation 1. Clone the Repository:\n\n!git clone https://github.com/mattdepaolis/llm-evaluation.git\n!cd llm-evaluation\n\n\nSet Up a Virtual Environment:\n\n\n!python -m venv .venv\n!source .venv/bin/activate  # On Windows use `.venv\\Scripts\\activate`\n\n\nInstall Dependencies:\n\n\n!pip install -e lm-evaluation-harness\n!pip install torch numpy tqdm transformers accelerate bitsandbytes sentencepiece\n!pip install vllm  # If you plan to use the vLLM backend",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **LLM Evaluation**",
      "LLM Evaluation Framework"
    ]
  },
  {
    "objectID": "posts/LLM_Evaluation_Framework.html#example-usage",
    "href": "posts/LLM_Evaluation_Framework.html#example-usage",
    "title": "LLM Evaluation Framework",
    "section": "üß™ Example Usage",
    "text": "üß™ Example Usage\nUsing the Command-Line Interface (CLI)\nEvaluate a model on the HellaSwag benchmark:\n\n!python llm_eval_cli.py \\\n  --model hf \\\n  --model_name google/gemma-2b \\\n  --tasks hellaswag \\\n  --num_fewshot 0 \\\n  --device cuda  # Use 'cpu' if you don't have a GPU\n\nThis command will download the gemma-2b model (if not cached), run it on the HellaSwag benchmark with 0 few-shot examples, and save the results in the results/ and reports/ directories.\nUsing as a Python Library\nIntegrate the evaluation logic directly into your Python scripts:\n\nfrom llm_eval import evaluate_model\nimport os\n\n# Define evaluation parameters\neval_config = {\n    \"model_type\": \"hf\",\n    \"model_name\": \"google/gemma-2b-it\",\n    \"tasks\": [\"mmlu\", \"gsm8k\"],\n    \"num_fewshot\": 0,\n    \"device\": \"cuda\",\n    \"quantize\": True,\n    \"quantization_method\": \"4bit\",\n    \"batch_size\": \"auto\",\n    \"output_dir\": \"./custom_results\"  # Optional: Specify output location\n}\n\n# Run the evaluation\ntry:\n    results_summary, results_file_path = evaluate_model(**eval_config)\n\n    print(\"Evaluation completed successfully!\")\n    print(f\"Results summary: {results_summary}\")\n    print(f\"Detailed JSON results saved to: {results_file_path}\")\n\n    # Construct the expected report path\n    base_name = os.path.splitext(os.path.basename(results_file_path))[0]\n    report_file_path = os.path.join(os.path.dirname(results_file_path).replace('results', 'reports'), f\"{base_name}_report.md\")\n\n    if os.path.exists(report_file_path):\n        print(f\"Markdown report saved to: {report_file_path}\")\n    else:\n        print(\"Markdown report not found at expected location.\")\n\nexcept Exception as e:\n    print(f\"An error occurred during evaluation: {e}\")",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **LLM Evaluation**",
      "LLM Evaluation Framework"
    ]
  },
  {
    "objectID": "posts/LLM_Evaluation_Framework.html#reporting-and-results",
    "href": "posts/LLM_Evaluation_Framework.html#reporting-and-results",
    "title": "LLM Evaluation Framework",
    "section": "üìä Reporting and Results",
    "text": "üìä Reporting and Results\nThe framework generates:\n\nJSON Results: Detailed results for each task, including individual sample predictions (if applicable), metrics, and configuration details, saved in the results/ directory.\nMarkdown Reports: A summary report aggregating scores across tasks, generated in the reports/ directory.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **LLM Evaluation**",
      "LLM Evaluation Framework"
    ]
  },
  {
    "objectID": "posts/LLM_Evaluation_Framework.html#extending-the-framework",
    "href": "posts/LLM_Evaluation_Framework.html#extending-the-framework",
    "title": "LLM Evaluation Framework",
    "section": "üîß Extending the Framework",
    "text": "üîß Extending the Framework\nThe modular design makes it easier to add new functionalities:\n\nAdding New Tasks/Benchmarks:\n\n\nDefine the task configuration in llm_eval/tasks/task_registry.py or a similar configuration file.\nEnsure the task is compatible with the lm-evaluation-harness structure or adapt it.\n\n\nSupporting New Model Backends:\n\n\nCreate a new model handler class in llm_eval/models/ inheriting from a base model class (if applicable).\nImplement the required methods for loading, inference, etc.\nRegister the new backend type. Ôøº\n\n\nCustomizing Reporting:\n\n\nModify the report generation logic in llm_eval/reporting/ to change the format or content of the Markdown/JSON outputs.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **LLM Evaluation**",
      "LLM Evaluation Framework"
    ]
  },
  {
    "objectID": "posts/LLM_Evaluation_Framework.html#contributing",
    "href": "posts/LLM_Evaluation_Framework.html#contributing",
    "title": "LLM Evaluation Framework",
    "section": "ü§ù Contributing",
    "text": "ü§ù Contributing\nContributions are welcome! Please follow standard practices:\n\nFork the repository.\nCreate a new branch for your feature or bug fix (git checkout -b feature/my-new-feature).\nMake your changes and commit them (git commit -am ‚ÄòAdd some feature‚Äô).\nPush to the branch (git push origin feature/my-new-feature).\nCreate a new Pull Request.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **LLM Evaluation**",
      "LLM Evaluation Framework"
    ]
  },
  {
    "objectID": "posts/LLM_Evaluation_Framework.html#how-the-evaluation-report-looks",
    "href": "posts/LLM_Evaluation_Framework.html#how-the-evaluation-report-looks",
    "title": "LLM Evaluation Framework",
    "section": "üìÑ How the Evaluation Report Looks",
    "text": "üìÑ How the Evaluation Report Looks\nWhen you run an evaluation using the LLM Evaluation Framework, it generates comprehensive yet easy-to-understand reports in both Markdown and JSON formats. Here‚Äôs a broad overview of what you can expect from the Markdown report:\n\n1. üìä Summary of Metrics\nThis section offers a concise table summarizing your model‚Äôs performance across each task evaluated. Each row clearly indicates:\n‚Ä¢ Task: The specific benchmark or task evaluated (e.g., leaderboard_bbh_boolean_expressions).\n‚Ä¢ Metric: The evaluation metric employed (e.g., accuracy, exact match).\n‚Ä¢ Value: Your model‚Äôs performance score on that task.\nThis summary makes it easy to quickly gauge overall performance across multiple tasks at a glance.\n\n\n2. üìà Normalized Scores\nTo provide clearer insights, the framework calculates normalized scores, presenting a straightforward percentage-based representation of your model‚Äôs performance relative to established benchmarks. Each benchmark will show:\n‚Ä¢ Benchmark: Name of the benchmark.\n‚Ä¢ Score: Normalized percentage score.\nThis helps you quickly pinpoint your model‚Äôs relative strengths and identify areas needing improvement.\n\n\n3. üîç Task Samples (Detailed Examples)\nThe detailed samples section gives you valuable qualitative insights into your model‚Äôs performance by presenting clear examples directly from evaluated tasks. Each example includes:\n‚Ä¢ Question: The evaluation sample question posed to your model.\n‚Ä¢ Ground Truth: The expected correct answer.\n‚Ä¢ Model Response: Your model‚Äôs exact response, explicitly marked as correct or incorrect.\nThese detailed examples are especially useful for conducting error analysis, allowing you to dive deeper into how your model handles specific questions or scenarios.\n\n\n‚öôÔ∏è Customization\nBeyond these default outputs, the reporting mechanism in this framework is highly customizable. You can easily extend or modify report generation logic to meet specialized requirements or incorporate additional analysis, enabling deeper and more tailored insights into your model‚Äôs performance.\nBy providing structured and comprehensive reports, this framework empowers you to effectively evaluate, understand, and communicate the strengths and limitations of your large language models.",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **LLM Evaluation**",
      "LLM Evaluation Framework"
    ]
  },
  {
    "objectID": "posts/LLM_Evaluation_Framework.html#example-evaluating-your-model-on-the-leaderboard-benchmark",
    "href": "posts/LLM_Evaluation_Framework.html#example-evaluating-your-model-on-the-leaderboard-benchmark",
    "title": "LLM Evaluation Framework",
    "section": "üß™ Example: Evaluating Your Model on the LEADERBOARD Benchmark",
    "text": "üß™ Example: Evaluating Your Model on the LEADERBOARD Benchmark\nUsing the Command-Line Interface (CLI)\nLet‚Äôs illustrate how the LLM Evaluation Framework simplifies benchmarking by replicating the popular Hugging Face Open LLM Leaderboard setup‚Äîparticularly useful given its recent discontinuation. Here‚Äôs a practical CLI example that runs the complete leaderboard evaluation:\n\n!python llm_eval_cli.py \\\n  --model hf \\\n  --model_name meta-llama/Llama-2-13b-chat-hf \\\n  --leaderboard \\\n  --device cuda \\\n  --gpu_memory_utilization 0.9  # Adjust based on your GPU availability\n\nWith this simple command, the framework evaluates your model across several key benchmarks including BBH, GPQA, MMLU-Pro, MUSR, IFEval, and Math-lvl-5, automatically configuring the appropriate few-shot examples for each benchmark.\nUsing as a Python Library\nIntegrate the evaluation logic directly into your Python scripts:\n\nfrom llm_eval import evaluate_model\n\n# Run the evaluation\nresults, output_path = evaluate_model(\n    model_type=\"hf\",\n    model_name=\"mistralai/Ministral-8B-Instruct-2410\",\n    tasks=[\"leaderboard\"],\n    num_samples=1,\n    device=\"cuda\",\n    quantize=True,\n    quantization_method=\"4bit\",\n    preserve_default_fewshot=True  # This ensures the correct few-shot settings for each benchmark task\n)\n\n# Print the paths to the results and report\nprint(f\"Results saved to: {output_path}\")\n\n# The report path is derived from the output path\nimport os\nfrom llm_eval.reporting.report_generator import get_reports_dir\n\n# Get the base filename without extension\nbasename = os.path.basename(output_path)\nbasename = os.path.splitext(basename)[0]\n\n# Construct the report path\nreports_dir = get_reports_dir()\nreport_path = os.path.join(reports_dir, f\"{basename}_report.md\")\n\nif os.path.exists(report_path):\n    print(f\"Report generated at: {report_path}\")\nelse:\n    print(\"Report was not generated. Check if there were any errors during evaluation.\")",
    "crumbs": [
      "{{< fa chalkboard >}} LLM Tutorials",
      "‚ö° **LLM Evaluation**",
      "LLM Evaluation Framework"
    ]
  }
]